text,target_M6_predict,target_predict_M6_label
"GNNExplainer [57] Continuous relaxation Mutual Information Size Yes GC+NC Transductive PGExplainer [30] Parameterized edge selection Mutual Information Size and/or connectivity No GC+NC Inductive TAGExplainer [51] Sampling Mutual Information Size, Entropy No GC+NC Inductive GEM [27] Granger Causality+Autoencoder Causal Contribution Size, Connectivity No GC+NC Inductive SubgraphX [62] Monte Carlo Tree Search Shapley Value Size, connectivity No GC Transductive GstarX [63] Monte Carlo sampling HN-value Size No GC Inductive ‚Ä¢ Empirical investigations: How susceptible are the explanations to topological noise, variations in G NN architectures, or optimization stochasticity?",1,neutral
"‚Ä¢ Perturbation-based: These methods [59, 30, 62, 18, 29, 43, 27, 6, 31, 1, 50] utilize perturbations of the input to identify important subgraphs that serve as factual or counterfactual explanations.",1,neutral
"However, GraphFrameX and GraphXAI collectively assess only GnnExplainer [59], PGExplainer [30], and SubgraphX [62].",2,positive
"In a follow-up work, PGExplainer [30] extends the same idea with an additional assumption of the graph to be a random Gilbert graph.",1,neutral
"Gradient: SA [7] , Guided-BP [7] , Grad-CAM [33]; Decomposition: Excitation-BP [33], GNN-LRP [38], CAM [33]; Perturbation: GNNExplainer [59], PGExplainer [30], SubgraphX [62], GEM [27], TAGExplainer [51], CF(2) [43], RCExplainer [6],CF-GNNexplainer [29], CLEAR [31]; Surrogate: GraphLime [18], Relex [64], PGM-Explainer [47]; Global: XGNN [60], GLG-Explainer [5], Xuanyuan et al.",1,neutral
"Explanations can be broadly classified into two categories: factual reasoning [59, 30, 40, 62, 18] and counterfactual reasoning [29, 43, 31, 6, 1, 50].",1,neutral
"Gradient: SA [7] , Guided-BP [7] , Grad-CAM [33]; Decomposition: Excitation-BP [33], GNN-LRP [38], CAM [33]; Perturbation: GNNExplainer [59], PGExplainer [30], SubgraphX [62], GEM [27], TAGExplainer [51], CF 2 [43], RCExplainer [6],CF-GNNexplainer [29], CLEAR [31]; Surro-gate: GraphLime [18], Relex [64], PGM-Explainer [47]; Global: XGNN [60], GLG-Explainer [5], Xuanyuan et al. [54], GCFExplainer [19].",1,neutral
"‚Ä¢ Instance-level: Instance-level or local explainers [59, 30, 40, 62, 18, 61, 29, 43, 27, 6, 1, 50] provide explanations for specific predictions made by a model.",1,neutral
"GNNExplainer [57] Continuous relaxation Mutual Information Size Yes GC+NC Transductive PGExplainer [30] Parameterized edge selection Mutual Information Size and/or connectivity No GC+NC Inductive TAGExplainer [51] Sampling Mutual Information Size, Entropy No GC+NC Inductive GEM [27] Granger Causality+Autoencoder Causal Contribution Size, Connectivity No GC+NC Inductive SubgraphX [62] Monte Carlo Tree Search Shapley Value Size, connectivity No GC Transductive GstarX [63] Monte Carlo sampling HN-value Size No GC Inductive",1,neutral
"[30] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang.",0,negative
"Specifically, in the CFAD framework, we first devise a Granger causality-based [19] causal explanation method to extract the core subgraph for a given node without requiring annotation information, according to the idea that causal explanations can extract a subgraph which is considered the main cause of the corresponding prediction [17], [20].",2,positive
"GNN explainers such as GNNExplainer [25], XGNN [26], and PGExplainer [13] have gained increasing attention in the field of explainable artificial intelligence (XAI), which attempts to identify the most important graph structures and/or features that contribute to GNNs‚Äô predictions.",1,neutral
"GNNexplainers such as GNNExplainer [25], XGNN [26], and PGExplainer [13] have gained increasing attention in the field of ex-plainable artificial intelligence (XAI), which attempts to identify the most important graph structures and/or features that contribute to GNNs‚Äô predictions.",1,neutral
PGExplainer [13] generates explanations for GNN models by using a probabilistic graph.,1,neutral
"Like most GNN-interpretability methods (Luo et al., 2020; Ying et al., 2019; Yuan et al., 2020), we limit our interpretability analysis to topology.",2,positive
"Interestingly, this objective correlates with the aim of GNN interpretability, which is to identify highly influential sparse subsets of nodes and edges with the largest impact on the model‚Äôs behaviour (Luo et al., 2020).",1,neutral
", 2014), perturbations of the input graph (Ying et al., 2019; Luo et al., 2020), and the training of a surrogate model (Vu and Thai, 2020) are examples of such techniques.",1,neutral
"Then, different motifs are added to each random graph (Wu et al., 2022; Ying et al., 2019; Luo et al., 2020) depending on the dataset.",1,neutral
"Datasets For graph classification, we use two synthetic datasets (BA2Motifs (Luo et al., 2020), SPMotifs.",2,positive
"Following works [99, 184] further introduce novel combination strategies and motifs for graph construction.",1,neutral
"To tackle these challenges, PGExplainer [29] aims to address the drawbacks associated with GNNExplainer.",2,positive
"Various recent research endeavors that have devised explanation methods for GNNs, consistently emphasize the interpretability aspects at the levels of nodes, edges, or node features [29, 44, 53].",1,neutral
"(2) There is a lack of research on developing efficient algorithms for subgraph extraction that is one of the leading explanation methods for GNNs [29, 53, 56].",1,neutral
Our implementations mainly follows the settings of officially public Pytorch code of PGExplainer [10] (https://github.com/divelab/DIG/tree/main/dig/ xgraph/PGExplainer).,2,positive
"At Œª =50%, PGExplainer fails to identity key motifs (NO2), yet sucessfully does so at Œª =100%.",0,negative
"We integrate two benchmark post-hoc explainers (GNNEXPLAINER [9] and PGEXPLAINER [10]) into a unified evaluation framework and carefully evaluate the effectiveness of explanations across four graph datasets, including two real-world datasets of different topics and two synthetic datasets.",2,positive
"In response to the black-box limitations of GNNs, a range of GNN explainers have been introduced [9, 10, 17, 18].",1,neutral
"3 Empirical Study To address the aforementioned research questions, we evaluate the explanation quality in terms of Fid and Fid‚àí of two benchmark post-hoc GNN explainers, GNNExplainer [9] and PGExplainer [10], on two GNN models, GCN [19] and GIN [20].",2,positive
We integrate the implementations of GCN and GIN from PyG [21] and GNNExplainer and PGExplainer from the original papers into a unified framework built with DIG [22].,2,positive
"To address the aforementioned research questions, we evaluate the explanation quality in terms of Fid+ and Fid‚àí of two benchmark post-hoc GNN explainers, GNNExplainer [9] and PGExplainer [10], on two GNN models, GCN [19] and GIN [20].",2,positive
Ba2Motif [34] and BAMultiShapes (BaMS for short) [35] are synthetic data sets.,2,positive
They use different techniques to weigh the importance of the node (AttentiveFP [33]) or the edge (GNNExplainer [35] and PGExplainer [13]) in a graph.,1,neutral
"However, AttentiveFP [33], GNNExplainer [35] and PGExplainer [13] only generate the rank of the atoms in a drug.",1,neutral
"For AttentiveFP [33], GNNExplainer [35] and PGExplainer [13], we use the hyper-parameters suggested in the orginal papers.",2,positive
"To make sure the key structures identified by different methods are comparable, for each drug, we count the number of atoms in the output of SLGNN and use the ranks generated in AttentiveFP [33], GNNExplainer [35] and PGExplainer [13] to pick the same number of atoms in the same drug.",2,positive
"In order to provide such information, many explainable GNN models have been designed [13, 33, 35, 36].",1,neutral
"First of all, all the current explainable GNN models [13, 33, 35, 37] use atoms-based graphs, where nodes are atoms and edges are bonds between atoms.",1,neutral
"For GNNExplainer [35] and PGExplainer [13], they become applying the basic message passing algorithm on the atom-based graphs and we name them GNNExplainer(Base) and PGExplainer(Base), respectively.",1,neutral
"The competing methods we consider include our SLGNNand three state-of-the-artmethods: AttentiveFP [33], GNNExplainer [35] and PGExplainer [13].",2,positive
"GNNExplainer [35] and PGExplainer [13] are also developed to capture the critical substructures in drug molecules, their fundamental ideas for interpretation are similar by utilizing mask matrix which is learned by optimizing the mutual information of prediction and subgraphs‚Äô distribution, where entries in the matrix represent nodes‚Äô importance.",1,neutral
"They [13, 33, 35] have been tested on BBBP, MUTAG, Tox21, synthetic datasets, etc.",1,neutral
PGExplainer [53] improves on GNNExplainer by selectively choosing the sub-graph for the candidate explanation instead of trying all permutations of the sub-graph.,1,neutral
Previous work [39] shows that task-specific explainers like PGExplainer cannot transfer to other tasks.,1,neutral
"Despite their strengths, GNNs are usually treated as black box models and thus cannot provide human-intelligible explanations [18, 42].",1,neutral
2) PGExplainer [18] is an inductive explanation method.,1,neutral
"Here, carbon rings with chemical groups NO2 groups and NH2 are widely known to be mutagenic [18, 20].",1,neutral
"3Note that on BA-2MOTIFS and MUTAG, GNNExplainer and PGExplainer work worse than results reported in previous work [18] as we do not cherry pick the target model.",0,negative
"According to the taxonomy provided in a recent survey [45], these methods can be subsumed into four technical route lines: the gradient-based [2, 22], perturbation-based [18, 24, 27, 33, 42], decomposition-based [2, 23], and surrogate modelbased [11, 32] methods.",1,neutral
BA-2MOTIFS is a synthetic dataset where house motifs and cycle motifs give class labels and thus are regarded as ground-truth explanations.,1,neutral
"We observe that 1) Our proposed methods supervised by the representation space (i.e., EGIB and EGIB-TA) outperform task-specific explanation methods (i.e., GNNExplainer, PGExplainer, and Refine).",2,positive
"Typical task-specific explanation methods [18, 33, 42] usually supervise the explanations in the label space by employing Mutual Information (MI) as a relevance metric to measure the performance of explanations.",1,neutral
", MUTAG [6] and BA-2MOTIFS [18] as well as quantitative results.",1,neutral
"The former term can be addressed following previous works, e.g., PGExplainer [18]:
Lùë°ùë† (ùëå ; ùëÜ ;Œ¶) = ‚àí ùëÅ‚àëÔ∏Å ùëñ=1 ùê∂‚àëÔ∏Å ùëê=1 ùëÉ (ùëìùë° (ùëîùëñ ) = ùëê) log ùëÉ (ùëìùë° (ùë†ùëñ ) = ùëê), (12)
where ùëìùë° = ùëìùëë ‚ó¶ ùëìùëí denotes the composition of GNN-based encoder ùëìùëí and downstream model ùëìùëë .",2,positive
"(4) Perturbation-based methods [18, 24, 27, 33, 42] generate masks with a parametrized explainer model.",1,neutral
"To tackle this problem, existing graph explanation methods [18, 33, 39] usually assume ei j follows the Bernoulli distribution.",1,neutral
"‚ñ°
B IMPLEMENTATION DETAILS B.1 Implementations of Explainers We adopt the multilayer perceptron (MLP) as the attributor TŒ¶ to calculate the logitsùë§ùëñ ùëó following PGExplainer [18].",2,positive
"As there are no ground-truth explanations for our above-used muti-task datasets, we provide more visualization results on two additional single-task datasets with explanation ground-truths, i.e., MUTAG [6] and BA-2MOTIFS [18] as well as quantitative results.",2,positive
"5, we evaluate the efficiency of three different explanation strategies: the task-specific transductive explanation strategy (e.g., GNNExplainer), the task-specific inductive explanation strategy (e.g., PGExplainer) and our proposed twostage task-agnostic explanation strategy in a muti-task setting.",2,positive
"Compared with GNNExplainer, PGExplainer provides a global understanding of predictions made by GNNs. 3) Refine [33] is an inductive explanation method that learns the multi-grained explanations with class-wise attributors and contrastive learning.",2,positive
"There are mainly three 3Note that on BA-2MOTIFS and MUTAG, GNNExplainer and PGExplainer work worse than results reported in previous work [18] as we do not cherry pick the target model.",0,negative
2) We adopt the typical Bernoulli distribution assumption [18] with GumbelSoftmax reparameterization trick instead of our Categorical assumption and refer to this variant as EGIB /wo Cat.,2,positive
"‚Ä¢ PGExplainer adopts a deep neural network to parameterize the generation process of explanations [31], PGExp for short.",2,positive
"So as to illustrate the effectiveness of our model, we compare our proposed method with interpretable graph learning methods including GRAD [51], ATT [40], GNNExplainer [51], PGExplainer [31], RCExplainer [1], and CF-GNNExplainer [30].",2,positive
proposed a deep neural network-based method to parameterize the generation process of explanations [31].,1,neutral
"PGExplainer, on the other hand, generates explanations using a deep neural network, also maximizing mutual information [31].",1,neutral
"Brought to GML, perturbation-based explainers learn masks that assign an importance to edges and/or features of the graph [24, 29, 48].",1,neutral
Many existing works [30] aim to identify a subgraph that is highly correlated with the classification prediction result which is likely to get misleading explanations.,1,neutral
"Perturbation-based methods, such as GNNExplainer [66], PGExplainer [35], ZORRO [15], GraphMask [52], RC-Explainer [60], SubgraphX [70], measure the impact of the perturbation of the input features on the output of the classifier, to detect the most important features.",1,neutral
"In addition, according methods for enhancing explainability and interpretability [42], [43] are a futher interesting direction for future research.",1,neutral
"For GNNExplainer and PGExplainer, which are previously used for the classification task, we replace the Cross-Entropy loss with the MSE loss.",2,positive
"In previous works [25, 27, 53] for graph classification, the mutual information I (G‚àó;Y ) is estimated with the Cross-Entropy between the the predictions f (G‚àó) from GNN model f and its prediction label Y from the original graph G .",1,neutral
"could also be classified into two categories based on their methodology: self-explainable GNNs [1, 8] and post-hoc explanation methods [25, 53, 58], where the former methods provide both predictions and explanations, while the latter methods use an additional model or strategy to explain the target GNN.",1,neutral
"Based on IB, a recent work unifies the most existing post-hoc explanation methods for GNN, such as GNNExplainer [53], PGExplainer [25], with the graph information bottleneck (GIB) principle [27, 47, 55].",2,positive
"(4) PGExplainer [25]: PGExplainer adopts a deep neural network to parameterize the generation process of explanations, which facilitates a comprehensive understanding of the predictions made by GNNs.",2,positive
"We follow [25] to make a widely-accepted assumption that a graph can be divided by G = G‚àó + GŒî, where G‚àó presents the underlying sub-graph that makes important contributions to GNN‚Äôs predictions, which is the expected explanatory graph, and GŒî consists of the remaining label-independent edges for predictions made by the GNN.",1,neutral
"In practice, we follow the previous work [25, 50, 53] to implement them.",2,positive
"Examples of such methods include GNNExplainer [53], which determines the importance of nodes and edges through perturbation, and PGExplainer [25], which trains a graph generator to incorporate global information.",1,neutral
"GNN explainability: The explanation methods for GNN models could be categorized into two types based on their granularity: instance-level [25, 34, 53, 58] and model-level [56], where the former methods explain the prediction for each instance by identifying important sub-graphs, and the latter method aims to understand the global decision rules captured by the GNN.",1,neutral
"In this paper, we focus on discovering the important sub-graph typologies following the previous work [25, 53].",1,neutral
"The subsequent work, such as PGExplainer (Luo et al., 2020), PGMExplainer (Yuan et al.",2,positive
"The subsequent work, such as PGExplainer (Luo et al., 2020), PGMExplainer (Yuan et al., 2021), and SubgraphX (Vu & Thai, 2020), followed (Ying et al., 2019) and evaluated GNN explainers on the suggested synthetic datasets or their close variants.",2,positive
"Such understanding fosters trust in the system and enables the discovery of latent patterns within the data [297], [299].",1,neutral
"Perturbation-based methods [19,20,26,41,50] estimate input scores with the assumption that removing important features will have a large impact on the",1,neutral
PGExplainer finds more crashes on ReVeal than all other methods on Devign for Libxml2 given Table 2.,2,positive
PGExplainer has the best MAZ for all PUTs and for both models since around 0.95‚àí 0.98% of the code lines score an accumulated relevance lower than 50%.,0,negative
"Although their DA is superior compared to GNNExplainer and PGExplainer, their located code lines are, however, unrelated to the actual underlying vulnerability concluding from our extrinsic results.",0,negative
"With the rise of graph neural networks, several works ported the underlying classic explanation concepts to the graph domain [4, 41], as well as completely new graphspecific algorithms have been invented [36, 44, 54].",1,neutral
"Considering the sparsity, GNNExplainer and LineVul achieve the worst MAZ results, and PGExplainer and Smoothgrad yield the best.",0,negative
"Although the average crashes per path are best for PGExplainer, GNNExplainer has the most precise explanations.",0,negative
"In particular, we focus on the graph-agnostic methods Smoothgrad [46] and GradCAM [45] that are widely applied in computer vision and the graph-specific methods GNNExplainer [54] and PGExplainer [36] tailored towards explaining GNNs.",2,positive
4) PGExplainer: This method uses a so-called explanation network on an embedding of the graph edges [36].,1,neutral
"However, it for example, beats PGExplainer and SmoothGrad on Libming using Devign, proving it to be a strong baseline.",2,positive
"Our results are in line with Ganz et al. [21] since according to them, Smoothgrad is among the best candidates considering the DA and PGExplainer produce the most concise explanations.",0,negative
Both methods outperform GradCam and PGExplainer but are inferior to GNNExplainer and SmoothGrad.,0,negative
"With the logarithmic stretch, we first assess the influence of the model on the EM‚Äôs output, for instance, GradCam, SmoothGrad, GNNExplainer and PGExplainer in Figure 5.",2,positive
"Renowned homogeneous GNNs include graph convolution networks (GCNs) [14], graph attention networks (GATs) [46], and graph isomorphism
networks (GINs) [53].",1,neutral
HGNNs can be grouped into meta-path-based methods and stacking-fashion models according to their way of entangling edge and node types into the network.,1,neutral
"The majority of current explainers remain topological-level [3, 21, 23, 42, 51, 52, 57], which cannot be easily extended tomulti-level as they are oriented towards the combinatorial nature of graph topology.",1,neutral
"Though an explainable fraud transaction detectionmodel was proposed [36], which contains an initial attempt to study explainability on heterogeneous graphs, as it is fully based on a variant of GNNExplainer, we do not consider it an model-agnostic explainer for HGNNs.",2,positive
"‚Ä¢ PGExplainer [23] is an inductive explainer, which can be directly used on new instances after training on a group of data.",1,neutral
"The stacking-fashion HGNNs adopt architectures consisting of stacking layers with the same structure, which is analog to homogeneous GNNs. Examples of these models include HGT [12], Simple-HGN [24], and the like.",1,neutral
"On the other hand, as an emerging line of works, there has not been any modelagnostic explainer for HGNNs to the best of our knowledge.",2,positive
"The former line of works [3, 21, 23, 42, 45, 51, 52, 54, 57] adopts heuristic metrics to quantify ‚Äúexplainability‚Äù for designing optimization objectives or loss functions.",1,neutral
"However, feature-level explanation is ignored by most existing methods [21, 23, 42, 54, 57], which can lead to unreasonable or even misleading explanations.",1,neutral
"When Œ¶t = max, the values of feature variables correspond to a factual explanation [23, 54], which can maximize the model‚Äôs belief in its decision by presenting only the critical parts.",1,neutral
"The pioneering research of explaining DGNs include GNNExplainer [54] and heat-map-based methods for GCNs [35], since then intensive research efforts have been devoted to explain homogeneous GNNs [11, 21, 23, 42, 47, 50, 54, 55, 57].",1,neutral
"Not to mention that model-level explanations provide easy generalization to an inductive setting, which is the nature of many GNN applications [31].",1,neutral
"Given the input data, instancelevel techniques [29, 31, 52, 54, 65, 68] have been present main stream of GNN explanation, which aim to acquire explanations for a target instance.",1,neutral
"As a result, intensive research efforts have been devoted to understand how GNNs make decisions [15, 31, 43, 54, 65, 72].",1,neutral
", instance-level explanation [29, 31, 52, 54, 56, 65, 69], which aims to find one explanation for a given input instance.",1,neutral
"These methods attribute model predictions to graph objects, such as nodes (Vu and Thai 2020), edges (Ying et al. 2019; Luo et al. 2020; Schlichtkrull, De Cao, and Titov 2020; Wang et al. 2021b; Lin, Lan, and Li 2021) and subgraphs (Yuan et al. 2021).",1,neutral
"These methods attribute model predictions to graph objects, such as nodes (Vu and Thai 2020), edges (Ying et al. 2019; Luo et al. 2020; Schlichtkrull, De Cao, and Titov 2020; Wang et al. 2021b; Lin, Lan, and Li 2021) and subgraphs (Yuan et al.",1,neutral
"2019), PGEXplainer (Luo et al. 2020) SubgraphX (Yuan et al.",2,positive
"In this paper, we compare our methods with state-of-the-art GNN explanation methods, including GNNExplainer (Ying et al. 2019), PGEXplainer (Luo et al. 2020) SubgraphX (Yuan et al. 2021) and ReFine (Wang et al. 2021), and inherently interpretable GNN methods, including GAT and GSAT (Miao, Liu, and Li 2022).",2,positive
"PGExplainer follows the same paradigm as GNNExplainer which maximizes the mutual information between the predictions of the input graph and that of the evidence subgraph; however, it only generates edge masks by using a deep neural network to parameterize the generation process of the evidence subgraph.",2,positive
"Due to its parameterized generation process, PGExplainer can explain multiple instances collectively and also works in an inductive setting.",1,neutral
", GNNExplainer [24], PGExplainer [10], GraphMask [14], and SubgraphX [27]), that is, masking some node features and/or edge features and analyzing the resulting changes when the modified graphs are passed through the GNN model.",1,neutral
"They could be perturbation-based (e.g., GNNExplainer [24], PGExplainer [10], GraphMask [14], and SubgraphX [27]), that is, masking some node features and/or edge features and analyzing the resulting changes when the modified graphs are passed through the GNN model.",1,neutral
"Perturbations-based methods monitor the change of prediction with respect to different input perturbations, including GNNExplainer [8], PGExplainer [9], GraphMask [10], SubgraphX [24], and so on.",1,neutral
PGExplainer [9] learns a parameterized model to predict the importance of an edge.,1,neutral
Yuan‚Äôs experimental results show that SubgraphX achieves significantly improved explanations compared with PGExplainer [9] and GNNExplainer [8].,2,positive
"After that, instance-level methods such as PGExplainer [9] and GraphMask [10] gradually became the research focus.",1,neutral
"In recent studies [9], [41], it is known that carbon rings and NO2 groups tend to be mutagenic.",1,neutral
"They are gradient-based [22], perturbation-based [31, 18], decomposition-based [11], and surrogate-based methods [27].",1,neutral
"To improve transparency and understand the behavior of GNNs, most recent works focus on providing post-hoc interpretation, which aims at explaining what patterns a pre-trained GNN model uses to make decisions [31, 18, 34, 28].",1,neutral
"Many works [29], [30], [31], [32], [33], [34], [35], [36] have been proposed to extract meaningful data patterns for prediction by post-hoc methods [31], [34] as well as inherently interpretable models [29], [32], [33].",1,neutral
"Several works are proposed to ind a subgraph structure and a small subset of node features for the target nodes as the explanations for GNN‚Äôs predictions [49, 52, 88].",1,neutral
PGExplainer [52] further learns the approximated discrete masks on edges to explain the predictions with a parameterized mask predictor.,1,neutral
"The experiments are conducted on six benchmark datasets: BA-Shapes, BA-Community, Tree-Cycles [36] and ogbn-arxiv [14] for node classification, and BA-2Motifs [24] and MUTAG [7] for graph classification.",2,positive
"Among the baseline methods, GNNExplainer, PGExplainer and OrphicX adopt the MI value for node attribution, which is less effective than LARA, especially on ogbn-arxiv.",2,positive
"Moreover, LARA exhibits about 90% less time latency compared to the competitive baseline PGExplainer in general.",0,negative
"We benchmark our methods with GNNExplainer, PGExplainer, and GraphSVX on synthetic datasets in Table 9.",2,positive
"According to the ground truth explanation, the key subgraph structure for the classification is a pentagon or a house structure on the BA-2Motifs dataset.",0,negative
"We deploy LARA to explain node classification on BA-Shapes, BA-Community, Tree-Cycles and ogbn-arxiv; and graph classification on the BA-2Motifs and MUTAG datasets.",2,positive
"We evaluate the faithfulness and efficiency of LARA compared to state-of-the-art methods: GNNExplainer [36], PGExplainer [24], GraphSVX [9] and OrphicX [19].",2,positive
"PGExplainer [24] learns a model using the reparameterization trick to predict edge masks indicating their importance, while OrphicX [19] identifies the causal factors by maximizing the information flow from the latent features to the model predictions, which are used to produce explanations.",1,neutral
"Node # 102 103 104 105
GNNExplainer 15.34 119.84 1186.43 12101.1 PGExplainer 5.63 35.52 490.48 3534.65 LARA 0.54 4.23 39.92 416.50
Table 4: Effectiveness of explainer layer number.",0,negative
"On BA-2Motifs, the graph is classified by the type of motifs.",1,neutral
", MI with regularization [24] or the causal counterpart of MI [19]), so that the trained explainer provides explanatory features (e.",1,neutral
"The BA-2Motifs has 1, 000 synthetic graphs with binary labels, while MUTAG contains 4, 337 molecule graphs with binary labels.",1,neutral
"D.2 Target Model Details
For each dataset, we use the target model under the settings as given in [24] for BA-Shapes, BACommunity, Tree-Cycles, BA-2Motifs and MUTAG 2.",2,positive
"4 Code for GNNExplainer and PGExplainer is at https://github.com/LarsHoldijk/RE-ParameterizedExplainerForGraphNeuralNetworks; for
OrphicX is at https://github.com/wanyugroup/cvpr2022-orphicx; for GraphSVX is at https://github.com/AlexDuvalinho/GraphSVX.",2,positive
"Although PGExplainer and OrphicX can simultaneously generate a batch of explanations, their time complexity increases with the edge number contained in the relevant subgraph of the target node.",2,positive
"The baseline follows existing work [36, 24] to adopt the mutual information for the supervision of training the explainer.",0,negative
"In order to provide efficient GNN explanations, previous work [24, 18, 30, 4] attempts to train a deep neural network-based explainer, amortizing the time and resource cost of generating explanations of many samples.",1,neutral
"In particular, as the node number grows from 102 to 105, GNNexplainer and PGExplainer show approximate 12000s and 3500s growth of time latency (‚àÜ time latency), respectively.",2,positive
"Existing Works: At a high level, GNN explainers can be classified into the two groups of instancelevel [32, 14, 18, 36, 7, 35, 13, 21, 12, 4, 1, 27] or model-level explanations [33].",1,neutral
"Instance-level methods can broadly be grouped into two categories: factual reasoning [32, 14, 18, 36, 7, 35] and counterfactual reasoning [13, 21, 4, 1, 27].",1,neutral
"Instance-level algorithms provide explanations for individual predictions of the GNN and include methods that utilize the gradients of the features to determine input importance, such as sensitivity analysis, Guided BP, and Grad-CAM [2,33,30], perturb inputs to observe changes in output as in GNNExplainer and PGExplainer [42,20], and learn relationships between the input and its neighbors using surrogate models [15,38].",1,neutral
"We compare MSInterpreter with four popular explainable methods PGExplainer (Luo et al., 2020), GNNExplainer (Ying et al.",2,positive
"For the BA2Motifs dataset, we compute the four metrics only for correctly predicted data.",2,positive
‚Ä¢ BA2Motifs is a synthetic graph motif detection dataset.,2,positive
BA2Motifs contains BarabasiAlbert (BA) base graphs of size 20 and each graph has five node patterns.,1,neutral
"Recently, several advanced approaches (Ying et al., 2019; Luo et al., 2020; Schlichtkrull et al., 2021; Huang et al., 2022; Vu & Thai, 2020; Gui et al., 2022; Yuan et al., 2021; Schnake et al., 2021; Yuan et al., 2020; Yu & Gao, 2022) have been proposed to explain the predictions of graph neural‚Ä¶",2,positive
"The statistics for the BA2Motif (Luo et al., 2020) and MUTAG0 (Tan et al., 2022) datasets are shown in Table 2.",0,negative
"We compare MSInterpreter with four popular explainable methods PGExplainer (Luo et al., 2020), GNNExplainer (Ying et al., 2019) ,SubgraphX (Yuan et al., 2021), and GStarX (Zhang et al., 2022).",2,positive
"Recently, several advanced approaches (Ying et al., 2019; Luo et al., 2020; Schlichtkrull et al., 2021; Huang et al., 2022; Vu & Thai, 2020; Gui et al., 2022; Yuan et al., 2021; Schnake et al., 2021; Yuan et al., 2020; Yu & Gao, 2022) have been proposed to explain the predictions of graph neural networks (GNNs), and are divided into two categories (Yuan et al.",2,positive
We compare two datasets with edge interpretation labels: MUTAG0 and BA2Motifs.,2,positive
"Lastly, GLGExplainer [2] uses local explanations from PGExplainer [56] and projects them to a set of learned prototype or concepts (similar to ProtGNN [125]) to derive a concept vector.",2,positive
"We highlight six popular synthetic datasets:
BA-Shapes [115]: This graph is formed by randomly connecting a base graph to a set of motifs.",1,neutral
"Information constraints: GIB [118], VGIB [116], GSAT [69], LRI [70]; Structural Constraints: DIR [107], ProtGNN [125], SEGNN [12], KER-GNN [21]; Decomposition: CAM [77], Excitation-BP [77], DEGREE [22], GNN-LRP [84]; Gradient-based: SA [5] , Guided-BP [5] , GradCAM [77]; Surrogate: PGM-Ex [56], GraphLime [34], GraphSVX [17], ReLex [124], DnX [75]; Perturbationbased: GNNExplainer[115], GraphMask [82], PGExplainer [56], ReFine [100], ZORRO [23], SubgraphX [122], GstarX [123]; Generation: XGNN [119], RGExplainer [85], GNNInterpreter [101], GFlowExplainer [46], GEM [49]; (2) Counterfactual.",2,positive
"Due to these challenges, explaining graph neural networks is non-trivial and a large variety of methods have been proposed in the literature to tackle it [115, 69, 107, 77, 5, 97, 56, 119, 93, 4, 73].",1,neutral
"For instance, they may involve identifying important substructures within the input data [56, 82, 122], providing additional examples from the training data [12], or constructing counterfactual examples by perturbing the input to produce a different prediction outcome [55, 93, 9].",1,neutral
"GNNExplainer [115] Continuous relaxation MI Size Yes SubgraphX [122] Monte Carlo Tree Search SV Size, connectivity No GraphMask [82] Layer-wise parameterized edge selection L0 norm Prediction divergence No PGexplainer [56] Parameterized edge selection MI Size and/or connectivity No Zorro [23] Greedy selection Fidelity Threshold fidelity Yes ReFine [100] Parameterized edge attribution MI Number of edges No GstarX [123] Monte Carlo sampling HN-value Size No",1,neutral
"RCExplainer [4] Instance level Graph classification Node classification Edge prediction with Neural Network Mutag [14], BA-2motifs [56], NCI1 [99] Tree-Cycles [115], Tree-Grids [115] BA-Shapes [56], BA-Community [115]",1,neutral
BA-Community [115]: The BA-community graph is a combination of two BA-Shapes graphs.,1,neutral
"Decomposition-based: CAM [77], Excitation-BP [77], DEGREE [22], GNNLRP [84]; Gradient-based: SA [5] , Guided-BP [5] , Grad-CAM [77]; Surrogate: PGM-Ex [56], GraphLime [34], GraphSVX [17], ReLex [124], DnX [75]; Perturbation-based: GNNExplainer [115], GraphMask [82], PGExplainer [56], ReFine [100], ZORRO [23], SubgraphX [122], GstarX [123]; Generation-based: XGNN [119], RGExplainer [85], GNNInterpreter [101], GFlowExplainer [46], GEM [49].",2,positive
"In a follow-up work, PGExplainer [56] extends the idea in GNNExplainer by assuming the graph to be a random Gilbert graph, where the probability distribution of edges is conditionally independent.",1,neutral
"The white box methods GNNExplainer [36], PGExplainer [37], and TAGE [54], have access to the target GNN‚Äôs internals.",2,positive
PGExplainer [24] shares the same objective as GNNExplainer and trains a generative model to generate explanations.,2,positive
"With the advances in network attribution methods [29], extensive attempts have been made to open such ‚Äúblackboxes‚Äù [24,39].",1,neutral
"PGExplainer [2] applies a similar objective, but samples subgraphs using the reparametrization trick.",1,neutral
", PGExplainer [2] concatenates node representations based on their index order).",1,neutral
"Several post-hoc explainers have been proposed for explaining Graph Neural Networks‚Äô predictions using subgraphs [1, 2, 30, 3, 31, 29].",1,neutral
"Finally, we consider inductive GNN explainers: PGExplainer [2], RCExplainer [3], TAGE [29].",1,neutral
"Recent papers [1, 2, 3] have proposed different alternative notions of explainability that do not take the user into consideration and instead are validated using examples.",1,neutral
"It is hypothesized that the nitro group (NO2) is one of the main reasons for the property of mutagenicity [15,17].",1,neutral
Table 1 shows the explanation AUC and time efficiency (the training time is shown outside the parentheses for PGExplainer).,0,negative
We also split data for baselines requiring additional training (e.g. PGExplainer).,0,negative
"We compare with four baselines methods: GRAD (Ying et al., 2019), GAT (VelicÃåkovicÃÅ et al., 2017), GNNExplainer (Ying et al., 2019) and PGExplainer (Luo et al., 2020).",2,positive
"Specifically, GNNExplainer and PGExplainer (Ying et al., 2019) maximize the mutual information between perturbed input and original input graph to identify the important features.",1,neutral
"For baselines (e.g., PGExplainer) who need training additional modules, we also split the data.",2,positive
"Some other methods borrow the idea from perturbation based explanation (Ying et al., 2019; Luo et al., 2020; Lucic et al., 2021), under the assumption that removing the vital information from input would significantly reduce output confidence.",1,neutral
"(4) PGExplainer learns an MLP (Multi-layer Perceptron) model to generate the mask using the reparameterization trick (Jang et al., 2017).",1,neutral
"There are few attempts of explainers on the node classification task [6, 21, 38, 43], while the work on interpretable link prediction based on GNNs is rather limited [36].",1,neutral
"To address the problem of lacking interpretability in GNNs, extensive works have been proposed [13, 21, 38].",1,neutral
PGExplainer [21] generates the edge masks with parameterized explainer to find the significant subgraphs.,1,neutral
", 2023a) and important vertex/edge identification (Ying et al., 2019; Luo et al., 2020; Vu and Thai, 2020; Yu et al., 2023; Kan et al., 2022c), towards tasks such as connectomebased disease prediction and multi-level neural pattern discovery.",2,positive
"We follow the graph pruning setup in [14, 37] and adapt it to the dynamic graph modeling context.",1,neutral
PGExplainer [14] learns a probabilistic graph generative model to provide explanations or importance on each edge.,1,neutral
"Following the practice in [14, 16, 37], we utilize the binary concrete distribution to approximate the sampling process and obtain the sampled sub-",1,neutral
"One way to rectify the aforementioned issues is graph pruning [14, 20, 37], a new yet promising technology that has paved the way to meet the challenges of reliable graph learning at scale.",2,positive
"Compounding the issue of GNNs‚Äô limited transparency, GNN explainability methods mostly rely on brittle and untrustworthy local/post-hoc methods (13; 25; 26; 33; 39) or pre-defined subgraphs for explanations (2; 36), which are often unknown in UA.",1,neutral
"Inspired by vision approaches (35; 32; 10), early explainability techniques focused on feature importance (30), while subsequent works aimed to extract local explanations (39; 25; 36) or global explanations using conceptual subgraphs by clustering the activation space (26; 40; 27).",1,neutral
"Due to the intended objective to learn the CFG node importance, the model is capable of addressing adversarial evasion techniques such as XOR obfuscation, Sematic NOP obfuscation, code manipulation, etc. Experiments with YANCFG dataset [20] over three state-of-the-art models (GNNExplainer [30], SubgraphX [31], and PGExplainer [32]) justify the feasibility of the approach.",2,positive
"Experiments with YANCFG dataset [20] over three state-of-the-art models (GNNExplainer [30], SubgraphX [31], and PGExplainer [32]) justify the feasibility of the approach.",2,positive
"‚Ä¶understanding oversmoothing (Li et al., 2018; Zhao & Akoglu, 2020; Oono & Suzuki, 2020a; Rong et al., 2020), interpretability (Ying et al., 2019; Luo et al., 2020; Vu & Thai, 2020; Yuan et al., 2020; 2021), expressiveness (Xu et al., 2019; Chen et al., 2019; Maron et al., 2019; Dehmamy et al.,‚Ä¶",1,neutral
", 2020), interpretability (Ying et al., 2019; Luo et al., 2020; Vu & Thai, 2020; Yuan et al., 2020; 2021), expressiveness (Xu et al.",1,neutral
"One is node classification trained on BA-Shapes and BA-Community, the other is graph classification trained on MUTAG and Circuits[10].",1,neutral
The Circuits dataset is consisting of 500 circuit graphs.,2,positive
"[27] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang.",0,negative
"Recently, the explainability of graph neural networks (GNNs) [19, 20, 21, 22, 23, 24] has been explored [25, 26, 27, 28, 29, 30, 31], but these studies are limited to understanding supervised GNNs.",1,neutral
"Moreover, [29, 30, 27, 31] have recently proposed generating explanations for supervised GNN models, but these methods cannot be applied to unsupervised node embedding.",1,neutral
"A small subset of GNN explainers, especially local, perturbation-based methods (such as Yuan et al. (2021); Duval & Malliaros (2021); Luo et al. (2020); Ying et al. (2019)) can be applied to black-box models.",1,neutral
In the second step we apply PGExplainer with a minor modification to circumvent the ‚Äúintroduced evidence‚Äù [21] issue due to the presence of soft masks.,0,negative
These modifications lead to design choices for the regularization terms that are different from those originally used in PGExplainer.,2,positive
"Of particular interest for this work is PGExplainer [6], which uses a small network to parametrize the probability of each edge œâij of being part of the explanatory subgraph, and sample from this distribution to obtain the final explanation subgraph characterized by edges eij .",1,neutral
"The vast majority of these techniques [5, 6] provide a post-hoc explanation, thus inferring the reasons that led to a specific outcome by a trained model.",1,neutral
"While PGExplainer generates a unique explanation subgraph, we are interested in collecting
several subgraphs to train the subgraph-based classifier.",2,positive
"In particular, there are gradient-based approaches [13], perturbation-based approaches [5, 6, 14], decomposition methods [15], and counterfactual explainers like [16] and GEM [17].",1,neutral
"In particular, several GNN explanation studies have been proposed [38,48,68] and claim that the behavior of GNN models is strongly related to the structure of the training graph.",1,neutral
‚Ä¢ BA-2motifs [114]: It is a graph classification dataset which contains 800 graphs.,2,positive
"Accuracy, F1 and AUC When ground-truth rationales are available for graphs, a direct evaluation method can be employed by comparing the identified explanatory components to the ground-truth explanations [114].",1,neutral
"Many existing works [114, 213, 216] aim to identify a subgraph that is highly correlated with the prediction result.",1,neutral
"Various efforts have been taken for the explainability of GNNs, which can be generally categorized into instance-level post-hoc explanations [114, 213], model-level post-hoc explanations [215] and self-explainable methods [37].",1,neutral
"Different from other explanation methods [114, 213] that only give substructures of high correlation score with the prediction, graph counterfactual explanation aims to get more actionable and useful explanations by understanding how the prediction can be changed in order to achieve an alternative outcome [173].",1,neutral
"Although the aforementioned methods [37, 114, 213, 215] can help to understand and explain GNNs, they can be easily stuck at spurious explanations [208].",1,neutral
"It should be noted that for other graph explanation methods [114, 213], we only remove edges to find the critical subgraph as the explanation for the current prediction.",1,neutral
"Other graph explanation methods such as PGExplainer [54] and GNNExplainer [55] can only identify discrete edges or nodes, which cannot be used for the interaction vulnerability explanation among nodes.",1,neutral
"Concerning this particular task, CFGExplainer outperforms other explainability frameworks like GNNExplainer [139], SubgraphX [140] and PGExplainer [141].",2,positive
"Considering the efficient optimization, we follow (Ying et al. 2019) and (Luo et al. 2020) to approximate it with cross-entropy between y and yÃÇ, where yÃÇ is the prediction with augmentation v as the input and calculated via
v = g(x; œµ) z = fŒ∏(v) yÃÇ = hw(z), (1) where z is the representation and‚Ä¶",1,neutral
"2019) and (Luo et al. 2020) to approximate it with cross-entropy between y and ≈∑, where ≈∑ is the prediction with augmentation v as the input and calculated via",1,neutral
"The uncovering of such black box models in order to gain chemical insights is addressed by the development of graph-specific XAI techniques[69, 70, 71] and by the adaptation of methods from other domains[72, 73, 74].",1,neutral
"We also note that, while DnX is often better than GNNExplainer and PGExplainer, its performance bests FastDnX only in 12.5% of cases.",0,negative
"Building on parameterized explainers by Luo et al. (2020), Wang et al. (2021) proposed ReFine to leverage both global information (e.g., class-wise knowledge) via pre-training and local one (i.e., instance specific patterns) using a fine-tuning process.",2,positive
"To alleviate the burden of optimizing again whenever we want to explain a different node, Luo et al. (2020, PGExplainer) propose using node embeddings to parameterize the masks, i.e., amortizing the inference.",1,neutral
"GNNand PGExplainer do not appear in the comparison for GIN since they require propagating edge masks, and Torch Geometric does not support edge features for GIN.",0,negative
"For GCN and GATED, PGExplainer yields the best results.",2,positive
"The same is not true for the competing methods, e.g., PGExplainer loses over 15% AUC for the BA-Community (cf., GCN and ARMA).",0,negative
"It is worth mentioning that PGExplainer and GNNExplainer ‚Äî as described in the experimental section of their respective papers ‚Äî output edge-level explanations, so their results are not immediately comparable to that of our methods and PGMExplainer.",0,negative
"For all datasets, we measure performance in terms of AUC, following Luo et al. (2020).",2,positive
"We compare DnX against three baseline explainers: GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), and PGMExplainer (Vu and Thai, 2020).",2,positive
"Nonetheless, GNNExplainer and PGExplainer impose strong assumptions on our access to the GNN we are trying to explain.",0,negative
", 2019), PGExplainer (Luo et al., 2020), and PGMExplainer (Vu and Thai, 2020).",2,positive
"Similarly to Luo et al. (2020), we use the cross-entropy function to replace the conditional entropy function H(Y | sf ) with N given instances, and define the reward function as follows:",1,neutral
"Note that different from [27], our method is suitable for the homogeneous and heterogeneous graphs simultaneously.",1,neutral
"Next, we follow [18, 27] to re-parameter the process using the uniform distribution with the parameter Œ© as follows:",1,neutral
"Note that it is a discrete process to select the important edges, similar to [27], we first use binary concrete distribution to calculate P (ei j ) for approximating the sampling process and then re-parameter the selection process to optimize the objective function.",1,neutral
further improved the performance and interpretation for multiple instances [27].,1,neutral
"Similar to [27, 49], our objective is to maximize the mutual information between the detection predictions and the interpretable sub-graph Ge .",2,positive
"Some GNN-based interpretable methods have been developed [27, 28, 33, 34, 37, 49, 61], which can be used to interpret the predictions/generations by GNNs.",1,neutral
"As the determinant subgraphs are expected to be connected, connective constraints [17, 37] are used to allocate more selective probabilities to the edges, which connect with the part selected already.",1,neutral
"Although studies [17, 29, 33] have been extensively conducted on identifying the salient subgraph, network dissection of GNNs remains largely unexplored.",0,negative
"Following previous studies [1, 17, 29], we focus on the structural features (i.",1,neutral
"Most prior studies [1, 15, 17, 33] realize post-hoc explainability from answering ‚ÄúWhich fractions of input graph aremost influential to the prediction?‚Äù, thus generating explanations at the granularity of input features [3].",1,neutral
PGExplainer [17] and GSAT [19] (in its post-hoc working mode) develop an additional encoder to calculate the selection probability of each edge.,2,positive
"The scheme of feature attribution and selection [1, 17, 29, 30, 33] prevails towards post-hoc explanations of GNN models.",1,neutral
"That is, unaware of this limitation, most previous explainers [17, 19, 30, 33] simply feed subgraph f (¬∑|Œò) into the original network Œò, rather than f (¬∑|ŒòÃÇ), and then use I (f (Gs |Œò);Y) as the main part of the loss function to optimize the explainer.",1,neutral
"Attention-based line [17, 19, 30] focuses on training an attention function for edge attribution according to input features.",1,neutral
"Following previous works [17, 33], we adopt the Barabasi-Albert (BA) graphs as the base and attach each graph with one of three motif types: house, cycle, and grid.",1,neutral
"To make the edge dropping procedure differentiable and enable an end-toend optimization process, we relax the discrete p e to a continuous variable in [0, 1] and apply the Gumbel-Max reparameterization trick [12, 15].",1,neutral
"PGExplainer [25] adopts the sameMI importance andmask-learning idea, but it trains a mask predictor to generate a discrete mask.",2,positive
"Given a data instance, most methods generate an explanation by learning a mask to select an edge-induced subgraph [25, 43] or searching over the space of subgraphs [46].",1,neutral
"Since PaGE-Link and both baselines can generate masks M, we also follow [25] to compare explainers by the masks they generated using the ROC-AUC score.",2,positive
"Many works [25, 43] use synthetic data as benchmarks, but no benchmark datasets are available for evaluating GNN explanations for heterogeneous LP.",1,neutral
PGExplainer has a training stage and an inference stage (separated by / in the table).,0,negative
"Thus, we extend the popular GNNExplainer [45] and PGExplainer [25] as our baselines.",2,positive
We refer to these two adapted explainers as GNNExp-Link and PGExp-Link below.,2,positive
"To get the importance of a path, we first use a mean-field approximation for the joint probability by multiplying ùëÉ (ùëí) together, and we normalize each
Algorithm 1 PaGE-Link
GNNExp [45] PGExp [25] SubgraphX [48] PaGE-Link (ours) ùëÇ ( |Eùëê |ùëá ) ùëÇ ( |E |ùëá ) / ùëÇ ( |Eùëê",1,neutral
PGExplainer [25] adopts the same MI importance but trains a mask predictor to generate a discrete mask instead.,2,positive
"We conduct a human evaluation by randomly picking 100 predicted links from the test set of AugCitation and generate explanations for each link using GNNExp-Link, PGExp-Link, and PaGE-Link.",2,positive
GNNExp [43] PGExp [25] SubgraphX [46] PaGE-Link (ours),2,positive
Therefore we extend the widespread GNNExplainer [43] and PGExplainer [25] as our baseline models.,2,positive
"Graph Neural Networks (GNNs) [41, 53] have recently achieved state-of-the-art performance on many graph ML tasks and attracted increased interest in studying their explainability [25, 43, 45, 50].",1,neutral
"Nowadays, GNNs are also widely used in recommender systems.",1,neutral
"Inspired by InfoMin principle proposed by [23], AD-GCL [21] optimizes adversarial graph augmentation strategies to train GNNs to avoid capturing redundant information during the training.",2,positive
"This style of edge learning has also been used in parameterized explanations and adversarial attacks of GNNs [16,22].",1,neutral
GNN-based methods utilize the structural information of bipartite graphs into representations by introducing the powerful GNNs.,1,neutral
"Based on modeling such bipartite graphs, Graph Neural Networks (GNNs) [9,26,8] can learn the effective node representations of users and items for personalized recommendations.",1,neutral
PGExplainer [4] also obtains an edge-based explanation for a target graph by optimizing a neural network that estimates the edge mask.,1,neutral
PGExplainer [4] learns a neural network that outputs an edge mask under the condition that each edge parameter follows a Bernoulli distribution independently.,1,neutral
"A random explainer (which places a random score on each node), GNNExplainer [3], PGExpaliner [4], and SubgraphX [5] were adopted as the baselines for graph classification.",2,positive
"for standard GNNs [49, 99, 46, 2], only few works focused on explaining TGNNs [90, 82, 31].",1,neutral
"Inspired by GIN, Principal Neighbourhood Aggregation (PNA) [108] employs all three of these functions to pool the node embeddings, while TextING [109] includes both mean and maximization pooling to capture the label distribution and strengthen the keyword features.",2,positive
"From the methodology perspective, existing methods can be categorized as (1) self-explainable GNNs [50, 51], where the GNN can simultaneously give prediction and explanations on the prediction; and (2) post-hoc explanations [19, 20, 24], which adopt another model or strategy to provide explanations of a target GNN.",1,neutral
"After incorporating Align Gaus on dataset TreeGrid, SHD distance of top-6 edges drops from 4.39 to 2.13 for GNNExplainer and from 1.38 to 0.13 for PGExplainer.",0,negative
"Our proposed algorithms in Section 5.2 are implemented and incorporated into two representative GNN explanation frameworks, i.e., GNNExplainer [12] and PGExplainer [13].",2,positive
PGExplainer [13] extends it by incorporating a graph generator to utilize global information.,2,positive
"For PGExplainer, learning rate is initialized to 0.003 and training epoch is set as 30.",0,negative
PGExplainer is adopted as the base method.,2,positive
‚Ä¢ ATT [20]: It utilizes average attention weights inside self-attention layers to distinguish important edges.,1,neutral
"‚Ä¢ GRAD [20]: A gradient-based method, which assigns importance weights to edges by computing gradients of GNN‚Äôs prediction w.",1,neutral
"Extending this idea, PGExplainer trains a graph generator to utilize global information in explanation and enable faster inference in the inductive setting [13].",1,neutral
"R denotes regularization terms on the explanation, imposing prior knowledge into the searching process, like constraints on budgets or connectivity distributions [20].",1,neutral
"The details are given as follows: ‚Ä¢ GRAD [13]: A gradient-based method, which assigns
importance weights to edges by computing gradients of GNN‚Äôs prediction w.r.t the adjacency matrix.",1,neutral
"GNNExplainer and PGExplainer are re-implemented, upon which four alignment strategies are instantiated and tested.",2,positive
‚Ä¢ ATT [13]: It utilizes average attention weights inside selfattention layers to distinguish important edges.,1,neutral
"Following PGExplainer [13], chemical groups NH2 and NO2 are used as ground-truth explanations.",1,neutral
"‚Ä¢ PGExplaienr [20]: A parameterized explainer that learns a GNN to predict important edges for each graph, and is trained via testing diferent perturbations; ‚Ä¢ Gem [56]: Similar to PGExplainer but from the causal view, based on the estimated individual causal efect.",2,positive
PGExplainer [20] extends it by incorporating a graph generator to utilize global information.,2,positive
"(3) In summary, our proposal can provide more faithful explanations for both clean and mixed settings, while PGExplainer would suffer from spurious explanations and fail to faithfully explain GNN‚Äôs predictions, especially in the existence of biases.",0,negative
We adopt GNNExplainer and PGExplainer as baselines.,2,positive
"In most cases, the variant utilizing latent Gaussian distribution demonstrates stronger improvements, showing the best results on 3 out of 4 datasets; ‚Ä¢ On more complex datasets like Mutag, the benefit of introducing embedding alignment is more significant, e.g., the performance of PGExplainer improves from 83.7% to 95.9% with Align Gaus.",2,positive
"However, PGExplainer would produce similar explanations as the clean model, still highly in accord with motif structures.",2,positive
"‚Ä¢ PGExplaienr [13]: A parameterized explainer that learns a GNN to predict important edges for each graph, and is trained via testing different perturbations; ‚Ä¢ Gem [45]: Similar to PGExplainer but from the causal view, based on the estimated individual causal effect.",2,positive
"This underlying assumption is rooted in optimization objectives adopted by existing works [19, 20, 24].",1,neutral
"Extending this idea, PGExplainer trains a graph generator to utilize global information in explanation and enable faster inference in the inductive setting [20].",1,neutral
"From the results, we can make the following observations: ‚Ä¢ Across all four datasets, with both GNNExplainer or PG-
Explainer as the base method, incorporating embedding alignment can improve the quality of obtained explanations; ‚Ä¢ Among proposed alignment strategies, those distributionaware approaches, particularly the variant based on Gaussian mixture models, achieve the best performance.",2,positive
"After incorporating it on dataset Mutag, SHD distance of top-6 edges drops from 4.78 to 3.85 for GNNExplainer and from 3.42 to 1.15 for
PGExplainer.",0,negative
"Following PGExplainer [20], chemical groups ÔøΩÔøΩ2 and ÔøΩÔøΩ2 are used as ground-truth explanations.",1,neutral
"For baseline methods GRAD, ATT, Gem, and RG-Explainer, their performances reported in their original papers are presented.",2,positive
PGExplainer and GNNExplainer are used as the backbone method.,2,positive
"From these two experiments, we can observe that embedding alignment can obtain explanations of better faithfulness and is flexible to be incorporated into various models such as GNNExplainer and PGExplainer, which answers RQ1.",2,positive
"A variety of strategies for post-hoc instance-level explanations have been explored in the literature, including utilizing signals from gradients based [49, 53], perturbed predictions based [19, 20, 24, 54], and decomposition based [49, 55].",1,neutral
"For BA2Motif dataset, Cycle and House motifs are causal factors which determine the graph label, while Tree motif is non-causal factor which is spuriously associated with the true label Luo et al. (2020). In Figure 7, CI-GNN could successfully recognize the Cycle and House motifs that explain the labels, while GNNExplainer, PGExplainer and RC-Explainer assign the larger weights on edges out of Cycle and House motifs, suggesting that Tree motif (the spurious correlation) obtained by GNNExplainer, PGExplainer and RC-Explainer could lead to unreliable prediction.",2,positive
"For example, GNNExplainer Ying et al. (2019) and PGExplainer Luo et al. (2020) employ edge masks to identify a compact subgraph structure, while ZORRO Funke et al. (2020) uses node masks and node feature masks to recognize essential input nodes and node features.",1,neutral
"‚Ä¶et al. (2018)) and six recently proposed state-of-the-art (SOTA) graph explainers, namely subgraph information bottleneck (SIB) Yu et al. (2021), GNNExplainer Ying et al. (2019), PGExplainer Luo et al. (2020), RC-Explainer Wang et al. (2022), OrphicX Lin et al. (2022) and DIR-GNN Wu et al. (2021).",2,positive
"For example, GNNExplainer Ying et al. (2019) and PGExplainer Luo et al. (2020) extract a compact subgraph to provide the instance-level explanations, while XGNN Yuan et al. (2020) generates the discriminative graph patterns to provide the model-level explanations.",2,positive
"For example, in the BA-2Motif dataset Luo et al. (2020) in which graphs with House motifs are labeled with 0 and the ones with Cycle are with 1.",1,neutral
"For BA2Motif dataset, Cycle and House motifs are causal factors which determine the graph label, while Tree motif is non-causal factor which is spuriously associated with the true label Luo et al. (2020).",1,neutral
"Specifically, a trained PGExplainer [14] and Gem [12] can be used in inductive scenarios to infer explanations for unexplained instances without retraining the explanation models.",1,neutral
PGExplainer [14] introduces explanations for GNNs with the use of a probabilistic graph.,1,neutral
"Recently, several explainers [25,5,26,14,12] have been proposed to tackle the problem of explaining GNN models.",1,neutral
"While PGExplainer [14], Gem [12], and XGNN [26] can provide a global explanation of the model prediction.",2,positive
"Recently, new approaches to explain and interpret the predictions of GNNs have also been proposed and they include GNNExplainer [55], PGExplainer [35], and PGMExplainer [47], which focus on providing single instance or multi-instance explanations for homogeneous networks.",1,neutral
"The instance-level category includes gradient/features-based methods [2], [23], perturbation-based methods [21], [26], [38], [42], decomposition-based methods [2], [27], [28], and surrogate-based methods [13], [34].",1,neutral
"Molecular graphs of the Mutagenic class have two topology groups, one with motif NO2 and another one with motif NH2 [13].",1,neutral
"The perturbation-based interpretation methods [8], [9] can only select the set of important nodes, but it is difficult to produce a fair and reasonable importance ranking for nodes.",1,neutral
", 2019; Zhang and Zitnik, 2020), Black-box explanation (Ying et al., 2019; Luo et al., 2020; Vu and Thai, 2020), etc.",2,positive
"2019), PGExplainer (Luo et al. 2020), GraphLIME (Huang et al.",2,positive
"Our work is directly related to black-box perturbationbased explaining methods for GNNs, including GNNExplainer (Ying et al. 2019), PGExplainer (Luo et al. 2020), GraphLIME (Huang et al. 2020), PGMExplainer (Vu and Thai 2020), RelEx (Zhang, DeFazio, and Ramesh 2021), GraphSVX (Duval and Malliaros 2021) and ZORRO (Funke, Khosla, and Anand 2021).",2,positive
"Our work is directly related to black-box perturbationbased explaining methods for GNNs, including GNNExplainer (Ying et al. 2019), PGExplainer (Luo et al. 2020), GraphLIME (Huang et al. 2020), PGMExplainer (Vu and Thai 2020), RelEx (Zhang, DeFazio, and Ramesh 2021), GraphSVX (Duval and Malliaros‚Ä¶",2,positive
"More specifically, these explanation methods can be broadly categorized into i) gradient-based methods [5] that leverage the gradients of the GNN model to generate explanations; ii) perturbation-based methods [9, 11, 13] that aim to generate explanations by calculating the change in GNN predictions upon perturbations of the input graph structure (nodes, edges, or subgraphs); and iii) surrogate-based methods [7, 12] that fit a simple interpretable model to approximate the predictive behavior of the given GNN model.",1,neutral
"While several categories of GNN explanation methods have been proposed: gradient-based [5, 10, 14], perturbation-based [8, 9, 11, 13, 15], and surrogatebased [7, 12], their utility is limited to generating post hoc node- and edge-level explanations for a given pre-trained GNN model.",1,neutral
"In the realm of learning on graphs, some existing works aim to interpret GNNs (Ying et al. 2019; Luo et al. 2020; Yuan et al. 2020a), and they mainly focus on understanding the utility (e.",1,neutral
"In the realm of learning on graphs, some existing works aim to interpret GNNs (Ying et al. 2019; Luo et al. 2020; Yuan et al. 2020a), and they mainly focus on understanding the utility (e.g., node classification accuracy) of GNNs on the test set.",1,neutral
"Notable ones include GNNExplainer [40], PGExplainer [20], and Zorro [12].",1,neutral
"In recent years, people have put forward several methods to explain GNNs prediction, such as XGNN [30], GNNExplainer [9], PGExplainer [26], etc.",1,neutral
"Disturbance based methods [9], [18], [25], [26] monitor the changes of predicted values under different input disturbances, to learn the importance score of input characteristics.",1,neutral
"PGExplainer uses the generation probability model of graph data, which can learn concise potential structure from the observed graph data [26].",2,positive
"Take PGExplainer as an example [26], which provides explanation for each instance from the global perspective of GNNs model.",1,neutral
"A variant of this method, the PGExplainer [63] uses a deep neural network to parameterize the generation process of the masks and the explanations in general, thereby explaining multiple instances of sub-graphs at the same time.",1,neutral
"The last two datasets are widely used to benchmark explanation methods [15, 17] for graph classification.",1,neutral
"In 2020, PGExplainer [16] was proposed, providing a global understanding of predictions made by GNNs.",2,positive
"In 2020, PGExplainer [16] was proposed, providing a global understanding of predictions made by GNNs. PGMExplainer was also introduced in 2020 [17], which is based on perturbation of the original graph to eliminate unimportant variables from input‚Äìoutput data, employing explanation Bayesian networks as the last step.",2,positive
"Recently, various GNN explanation [15], [16], [38], [39] or graph denoising [17] techniques have been developed to find the most informative structures.",1,neutral
PGExplainer [9] proposed a probabilistic graph generative model to provide explanations of multiple instances collectively.,1,neutral
") input graph that contributed most towards the underlying GNN model‚Äôs prediction [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].",1,neutral
", [5], [6], [9], [13]), model-level explanations for GNNs have been largely underexplored in the literature.",1,neutral
"On the other hand, instead of directly searching for subgraphs, several studies [9], [10], [12] were shown to learn parameterized models for explanations of GNN‚Äôs predictions.",1,neutral
"This approach generalizes PGExplainer (Luo et al., 2020) and learns a node mask m ‚àà [0, 1]n for each sample C. Similar to BernMask, it also has mask size and entropy constraints in its loss function, and their coefficients are both tuned from {1.0, 0.1, 0.01} as well.",1,neutral
BernMask-P is extended from Luo et al. (2020) based on the authors‚Äô code and a recent PR in PyG.,2,positive
"Some methods to interpret graph neural networks can be applied to geometric data (Ying et al., 2019; Luo et al., 2020; Schlichtkrull et al., 2021; Yuan et al., 2021).",1,neutral
", 2017) were widely used for inherent interpretability, multiple recent studies show that they cannot provide reliable interpretation, especially for data with irregular structures (Serrano & Smith, 2019; Jain & Wallace, 2019; Ying et al., 2019; Luo et al., 2020).",1,neutral
"This approach generalizes PGExplainer (Luo et al., 2020) and learns a node mask m ‚àà [0, 1] for each sample C.",1,neutral
"Among them, BernMask and BernMask-P are post-hoc extended from two previous methods on graph-structured data, i.e., Ying et al. (2019) and Luo et al. (2020).",1,neutral
"‚Ä¶et al., 2015; Vaswani et al., 2017) were widely used for inherent interpretability, multiple recent studies show that they cannot provide reliable interpretation, especially for data with irregular structures (Serrano & Smith, 2019; Jain & Wallace, 2019; Ying et al., 2019; Luo et al., 2020).",1,neutral
"PgExpl: (Perturbation) [53]: The Parametrized Explainer for GNNs (PgExpl) [53] adopts a very similar formulation of the explanation problem as GnnExpl where the two major differences are: i) PgExpl provides solely explanations in terms of subgraph structures, neglecting explanations in terms of node features; ii) instead of directly optimizing continuous edge and features masks as done by GnnExpl, it uses Gradient Descend to train a MLP which, given the two concatenated node embeddings [X (t) i ||X (t) j ], predicts the likelihood of the edge (i, j) being a relevant edge.",1,neutral
"[95] proposed a categorization of explainers into four categories: gradient-based which exploit gradients of the input neural network [79, 80, 57]; perturbation-based where perturbations of the input graphs are aimed at obtaining explainable subgraphs [91, 53, 24, 69]; decomposition-based which try to decompose the input identifying the explanations [6, 57, 70]; and surrogate-based where a simple interpretable surrogate model is used to explain the original neural network [38, 101, 84].",1,neutral
"Independently from this categorization, a further fundamental distinction is among explainers providing explanations in terms of edge [91, 53, 69, 96] or node masks [79, 80, 57, 6, 57, 70].",1,neutral
"Roughly speaking, gradient-based explainers exploit gradients of the input neural network [79, 80, 57], perturbation-based models perturb the input aiming to obtain explainable subgraphs[91, 53, 24, 69], decomposition-based models try to decompose the input identifying the explanations [6, 57, 70], while surrogate-based models use a simple interpretable surrogate to explain the original neural network [38, 101, 84].",1,neutral
"However, their study is limited to node classification and the three explainers under analysis [91, 53, 69] are not well representative of the diversity of explanation strategies that have been proposed, as summarized in the aforementioned taxonomy [95].",1,neutral
"[91], is a widely used dataset for benchmarking GNN explainers [91, 53, 75, 95, 96, 104].",1,neutral
"Following [7, 8], our explanation framework concentrates on finding important factors from graph structures, node features, and edge features that affect predictions the most.",2,positive
"We chose one instance from each dataset and visualized explanations provided by SCALE, GNNExplainer, and PGExplainer in Figure 3.",0,negative
"BA-2motifs (BA-2m) [8] consists of 1000 graphs with two classes constructed by adding specific motifs to BA graphs, where half contain 5-node house motifs and the other half include 5-node cycle motifs.",1,neutral
"Besides, post-hoc explanation methods [7, 8] often transform node classification problems into graph classification problems via subgraph (Khop) sampling.",1,neutral
Its performance is also comparable to PGExplainer on the BA-2motifs dataset.,2,positive
"Specifically, it achieves outstanding precision and recall scores in node classification datasets and outperforms state-of-the-art methods GNNExplainer and PGExplainer.",2,positive
"(5) Inspired by [8], the mask matrix is initialized via an MLP network in which inputs are edge embedding vectors constructed by concatenating embedding vectors of source and target nodes taken from the black-box GNN model.",1,neutral
The performance gains are up to 94x compared to GNNExplainer and 120x in comparison with PGExplainer.,0,negative
"[7], is an active research area with several following research papers [8, 22, 23].",1,neutral
"Their proposed solutions for GNN explanations are too straightforward, making it hard to achieve significant results on other datasets, such as the ones in [7, 8].",1,neutral
"Among them, perturbation methods such as GNNExplainer [7] and PGExplainer [8] have been widely adopted since they introduced benchmark datasets for explanation tasks and achieved outstanding results.",1,neutral
"For fair comparisons, we contacted the authors of GNNExplainer, PGExplainer, and SEGNN to request evaluation scripts for all datasets.",2,positive
"Even though GNNExplainer and PGExplainer can also present explanations on multiple levels by adjusting the visibility threshold, setting this value too low may result in outputs with multiple disconnected components due to the independence of edge selections.",1,neutral
"Second, we compared our framework with two state-of-the-art post-hoc explanation methods [7, 8] in qualitative aspects to highlight the quality of explanations provided by SCALE.",2,positive
‚Ä¢ PGExplainer [8] shared the same approach with GNNExplainer [7] but initialized masks using embedding vectors from the pre-trained model.,2,positive
"Even though GNNExplainer and PGExplainer can highlight impactful edges of target nodes in node classification explanations, they cannot differentiate the contributions of these edges since edge weights only represent selection probabilities.",1,neutral
"SCALE outperforms baselines on Mutag, wherein the precision score gains are 15.54% compared to PGExplainer and 51.52% in comparison with GNNExplainer.",0,negative
Other baselines except PGExplainer were also executed using the same PyTorch version.,0,negative
9.1 for experiments with PGExplainer.,0,negative
"Therefore, we follow [7, 8] to formulate structural explanations as binary classification tasks, wherein influential nodes and edges of predictions are included in explanations.",1,neutral
"Typical post-hoc interpretation methods include approximation-based methods [2], relevance-propagationbased methods [34, 48], perturbation-based methods [25, 46], and decomposition methods [8].",1,neutral
"Other works have explored post-hoc analysis for explainable predictions in GNNs, notably, GNNExplainer Ying et al. [2019], PGExplainer Luo et al. [2020], PGM-Explainer Vu and Thai [2020], and SubgraphX Yuan et al. [2021] have all been developed for this purpose.",2,positive
"[2019], PGExplainer Luo et al. [2020], PGM-Explainer Vu and Thai [2020], and SubgraphX Yuan et al.",2,positive
"[2019], PGExplainer Luo et al. [2020], PGM-Explainer Vu and Thai [2020], and SubgraphX Yuan et al. [2021] have all been developed for this purpose.",2,positive
"The authors in [2] limited 250 their analysis to graphs that were containing the ground truth motifs, and proposed to just keep the 251 top-k edges.",1,neutral
1 Training the GNN f 236 For both BAMultiShapes and Mutagenicity we relied on the codebase provided by [2] for training 237 the GNN f to explain and to train the Local Explainer.,0,negative
"For Mutagenicity we replicated the setting in the PGExplainer paper [21], while for BAMultiShapes and HIN we trained until convergence a 3-layer GCN. Details about the training of the networks and their accuracies are in the Appendix.",0,negative
The results for Mutagenicity are in line with the one reported in [2].,0,negative
"1, 2, 3 144 [2] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang 145 Zhang.",1,neutral
"2 Local Explanations Processing 245 As detailed in [2], the output of PGExplainer consists in a weighted edge mask wij ‚àà V √ó V where 246 each wij is the likelihood of the edge being an important edge.",1,neutral
"For Mutagenicity, we sticked to the 247 original implementation which was correctly able to reproduce the results presented in the paper [2].",2,positive
"For Mutagenicity, over which PGExplainer was originally evaluated, we simply selected the threshold Œ∏ that maximises the F1 score of the local explainer over all graphs, including those that do not contain the ground-truth motif.",2,positive
"Nonetheless, in this work, we relied on PGExplainer [2] since it allows 57 the extraction of arbitrary disconnected motifs as explanations and it gave excellent results in our 58 experiments.",2,positive
"Finally, for BAMultiShapes and HIN, for which we extracted our own local explanations, we trained PGExplainer on the train split of the original dataset.",0,negative
"01 For Mutagenicity we replicated the model accuracy and the local explanations presented in [2], while 116 for BAMultiShapes we trained until convergence a 3-layers GCN.",0,negative
"For BAMultiShapes we trained a 3-layers 238 GCN [27] (20-20-20 hidden units) with mean graph pooling for the final prediction, whereas for 239 Mutagenicty we reproduced the results of [2].",2,positive
"In this work we relied mainly on two off-the-shelf explainers, namely, PGExplainer [21] and XGNN [36].",2,positive
"PGExplainer: For Mutagenicity and BAMultiShapes, we used the original implementation as provided by [21].",2,positive
"[35] introduced a prior to graph, and sampled a new graph from observed one via uniform distribution.",1,neutral
"Recently, some initial efforts [14, 22, 38, 41, 43] have been taken to address the explainability issue of GNNs.",1,neutral
PGExplainer [22] proposes to combine the global view of GNNs to facilitate the extraction of important graphs by applying a parameterized explainer.,2,positive
‚Ä¢ PGExplainer [22]: It adopts a MLP-based explainer to obtain the important subgraphs from a global view to reduce the computation cost and obtain better explanations.,2,positive
"Recently, some works in explainability of GNNs are emerging [3, 10, 22, 25, 38, 41].",1,neutral
", 2019], PGE-Explainer [Luo et al., 2020], GradCAM [Pope et al.",2,positive
"To this end, we use the BA2Motifs dataset [Luo et al., 2020].",2,positive
"We compare the proposed method with popular post-hoc explanation techniques including the GNN-Explainer Ying et al. [2019], PGE-Explainer Luo et al. [2020], GradCAM Pope et al. [2019], GNN-LRP Schnake et al. [2020], and SubgraphX Yuan et al. [2021]2.",2,positive
are different from those reported in the original paper [6].,0,negative
"On the other hand, a more recent method called PGExplainer [6] can learn how to sample subgraphs that highlight the most relevant parts of the input that influence the GNN output.",1,neutral
"‚Ä¢ a synthetic dataset, BA-2motifs [6], which contains 1000 graphs divided into two classes according to the motif they contain: either a ‚Äúhouse‚Äù or a five-node cycle;",1,neutral
"The existing works can be classified into two categories: instance-level explanations [11, 12, 13] and model-level explanations [14].",1,neutral
"In recent years, several methods for Explainability in Graph Neural Networks have been proposed, such as XGNN [39] , gnnexplainer [38], PGExplainer [21] , etc.",1,neutral
"Perturbation-based methods [9, 21, 27, 34, 38] monitor the changes in predicted values under different input perturbations, thereby learning the importance scores of input features.",1,neutral
Many other works focus on providing local explanations which are more robust and faithful (Dai et al. 2022; Luo et al. 2020; Vu and Thai 2020).,2,positive
"These explanations can be given with respect to node attributes Ma ‚àà R, nodes Mn ‚àà R , or edges Me ‚àà RN√óN , depending on specific GNN explainer, such as GNNExplainer [14], PGExplainer [10], and SubgraphX [31].",1,neutral
"These explanations can be given with respect to node attributes Ma d‚àà , nodes ‚àà Mn N , or edges ‚àà √óMe
N N , depending on specific GNN explainer, such as GNNExplainer14, PGExplainer10, and SubgraphX31.",1,neutral
"We incorporate eight GNN explainability methods, including gradient-based: Grad29, GradCAM11, GuidedBP6, Integrated Gradients30; perturbation-based: GNNExplainer14, PGExplainer10, SubgraphX31; and surrogate-based methods: PGMExplainer13.",2,positive
"Still, this faithfulness is relatively weak, only 0.001 better than
Method GEA (‚Üë) GEF (‚Üì) GES (‚Üì) GECF (‚Üì) GEGF (‚Üì)
Random 0.148 ¬± 0.002 0.579 ¬± 0.007 0.920 ¬± 0.002 0.763 ¬± 0.003 0.023 ¬± 0.002
Grad 0.193 ¬± 0.002 0.392 ¬± 0.006 0.806 ¬± 0.004 0.159 ¬± 0.004 0.039 ¬± 0.003
GradCAM 0.222 ¬± 0.002 0.452 ¬± 0.006 0.263 ¬± 0.004 0.010 ¬± 0.001 0.020 ¬± 0.002
GuidedBP 0.194 ¬± 0.001 0.557 ¬± 0.007 0.432 ¬± 0.004 0.067 ¬± 0.002 0.021 ¬± 0.002
IG 0.142 ¬± 0.002 0.545 ¬± 0.007 0.727 ¬± 0.005 0.110 ¬± 0.003 0.021 ¬± 0.002
GNNExplainer 0.102 ¬± 0.003 0.534 ¬± 0.007 0.431 ¬± 0.008 0.233 ¬± 0.006 0.027 ¬± 0.002
PGMExplainer 0.133 ¬± 0.002 0.541 ¬± 0.007 0.984 ¬± 0.001 0.791 ¬± 0.003 0.096 ¬± 0.004
PGExplainer 0.194 ¬± 0.002 0.557 ¬± 0.007 0.217 ¬± 0.004 0.009 ¬± 0.000 0.029 ¬± 0.002
SubgraphX 0.324 ¬± 0.004 0.254 ¬± 0.006 0.745 ¬± 0.005 0.241 ¬± 0.006 0.035 ¬± 0.003
Dataset Method GEA (‚Üë) GEF (‚Üì)
Mutag
Random 0.044 ¬± 0.007 0.590 ¬± 0.031
Grad 0.022 ¬± 0.006 0.598 ¬± 0.030
GradCAM 0.085 ¬± 0.012 0.672 ¬± 0.029
GuidedBP 0.036 ¬± 0.007 0.649 ¬± 0.030
Integrated Grad (IG) 0.049 ¬± 0.010 0.443 ¬± 0.031
GNNExplainer 0.031 ¬± 0.005 0.618 ¬± 0.030
PGMExplainer 0.042 ¬± 0.007 0.503 ¬± 0.031
PGExplainer 0.046 ¬± 0.007 0.504 ¬± 0.031
SubgraphX 0.039 ¬± 0.007 0.611 ¬± 0.030
Benzene
Random 0.108 ¬± 0.003 0.513 ¬± 0.012
Grad 0.122 ¬± 0.007 0.262 ¬± 0.011
GradCAM 0.291 ¬± 0.007 0.551 ¬± 0.012
GuidedBP 0.205 ¬± 0.007 0.438 ¬± 0.012
Integrated Grad (IG) 0.044 ¬± 0.003 0.182 ¬± 0.010
GNNExplainer 0.129 ¬± 0.005 0.444 ¬± 0.012
PGMExplainer 0.154 ¬± 0.006 0.433 ¬± 0.012
PGExplainer 0.169 ¬± 0.007 0.375 ¬± 0.012
SubgraphX 0.371 ¬± 0.009 0.513 ¬± 0.012
Fl-Carbonyl
Random 0.087 ¬± 0.007 0.440 ¬± 0.26
Grad 0.132 ¬± 0.010 0.210 ¬± 0.021
GradCAM 0.005 ¬± 0.007 0.500 ¬± 0.026
GuidedBP 0.089 ¬± 0.010 0.315 ¬± 0.024
Integrated Grad (IG) 0.091 ¬± 0.007 0.174 ¬± 0.019
GNNExplainer 0.094 ¬± 0.009 0.423 ¬± 0.026
PGMExplainer 0.078 ¬± 0.008 0.426 ¬± 0.026
PGExplainer 0.079 ¬± 0.009 0.372 ¬± 0.025
SubgraphX 0.008 ¬± 0.002 0.466 ¬± 0.026
9Scientific Data | (2023) 10:144 | https://doi.org/10.1038/s41597-023-01974-x
random explanation.",0,negative
PGExplainer generates the least unstable explanations‚Äì35.35% less unstable explanations than the average instability across other GNN explainers.,0,negative
"We incorporate eight GNN explainability methods, including gradient-based: Grad [29], GradCAM [11], GuidedBP [6], Integrated Gradients [30]; perturbation-based: GNNExplainer [14], PGExplainer [10], SubgraphX [31]; and surrogate-based methods: PGMExplainer [13].",2,positive
(Ying et al. 2019; Luo et al. 2020; Yuan et al. 2021) propose to adopt an explanation method to figure out the causal relationship between the model‚Äôs inputs and outputs.,2,positive
"Explanation for Graph Neural Network In this task, we follow the setting in GNNExplainer [39] and PGExplainer [17] and construct four kinds of node classification datasets.",1,neutral
"For the explanation task, we follow the quantitative evaluation settings in GNNExplainer [39] and PGExplainer [17].",2,positive
"The majority of existing methods provide a factual explanation in the form of a subgraph of the original graph that is deemed to be important for the prediction [3,9,22,27,30,36,46,51,54].",1,neutral
"Datasets We perform the experiments on the same set of datasets as GNNExplainer [12], as subsequent research established them as benchmarks [13, 14, 21].",2,positive
"[13] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang.",0,negative
"Notice that we do not focus on other post-hoc explainability methods, such as GNNExplainer [12], PGExplainer [13] or PGMExplainer [14], as to the best of our knowledge GCExplainer is the only explainability method providing global concept-based explanations for GNNs.",2,positive
"In an attempt to alleviate this issue, the Parameterised Graph Explainer (PGExplainer, [13]) and the Probabilistic Graphical Model Explainer (PGM-Explainer, [14]) parametrize the process of generating explanations using deep neural networks to provide multi-instance explanations.",1,neutral
"For instance, the PGExplainer focuses on the explanation of the complete graph structures and provides a global understanding of predictions made by GNNs.",2,positive
", nodes, node features, and edges) to the true important truth [16,33,39] (see Eq.",1,neutral
[16] proposed a model-agnostic method of explainable GNNs called PGExplainer.,1,neutral
PGExplainer provides explanations for GNNs by generating a probabilistic graph.,1,neutral
"Then a new graph is generated that contains only the structure necessary for the decision making by GNNs.
Similar to the PGM-Explainer analysing the explained features from conditional probabilities, Luo et al. [16] proposed a model-agnostic method of explainable GNNs called PGExplainer.",1,neutral
"Qualitative analyses have been widely used in recent research, such as GNNExplainer [33], PGExplainer [16], GAN-GNNExplainer [14], etc.",1,neutral
"A robust interpretation method can provide similar explanations despite the presence of such attacks [16,39].",1,neutral
"Consequently, an increasing number of works are focussing on explaining [11, 12, 13, 14] the decisions of",1,neutral
"In terms of methodologies, several techniques based on input perturbations [11, 12, 13], input gradients[36, 37], causal techniques [34, 38, 33] as well as utilizing simpler surrogate models [14] have been explored.",1,neutral
"Explanations usually include the importance scores for nodes/edges in a subgraph (or node‚Äôs neighborhood in case of node-level task) and the node features [11, 12, 13].",1,neutral
"Another popular perspective is from sensitivity analysis, which perturbs minor component and evaluate its influence on the global level [32, 17].",1,neutral
PGExplainer [24] adopts a deep neural network to parameterize the generation process of explanations enabling the explanation of multiple instances collectively.,1,neutral
"A general paradigm to generate explanations for GNNs is to find an explanation graph G‚Ä≤ that has the maximum agreement with the label distribution on the original graph G = (V,E,W ), where G‚Ä≤ can be a subgraph [39] or other variations of G [24, 40].",1,neutral
"Although several methods have been proposed for GNN explanation [24, 36, 39], most of them focus on node-level prediction tasks and will produce a unique explanation for each subject when applied to graph-level tasks.",1,neutral
The perturbation-based approaches (6; 38; 20; 41; 27) learns the important features and structural information by observing the predictive power of the model when noise is added to the input.,1,neutral
"With the trained graph models, we quantitatively and qualitatively compare our FlowX with eight baselines, including GradCAM [34], DeepLIFT [43], GNNExplainer [27], PGExplainer [28], PGMExplainer [31], SubgraphX [29], GNNGI [37], GNN-LRP [37].",2,positive
"Second, several existing methods, such as GNNExplainer [27], PGExplainer [28], and GraphMask [36], explain GNNs by studying the importance of different graph edges.",1,neutral
"Thus, different methods have been proposed to explain the predictions of GNNs, such as GraphLime [30], GNNExplainer [27], PGExplainer [28], PGMExplainer [31], SubgraphX [29], TAGE [32], XGNN [26], and GraphSVX [33].",1,neutral
"Recently, several techniques have been proposed to explain GNNs, such as XGNN [26], GNNExplainer [27], PGExplainer [28], and SubgraphX [29], etc.",1,neutral
"For GNN Explainer and PGExplainer, we modified their objective function in a similar way as the attention-based explanation.",2,positive
"(3) Faithfulness Gap: how to obtain bias (fairness) explanations that are faithful to the GNN prediction? To ensure the obtained explanations reflect the true reasoning results based on the given GNN model, most existing works on the instance-level GNN explanation obtain explanations that encode as much critical information as possible for a given GNN prediction [22, 30, 39].",1,neutral
"prediction of a node based on its computation graph [22, 30, 39].",1,neutral
"The adopted existing GNN explanation approaches for adaptation include the attention-based GNN explanation [29], the gradient-based GNN explanation [29], and two state-of-the-art GNN explanation approaches (GNN Explainer [39] and PGExplainer [22]).",2,positive
"‚Ä¢ Existing GNN explanation models (e.g., GNN Explainer and PGExplainer) do not show any superior performance over other straightforward GNN explanation approaches such as Att and Grad.",1,neutral
"For the training of PGExplainer, the learning rate is set as 0.003.",0,negative
"The basic rationale is that if small perturbations lead to dramatic changes in the GNN prediction, then what has been perturbed is regarded as critical for the GNN prediction [22, 27, 32, 39, 42].",1,neutral
‚Ä¢ PGExplainer.,0,negative
"For PGExplainer, we adopt the implementations of [22].",2,positive
"For all adopted GNN explanation baselines (i.e., Att, Grad, GNN Explainer, and PGExplainer), we also adopt their released implementations for a fair comparison.",2,positive
"To evaluate how well the proposed framework can be generalized to different explanation backbones, we adopt GNN Explainer [39] and PGExplainer [22] as two backbones of explainers for evaluation.",2,positive
"Furthermore, most explanation methods for GNNs‚Äô focus on providing factual explanations [16,29,11].",1,neutral
PGExplainer [10] is also based on maximizing mutual information between a class label and a highly contributing graph towards GNN prediction.,1,neutral
"This
extra speed in explaining graphs comes at the cost of an offline training procedure in both CFGExplainer and PGExplainer.",0,negative
"PGExplainer [17], three state-of-the-art interpretability models that operate on graph neural networks.",1,neutral
"work [16], [17], [18], CFGExplainer explains the prediction by a particular classifier on a particular sample.",1,neutral
"While both CFGExplainer and PGExplainer require an offline training procedure, PGExplainer requires an input constructed from edge embeddings, as opposed to node embeddings that are used by CFGExplainer.",2,positive
"In this section, we provide a brief overview of Attributed Control Flow Graphs (ACFGs) used for malware classification, Graph Neural Networks (GNNs), and three graph based interpretability models used in our evaluations, namely GNNExplainer [16], SubgraphX [18] and PGExplainer [17].",2,positive
"This is in contrast to PGExplainer [17] that uses edge embeddings, where the",1,neutral
This is in contrast to our solution CFGExplainer and PGExplainer [17] which leverage global information to provide instance-level explanations.,2,positive
such as GNNExplainer [16] and PGExplainer [17] learn to identify subgraphs by perturbing the original graphs by means of identifying a mask for edges or features and then combining it with the original graph.,1,neutral
"Recently, there have been efforts in explaining GNN-based classification results [16], [17], [18] by identifying graph structures that contribute most towards classification.",1,neutral
"In this section, we evaluate the classification accuracy of equisized subgraphs produced by four GNN-based interpretability models ‚Äì CFGExplainer, GNNExplainer [16], SubgraphX [18] and PGExplainer [17].",2,positive
", G = Gs+‚àÜG) similar to works in [16], [17], where Gs is the subgraph that makes important contribution towards malware classification and ‚àÜG the subgraph containing the rest of the nodes.",1,neutral
"We have compared CFGExplainer against three state-ofthe-art GNN-oriented explainers, namely GNNExplainer [16], SubgraphX [18] and PGExplainer [17], using eleven malware families (Bagle, Bifrose, Hupigon, Ldpinch, Lmir, Rbot, Sdbot, Swizzor, Vundo, Zbot and Zlob) and one benign family.",2,positive
"In contrast, CFGExplainer and PGExplainer generate explanations in around 3.9 and 6.4 minutes, respectively.",0,negative
"PGExplainer [25] provides an inductive edge explanation method working on a set of graphs, by learning edge masks with a multi-layer neural network.",1,neutral
"On the other hand, several GNN explanation methods are proposed recently [23], [24], [25], [26], [27].",1,neutral
", GNNExplainer [26], PGExplainer [25], and GraphMask [27], apply an edgecentric strategy by identifying the important edges and regarding the constructed subgraph as the explanation result.",1,neutral
"BA-2motifs [25] is a motif-based synthetic dataset, each graph from which contains a fivenode house-like motif or a cycle motif.",1,neutral
"Following the existing works [25], [26], in consideration of relaxation, we adopt Bernoulli distribution P (Gs) = ‚àè (i,j)‚ààE P ((i, j)) for edge explanation, where P ((i, j)) is the probability of the edge (i, j)‚Äôs existence.",1,neutral
"We compare ILLUMINATI with the following baseline GNN explanation methods, GNNExplainer [26], PGMExplainer [23], and PGExplainer [25].",2,positive
"that contribute to the prediction, for example, by retaining important edges [25], [27].",1,neutral
"For simplicity, we assume that the selections of edges from the original graph G are conditionally independent to each other [Luo et al., 2020], that is Pw = ‚àèM i=1 Pwi .",1,neutral
"Furthermore, [Luo et al., 2020] presented PGExplainer to explain GLNNs-GNNs collectively and inductively.",1,neutral
"These explanation models work in a post-hoc fashion and can be roughly divided into perturbation-based [Yuan et al., 2020a; Luo et al., 2020; Ying et al., 2019], surrogate modelbased [Huang et al., 2020], and gradient-based [Pope et al., 2019].",2,positive
"We discuss these for the BA-2Motifs [31], Tree-Cycle [54], and MUTAG[10] datasets in Section 4.3, and others in Appendix C.",2,positive
[31] follow a similar idea but emphasize finding important edges and finding explanations for many predictions at the same time.,1,neutral
"We discuss these for the BA-2Motifs [31], Tree-Cycle [54], and MUTAG[10] datasets in Section 4.",2,positive
"We use the Infection and Negative Evidence benchmarks from Faber et al. [15], The BA-Shapes, Tree-Cycle, and Tree-Grid benchmarks from Ying et al. [54], and the BA-2Motifs dataset from Luo et al. [31].",2,positive
There are several works [11; 31; 54; 55; 56] that use the Mutagenicity dataset but call it MUTAG.,1,neutral
We observe this in the BA-2Motifs dataset.,2,positive
"For BA-shapes, we use node indices [400:700:5] following the choice in the previous work [37, 20, 32, 2].",1,neutral
More details on the implementaion of GNNExpl and PGExpl can be found in Appendix B.2.,2,positive
"For subgraph explanations, we include: 1) GNNExpl as it is the building block for many follow-up works; 2) a continuous version of GNNExpl (GNNExpl (soft)) where we do not drop edges with small importance scores and we instead consider it as providing a constinous and weighted adjacency matrix Asij ‚â§ 1 and 3) PGExpl.",2,positive
This paper mainly considers GNNExpl and PGExpl as the evaluation of SubgraphX using the default parameters from DIG [18] exceeds the timeout limit (10 minuteshttps://us06web.zoom.us/j/86767269620 per node) with our current computational resources (Titan RTX) on Cora.,2,positive
"In the context of GNNs, recent work has proposed numerous subgraph explanation techniques [37, 20, 32, 2], which aim to specifically leverage graph structure when identifying the node and edge features that are important for a prediction.",1,neutral
The sparsity of PGExpl shows the highest sensitivity against the model‚Äôs accuracy; 2) in both datasets all explanation methods become less faithful when the model becomes more accurate.,1,neutral
"1] and can be direclty usd as ‚Äúprobabilities"" as the attribution scores from GNNExpl and PGExpl.",0,negative
"Since PGExpl shares the same motivation with GNNExpl and only differs on techniques that 1) narrow down the searching space for the optimal As and 2) learn a dense layer to jointly explain a set of nodes, we therefore point the readers to Luo et al. [20] for details.",2,positive
"B.2 Implementation GNNExpl and PGExpl
We use the implementation on Pytorch Geometric for GNNExpl2.",2,positive
"3a and does not apply to GNNExpl and PGExpl as they are already in the range of [0, 1].",0,negative
PGExpl was originally built in TensorFlow.,2,positive
"The output of a subgraph explanation is a perturbed matrix of node features or/and an adjacency matrix with more zero entries, which are considered as the most relevant part of the input graph towards the output [37, 20, 32, 2].",1,neutral
"Example choices of L(Xs, As, F ) are mutual information, i.e. GNNExpl [37] and PGExpl [20], and Shapley Value, i.e. SubgraphX [2].",1,neutral
"In BAshapes, we find that training PGExpl with all points provides a slightly better results on ROC-AUC score so we train all nodes together.",2,positive
"a continuous mask [37] or `1 norm [20], for the size function s.",1,neutral
We use the default number of epochs for GNNExpl and PGExpl in the public repository.,2,positive
"3a, we find that the ROC-AUC scores for SoftGNNExpl and PGExpl are pretty high even when the model is in its early stage of training while the correlations of the rest methods between explanation AUC and the test accuracy are stronger.",0,negative
PGExpl is excluded because it does not attribute over node features.,2,positive
"GNNExpl [37] and PGExpl [20], and Shapley Value, i.",1,neutral
Experimental details and relevant plots are included in Appendix C.3) Our results suggest that SoftGNNExpl and PGExpl may not be sensitive enough to reflect the model‚Äôs performance.,0,negative
"Generally, they highlight the important patterns of the input graphs such as nodes [52, 95], edges [144, 78] and sub-graphs [147, 152] which are crucial for the model predictions.",1,neutral
PGExplainer [78] proposes to explain the predictions via edge masks.,2,positive
"Despite their great success, GNNs are generally treated as black-box since their decisions are less understood [144, 78], leading to the increasing concerns about the explainability of GNNs.",1,neutral
"On this account, we follow the Gumbel-Softmax reparametrization trick [35, 36] and relax the binary variables ei,j to a continuous edge weight variables √™i,j = œÉ((log ‚àí log(1‚àí ) + wi,j)/œÑ) ‚àà [0, 1], where œÉ(¬∑) is the sigmoid function; ‚àº Uniform(0, 1); œÑ is the temperature hyperparameter such that limœÑ‚Üí0 p(√™i,j = 1) = œÉ(wi,j); wi,j is the latent variables which is calculated by a neural network following previous work [10],",1,neutral
"(2) Parametric explanation methods [9, 24, 10, 25, 14] generate the explanatory subgraphs with a parametrized explainer model.",1,neutral
"Following the previous explanation works [9, 10], we leverage mutual information to measure the relevance and therefore formulate the explanation problem as argmaxS I(S;Z).",1,neutral
"To provide a global understanding of the model prediction, PGExplainer [10] generates the explanatory subgraphs with a deep neural network whose parameters are shared across the explained instances.",2,positive
"Despite their strengths, GNNs are usually treated as black boxes and thus cannot provide human-intelligible explanations [9, 10].",1,neutral
"Following this strategy, previous explanation methods for supervised setting, such as GNNExplainer [9], PGExplainer [10], PGM-Explainer [14], and Refine [25], could be adopted as baselines.",2,positive
", GNNExplainer [22], PGExplainer [56] XGNN [132], RG-Explainer [152], OrphicX [153]) or counterfactual reason-",1,neutral
GNNExplainer [22] Explainability Perturbation-based Grey-box Instance/Group NC/GC Edge/Feature PGExplainer [56] Explainability Perturbation-based Grey-box Instance NC/GC Edge ZORRO [159] Explainability Perturbation-based Grey-box Instance NC Node/Feature Causal Screening [149] Explainability Perturbation-based Grey-box Instance GC Edge GraphMask [160] Explainability Perturbation-based White-box Instance SRL/MQA Edge SubgraphX [161] Explainability Perturbation-based Black-box Instance NC/GC Subgraph CF-GNNExplainer [162] Explainability Perturbation-based Grey-box Instance NC Edge RCExplainer [136] Explainability Perturbation-based Grey-box Instance NC/GC Edge ReFine [163] Explainability Perturbation-based Grey-box Instance GC Edge CF2 [164] Explainability Perturbation-based Grey-box Instance NC/GC Edge/Feature,0,negative
", PGExplainer [56]) provide a sample-dependant explanation for each graph sample.",1,neutral
", subgraphs) can reveal which components in molecule graphs support the final biochemical functionality predictions of GNNs [56].",1,neutral
embedding in PGExplainer [56]) because access to the GNN system may be limited (e.,1,neutral
"ture; (2) masks or attention scores of edges [16], [17], [18], which are derived from the masking functions or attention networks to approximate the target prediction via the fractional (masked or attentive) graph; or (3) prediction changes on perturbed edges [4], [19], [20], which are fetched by per-",1,neutral
"cient to offer a global understanding of the GNN, such as class-wise explanations [17], [18].",2,positive
"In this work, we follow the prior studies [14], [16], [17] and focus mainly on the structural features (i.",1,neutral
"Masks or attention scores of structural features [16], [17], [18]: The basic idea is to maximize the mutual",1,neutral
"ers, covering the gradient-based (SA [13], Grad-CAM [14]), masking-based (GNNExplainer [16]), attention-based (PGExplainer [17]), and perturbation-based (CXPlain [20], PGM-",1,neutral
"and effective description of the GNN [117], [118], [119].",1,neutral
PGExplainer [14] was proposed to learn a mask predictor to obtain the edge masks for providing explanations.,2,positive
"Consistent with prior works [11, 14, 34], we focus on explanations on graph structures.",1,neutral
"Other prior works, including GNNExplainer [32], PGExplainer [14], PGM-Explainer [25], SubgraphX [35], GraphMask [23], XGNN [34] and others [21] are provided in Appendix A.",2,positive
"They are GNNExplainer [32], PGExplainer [14], and Gem [11]3.",1,neutral
"We employ several real-world datasets ENZYMES, Mutagenicity, PC-3, NCI109, NCI-H23H [40] and one synthesized dataset BA-2Motifs [39] for graph classification.",2,positive
"Inspired by the recent research on GNN explainability [38], [39], we employ all perturbation operation DG 2 T1√∞G√û of the clean graph G (i.",2,positive
"%) on BA-2Motifs
current samples unseen samples
k 1 2 3 1 2 3
Clean 99.63 99.63 99.63 100.00 100.00 100.00 Rand 78.85 54.35 50.09 81.00 55.35 50.00 Grad 60.50 51.13 50.13 - - - RL-S2V 50.00 50.00 50.00 50.00 50.00 50.00 Ours 50.00 50.00 50.00 50.00 50.00 50.00
Authorized licensed use limited to the terms of the applicable license agreement with IEEE.",0,negative
"To reveal the attack strategies of the projective ranking method, we visualize the perturbations in adversarial samples on the BA-2Motifs dataset based on the GCN model.",2,positive
BA-2Motifs is a synthetic dataset in which two motifs (House and Pentagon) are attached on random base graphs.,1,neutral
"nicity, PC-3, NCI109, NCI-H23H [40] and one synthesized dataset BA-2Motifs [39] for graph classification.",1,neutral
"based [23]‚Äì[25], perturbation-based [26]‚Äì[29], and surrogate methods [30]‚Äì[32].",1,neutral
"In a high-level view, recent state-of-the-art GNN explanation methods are based on either factual reasoning [24, 43] or counterfactual reasoning [22, 23].",1,neutral
"[24] requires explicit motif to generate explanations thus could not be applied on NCI1 and CiteSeer, which is the reason why it is not included.",0,negative
[24] assumed that the nitro group (NO2) and amino group (NH2) are the true reasons for mutaginicity and filtered out the mutagens that do not contain them.,1,neutral
"To solve the problem, PGExplainer [24] adopts the reparameterization trick and learns approximate discrete masks that maximizes the mutual information between key structures and predictions, and XGNN [44] generates a graph based on reinforcement learning to approximate the prediction of the original graph.",1,neutral
"To restrict the size of subgraphs given by the explainer, we follow previous studies [15] to add a size regularization term R, computed as the averaged importance score, to the above objectives.",1,neutral
"In contrast, PGExplainer [15] performs inductive learning, i.",1,neutral
"Whereas existing studies focus on designing optimization approaches [34, 36] and explainer architectures [15] under the typical task-specific setting, our work focuses on an orthogonal problem to enable task-agnostic explanations with the proposed framework including the universal embedding explainer and conditioned learning objectives.",2,positive
Learning Inductive Task-agnostic # explainers required Gradient- & Rule-based No 1 GNNExplainer [34] Yes No No M ‚àóN SubgraphX [36] Yes No No M ‚àóN PGExplainer [15] Yes Yes No M Task-agnostic explainers Yes Yes Yes 1,0,negative
"In our study, we follow [15], focusing on the importance of edges to provide explanations to GNNs.",2,positive
"We do this by comparing TAGE with multiple baseline methods including non-learning-based methods GradCAM [20] and DeepLIFT [21], as well as learning-based methods GNNExplainer [34] and PGExplainer [15].",2,positive
"In particular, perturbation methods involve learning or optimization [12, 13, 15, 34, 36] and, while bearing higher computational costs, generally achieve state-of-the-art performance in terms of explanation quality.",1,neutral
"Previous works on explainability in graph neural networks include gradient-based methods (Pope et al., 2019), graph decomposition (Schnake et al., 2021) graph perturbations (Ying et al., 2019; Luo et al., 2020), and a local approxima-
tion using simpler models (Huang et al., 2020).",1,neutral
", 2021) graph perturbations (Ying et al., 2019; Luo et al., 2020), and a local approximation using simpler models (Huang et al.",1,neutral
"Based on the synthetic BAMotif graph classification task [58, 104] shown in Fig.",1,neutral
"To begin with, we construct 3-class synthetic datasets based on BAMotif [58] and follow Wu et al. [104] to inject spurious correlations between motif graph and base graph during the generation.",2,positive
"To begin with, we construct 3-class synthetic datasets based on BAMotif [58] and follow Wu et al.",2,positive
"We construct 3-class synthetic datasets based on BAMotif [116, 58] following [104], where the model needs to tell which one of three motifs (House, Cycle, Crane) that the graph contains.",2,positive
"Furthermore, we believe that a comparison of different explainability techniques like [65], [66], [67], [68] will provide even more insight into the differences between DP and non-DP training, which we also intend to investigate in future work.",2,positive
"Besides the models (Ying et al., 2019; Luo et al., 2020; Schlichtkrull et al., 2021; Yu et al., 2021) that we have compared with in detail in Sec.",2,positive
"Other Related Works Besides the models (Ying et al., 2019; Luo et al., 2020; Schlichtkrull et al., 2021; Yu et al., 2021) that we have compared with in detail in Sec.",2,positive
"For Mutag, we split it randomly into 80%/20% to train and validate models, and following (Luo et al., 2020) we use mutagen molecules with -NO2 or -NH2 as test data (because only these samples have explanation labels).",2,positive
"Many works have been recently proposed to extract critical data patterns for the prediction by interpreting GNNs in post-hoc ways (Ying et al., 2019; Yuan et al., 2020a; Vu & Thai, 2020; Luo et al., 2020; Schlichtkrull et al., 2021; Yuan et al., 2021; Lin et al., 2021; Henderson et al., 2021).",2,positive
"We compare interpretability with post-hoc methods GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), GraphMask (Schlichtkrull et al., 2021), and inherently interpretable models DIR (Wu et al., 2022) and IB-subgraph (Yu et al., 2021).",2,positive
"Table 6 shows a direct comparison with PGExplainer and GNNExplainer between the interpretation ROC AUC reported in (Luo et al., 2020) and the performance of GSAT.",0,negative
"Almost all previous GNN interpretation methods are posthoc, such as GNNExplainer (Ying et al., 2019), PGEx-
plainer (Luo et al., 2020) and GraphMask (Schlichtkrull et al., 2021).",2,positive
"BA-2Motifs (Luo et al., 2020) is a synthetic dataset, where the base graph is generated by BarabaÃÅsi-Albert (BA) model.",1,neutral
"Over Ba-2Motifs and Mutag, GNNExplainer and PGExplainer work worse than what reported in (Luo et al., 2020) as we do not cherry pick the pre-trained model.",0,negative
"Further Comparison on Interpretation Mechanism PGExplainer and GraphMask also have stochasticity in their models (Luo et al., 2020; Schlichtkrull et al., 2021).",2,positive
"Following (Luo et al., 2020), -NO2 and -NH2 in mutagen graphs are labeled as ground-truth explanations.",1,neutral
"For post-hoc methods, the pre-trained models are also trained with 10 different random seeds instead of a fixed pre-trained model in (Luo et al., 2020).",1,neutral
"BA-2Motifs (Luo et al., 2020) is a synthetic dataset with binary graph labels.",1,neutral
"We use the tuned recommended settings from (Luo et al., 2020), including the temperature, the coefficient of `1-norm regularization and the coefficient of entropy regularization.",2,positive
"PGExplainer and GraphMask also have stochasticity in their models (Luo et al., 2020; Schlichtkrull et al., 2021).",2,positive
"GSAT is substantially different from previous methods, as we do not use any sparsity constraints such as `1-norm (Ying et al., 2019; Luo et al., 2020), `0-norm (Schlichtkrull et al.",2,positive
"For interpretation evaluation, we report explanation ROC AUC following (Ying et al., 2019; Luo et al., 2020).",0,negative
", 2019), PGExplainer (Luo et al., 2020), GraphMask (Schlichtkrull et al.",2,positive
"GSAT is substantially different from previous methods, as we do not use any sparsity constraints such as `1-norm (Ying et al., 2019; Luo et al., 2020), `0-norm (Schlichtkrull et al., 2021) or `2-regression to {0, 1} (Yu et al., 2021) to select size-constrained (or connectivity-constrained)‚Ä¶",2,positive
"plainer (Luo et al., 2020) and GraphMask (Schlichtkrull et al.",1,neutral
BA2Motifs [25] contains graphs with a Barabasi-Albert (BA) base graph of size 20 and a 5-node motif in each graph.,1,neutral
"In fact, GNNExplainer, PGExplainer, and SubgraphX can never generate explanations including only disconnected -",1,neutral
PGExplainer [25] uses the same scoring function as [41] but generates a discrete mask on edges by training an edge mask predictor.,1,neutral
"We compare with 5 strong baselines representing the SOTA methods for GNN explanation: GNNExplainer [41], PGExplainer [25], SubgraphX [44], GraphSVX [9], and OrphicX [21].",2,positive
"Moreover, many approaches [Ying et al., 2019, Luo et al., 2020, Schlichtkrull et al., 2021] involve the perturbation of graphs by means of atom removal, masking the graphs or edge cutting, but they do not take into account the fact that, in chemistry, removing an atom or a bond from a molecule‚Ä¶",1,neutral
"Towards this end, a variety of explainer models are proposed for feature attribution (Selvaraju et al., 2017; Ying et al., 2019; Luo et al., 2020; Vu & Thai, 2020), which decomposes the predictor‚Äôs prediction as contributions (i.",2,positive
"Explainability of graph neural networks (GNNs) (Hamilton et al., 2017; Dwivedi et al., 2020) is crucial to model understanding and reliability in real-world applications, especially when about fairness and privacy (Ying et al., 2019; Luo et al., 2020).",1,neutral
"A prevalent solution is building an explainer model to conduct feature attribution (Ying et al., 2019; Luo et al., 2020; Pope et al., 2019).",2,positive
"Specifically, Gs can be a structure-wise (Ying et al., 2019; Luo et al., 2020) or featurewise (Ying et al.",1,neutral
"Another line (Luo et al., 2020; Ying et al., 2019; Yuan et al., 2020a; Yue Zhang, 2020; Michael Sejr Schlichtkrull, 2021) learns the masks on graph features.",1,neutral
"Towards this end, a variety of explainer models are proposed for feature attribution (Selvaraju et al., 2017; Ying et al., 2019; Luo et al., 2020; Vu & Thai, 2020), which decomposes the predictor‚Äôs prediction as contributions (i.e., importance) of its input features (e.g., edges, nodes).",2,positive
"Going beyond the instance-wise explanation, PGExplainer (Luo et al., 2020) generates masks for multiple instances inductively.",1,neutral
", 2020) is crucial to model understanding and reliability in real-world applications, especially when about fairness and privacy (Ying et al., 2019; Luo et al., 2020).",1,neutral
"By ‚Äúground-truth‚Äù, we follow the prior studies (Ying et al., 2019; Yuan et al., 2020a; Luo et al., 2020) and treat the subgraphs coherent to the model knowledge (e.",2,positive
"By ‚Äúground-truth‚Äù, we follow the prior studies (Ying et al., 2019; Yuan et al., 2020a; Luo et al., 2020) and treat the subgraphs coherent to the model knowledge (e.g., the motif subgraphs in TR3) or human knowledge (e.g., the digit subgraphs in MNISTsup) as the ground-truth explanations.",2,positive
"‚Ä¶of the input features, which redistributes the probability of features according to their importance and sample the salient features as an explanatory subgraph Gs. Specifically, Gs can be a structure-wise (Ying et al., 2019; Luo et al., 2020) or featurewise (Ying et al., 2019) subgraph of G.",1,neutral
"However, the PGExplainer explicitly works on edge masks and thus requires the GNN model to internally adjust edge weights, which was not applicable in our case.",2,positive
"for relevant subgraphs and their motifs [28], [29], walks [30]",1,neutral
"In fact, the mentioned problematic was recently addressed by a method called PGExplainer [29].",0,negative
"As a consequence, explanations may not reflect the global decisions made by the GNN classifier [29].",0,negative
"Here, also explanation techniques [49, 50] could yield interesting insights, in order to lead feature construction and modeling.",1,neutral
"We leverage existing works in network graphs and GNNs such as GNNExplainer and PGExplainer [11, 20] to mine common and influential substructures from sample input graphs.",2,positive
"These substructures are compact and summarize important behaviors of a GNN[9, 15], and existing works such as GNNExplainer and PGExplainer [9, 15] can extract substructures that are important to each GNN prediction (e.",1,neutral
"We compare VGIB with various explanation model including GNNExplainer [49], PGExplainer [28], GraphMask [37], IGExplainer [41], GraphGrad-CAM [31] and GIB [52].",2,positive
"In the explainability of Graph Neural Networks (GNNs), it is vital to generate the explanatory subgraph of the input, which faithfully interprets the predicted results [28, 49].",1,neutral
"To this end, several methods have been proposed to discover an important subgraph of the input for the explanation [35, 23, 37, 12].",1,neutral
"Note that M is a soft mask with continues values instead of a binary mask, we further employ an entropy constraint to encourage discrete values in M [23]:",1,neutral
"Furthermore, PGEXPLAINER learns a parametric model to generate explanations [23].",2,positive
"We apply a generative probabilistic model to learn intrinsic underlying molecular graph structures as the topology-level augmentations, which are believed to make the most contribution to molecule representations readout from GNNs. Simultaneously, MolCLE also learns feature selectors that mask out unimportant atom features to generate attribute-level augmentations.",2,positive
"Finally, we can notice that our MolCLE model shows a comparative performance to GROVER [47], which has the largest GNNs architecture tailored for molecular graph data with tens of millions of parameters.",2,positive
"Recently, considering molecule naturally as graph-structure data, many works [27, 56] explore the flourished GNNs to encode molecular graphs into low-dimensional representations.",1,neutral
"Previous work [35] imposes an alternative solution, namely budget constraint, which applies a hard threshold B to limit the size of explanatory graphs.",1,neutral
The intuition behind is obvious that the graph embedding with a well-augmented graph view as GNN‚Äôs input should be closest to the original one.,1,neutral
"Due to the intractable number of potential subgraphs hindering the model from optimizing the objective directly [35], we follow Janson et al.",1,neutral
"Thus, we also apply the element-wise entropy [35, 68] constraint to increase the",1,neutral
"The significance of this claim has been empirically proved and explicitly introduced [35, 68].",1,neutral
"Inspired by previous works on explaining GNNs [35, 68, 70], we utilize a generative probabilistic model to obtain topological augmentations.",1,neutral
"2.2 Graph Neural Networks Recent success of GNNs [8, 12, 29] have aroused many attention to use these methods for analyzing various graph-structured data.",1,neutral
"For example, GNNExplainer (Ying et al. 2019) and PGExplainer (Luo et al. 2020) are proposed to select a compact subgraph structure that maximizes the mutual information with the GNN‚Äôs predictions as the explanation.",2,positive
"2019), perturbationbased methods (Ying et al. 2019; Luo et al. 2020; Yuan et al. 2021; Schlichtkrull, De Cao, and Titov 2020), decomposition methods (Schwarzenberg et al.",1,neutral
We compare ProtGNN+ with two post-hoc methods: PGExplainer (Luo et al. 2020) and GNNExplainer (Ying et al. 2019).,2,positive
2019) and PGExplainer (Luo et al. 2020) are proposed to select a compact subgraph structure that maximizes the mutual information with the GNN‚Äôs predictions as the explanation.,2,positive
"‚Ä¶several classes: gradients/features-based methods (Baldassarre and Azizpour 2019; Pope et al. 2019), perturbationbased methods (Ying et al. 2019; Luo et al. 2020; Yuan et al. 2021; Schlichtkrull, De Cao, and Titov 2020), decomposition methods (Schwarzenberg et al. 2019; Schnake et al. 2020),‚Ä¶",1,neutral
We compare ProtGNN+ with two post-hoc methods: PGExplainer (Luo et al. 2020) and GNNExplainer (Ying et al.,2,positive
"Existing methods are GNNExplainer [13], PGExplainer [14], GraphMask [15].",2,positive
"Unfortunately, most existing methods that focus on edge-level or subgraph-level explanations such as GNNExplainer [13], PGExplainer [14],",1,neutral
"Unfortunately, most existing methods that focus on edge-level or subgraph-level explanations such as GNNExplainer [13], PGExplainer [14], and GraphMask [15] can not be used under the explanation supervision framework, as those explanations typically require additional objectives and optimization steps, making it not differentiable to the backbone model‚Äôs parameters.",2,positive
‚Ä¢ PGExplainer (PGExp) [23] trains a deep neural network to parameterize the generation of explanations.,1,neutral
"Overall, for the BA-Shape dataset, in [10] 19% of labels flip, in [18] 39% of labels flip and in [15] 18% of labels flip.",1,neutral
"with several papers [18,13,15] using state-of-the-art explainer method [10] and its evaluation protocol as benchmark.",2,positive
"We can observe three different types of masks that have been proposed, including soft masks (GNNExplainer [10], CF-GNNExplainer[18]), discrete masks (ZORRO [13]) and approximated discrete masks (PGExplainer [15])(3).",1,neutral
"For all 4 introduced explainer methods [10,18,13,15], the synthetic datasets BAShapes and Tree-Cycles are used for evaluation.",1,neutral
"In [13], accuracy is used, defined as the matching rate for important edges in explanations compared with those in the ground truthsd [15], an accuracy is formalized according to a binary classification task, where edges in the groundtruth explanation are treated as labels and the importance scores are viewed as prediction scores.",1,neutral
"Taking the classifying mutagenic property of a molecular graph (Luo et al., 2020) as an example, G is a molecular graph where the nodes are atoms and the edges are the chemical bonds between atoms, and Y is the class label, e.g., whether the molecule is mutagenic or not.",1,neutral
"‚Ä¶causes Z such as the factors to generate nitrogen dioxide (NO2) group, which has an determinative effect on the mutagenicity of molecule, and the effect of irrelevant variable M , such as the carbon ring which exist more frequently in mutagenic molecule but not determinative (Luo et al., 2020).",1,neutral
"Taking the classifying mutagenic property of a molecular graph (Luo et al., 2020) as an example, G is a molecular graph where the nodes are atoms and the edges are the chemical bonds between atoms, and Y is the class label, e.",1,neutral
", 1991), as we could explain the results based on the knowledge used in (Luo et al., 2020).",2,positive
"StableGNN correctly identifies chemicalNO2 andNH2, which are known to be mutagenic (Luo et al., 2020) while baselines fail in.",2,positive
"The whole molecular graph G is an effect of relevant latent causes Z such as the factors to generate nitrogen dioxide (NO2) group, which has an determinative effect on the mutagenicity of molecule, and the effect of irrelevant variable M , such as the carbon ring which exist more frequently in mutagenic molecule but not determinative (Luo et al., 2020).",1,neutral
"Moreover, we also conduct the experiments on a commonly used graph classification dataset, MUTAG (Debnath et al., 1991), as we could explain the results based on the knowledge used in (Luo et al., 2020).",2,positive
[19]) PGExplainer uses a so-called explanation network on a universal embedding of the graph edges to obtain a transferable version of the EM.,1,neutral
"For graph-specific approaches we consider GNNExplainer [35], PGExplainer [19], and Graph-LRP [24], which all have been specifically designed to provide insights on GNNs.",2,positive
"Following prior work [5, 22, 43], we formalize the notion of importance using mutual information (MI) and formulate our explanation module as the following optimization framework:",2,positive
"In addition to grid-like data, interpretable graph neural networks have been studied to provide explanatory subgraphs for instances [22, 43] or classes [44].",1,neutral
"Table 4 shows the results of three explainers by formalizing quantitative interpretation evaluation as a classification problem [22, 43].",1,neutral
"DatasetsWe employ one synthesized dataset BA-2Motifs [8] and five real-world datasets ENZYMES, Mutagenicity, PC-3, NCI109, NCI-H23H [11] for graph classification.",2,positive
", GNNExplainer [25], PGExplainer [17] and SubgraphX [28].",0,negative
"Explanation methods can be broadly categorized as model-level explainers [17], [20], [26], which try to extract global explanatory patterns from the trained model, and instance-level algorithms [5], [12], [16], [25], [28], which try to explain individual predictions performed by the model.",1,neutral
"Yet, the results of PGExplainer over the model trained with MATE are comparable with the ones presented in [17].",0,negative
"For the quantitative part, following [11], [17], [25] we compute the AUC score between the edges inside motifs, considered as positive edges, and the importance weights provided by the explanation methods.",1,neutral
"Concerning the explainers we use GNNExplainer [25], PGExplainer [17] and SubgraphX [28].",2,positive
PGExplainer [17] learns a parameterized model trained on the entire dataset to predict edge importance.,2,positive
"However, the authors of [17] observed that carbon rings exist in both mutagen and nonmutagenic graphs.",1,neutral
"For the graph explanation model E–¥ with parameters Œ∏–¥ , we use parameterized graph explainer (PGExplainer) model [19], minimizing the entropy loss:",1,neutral
"We use the recently proposed methods of concept learning [41] for explanations using textual data, and PGExplainer [19] for explanations using graphs data.",2,positive
"We use the recently proposed methods of concept learning [40] for explanations using textual data, and PGExplainer [18] for explanations using graphs data.",2,positive
"For the graph explanation model ùê∏ùëî with parameters ùúÉùëî , we use parameterized graph explainer (PGExplainer) model [18], minimizing the entropy loss: Lùê∏ùëî (ùúÉùëî) = Eùê∫ùëÜ‚àºùëû (ùúÉùëî) [ùêª (ùë¥ùíê (‚Ñé( ¬Æùë•),ùë¥ùíà (ùê∫)) | ùë¥ùíê (‚Ñé( ¬Æùë•),ùë¥ùíà (ùê∫ùëÜ )))]
where ùê∫ùëÜ is explanation subgraph sampled from a distribution ùëû parameterized by ùúÉùëî .",2,positive
PGExplainer [22] applies a parameterized explainer to generate the edge masks from a global view to identify the important subgraphs.,1,neutral
"data such as images and texts; while the work on interpretable GNNs for graph structured data are rather limited [14, 22, 45, 48].",1,neutral
‚Ä¢ PGExplainer [22]: It adopts a MLP-based explainer to obtain the important subgraphs from a global view to reduce the computation cost and obtain better explanations.,2,positive
"Some initial efforts [14, 22, 45] have been taken to address this problem.",1,neutral
"There are few attempts of post-hoc explainers [14, 22, 45, 48] to provide explanations for trained GNNs; while the work on self-explainable GNNs is rather limited.",1,neutral
"BA-Shapes: To compare with the state-of-the-art GNN explainers [22, 45] which identify crucial subgraphs for predictions, we construct BA-Shapes following the setting in GNNExplainer [45].",2,positive
"Furthermore, previous research in graphs tried attention-based explanation with weak results [18, 36] and other domains still investigate whether attention yields promising explanations without a definite answer yet.",1,neutral
"Follow-up works on GNN explanation pick up these datasets [13, 16, 18, 25, 31] or vary them slightly [7].",1,neutral
Note that similar observations on another representative explainer (PGExplainer [23]) can be found in Figure 7 (Refer to Appendix Section).,0,negative
"In this section, in order to evaluate the effectiveness of our proposed attacking method on both GNNs and its explanations, we apply our proposed method to another representative explainer for the GNNs model (PGExplainer [23]), which adopts a deep model to parameterize the generation process of explanations in the inductive setting.",2,positive
"Experimental results on two explainers (GNNEXPLAINER[20] and PGExplainer [23]) demonstrate that GEAttack achieves good performance for attacking GNN models, and adversarial edges generated by GEAttack are much harder to be detected by GNNEXPLAINER, which successfully achieve the joint attacks on a GNN model and its explanations.",2,positive
PGExplainer [23] is proposed to generate an explanation for each instance with a global understanding of the target GNN model in an inductive setting.,2,positive
"Experimentally, the objective function of GNNEXPLAINER can be optimized to learn adjacency mask matrix MA and feature selection mask matrix MF in the following manner [20, 23]:",1,neutral
"In order to explain why a GNN model fŒ∏ predicts a given node vi‚Äôs label as Y , the GNNEXPLAINER acts to provide a local interpretation GS = (AS ,XS) by highlighting the relevant features XS and the relevant subgraph structure AS for its prediction [20, 23].",1,neutral
"After all, the adversarial perturbations on graphs are highly correlated with the target label, since it is the perturbations that cause such malicious prediction [20, 23].",1,neutral
"In particular, given a trained GNN model and its prediction on a test node, GNNEXPLAINER [20, 23] will return a small subgraph together with a small subset of node features that are most influential for its prediction.",1,neutral
"PGExplainer (Luo et al., 2020) aims to address the issues of GNNExplainer.",2,positive
"Recent research has attempted to improve the understanding of GNNs by producing various explainability techniques (Pope et al., 2019; Baldassarre & Azizpour, 2019; Ying et al., 2019; Schnake et al., 2020; Luo et al., 2020; Vu & Thai, 2020).",2,positive
"We perform the experiments on the same set of datasets as GNNExplainer (Ying et al., 2019), as GNNExplainer has been established as a benchmark by subsequent research (Luo et al., 2020; Vu & Thai, 2020).",2,positive
"More recent research has focused on GNNspecific explainability techniques, which take into account the graph structure (Ying et al., 2019; Luo et al., 2020; Vu & Thai, 2020).",1,neutral
"Alternatively, works in (Ying et al. 2019; Vu and Thai 2020; Luo et al. 2020) focus on more complex approaches unique to GNN explainability, such as those based on mutual information maximisation, or Markov blanket conditional probabilities of feature explanations.",1,neutral
"Additionally, to understand how any graph neural networks (GNNs) make a certain decision on graphstructured data, GNNExplainer learns soft masks for edges and node features to explain the predictions via maximizing the mutual information between the predictions of the original graph and those of the newly obtained graph [240, 372].",1,neutral
"Explanations of graph neural networks have been conducted on a set of molecules graph-labeled for their mutagenic efect on the Gram-negative bacterium Salmonella typhimurium, with the goal of identifying several known mutagenic functional groups ÔøΩÔøΩ2 and ÔøΩÔøΩ2 [240, 372, 378].",1,neutral
"However, GNNs as a family of deep learning models are prone to overfitting and lack transparency in their predictions, which prevent their usage in decision-critical applications such as disease diagnosis.",1,neutral
"Previous methods usually produce a unique explanation subgraph for each graph subject (e.g. GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), and PGM-Explainer (Vu & Thai, 2020)), or through the model-level explanation (e.g. GAT (VelicÃåkovicÃÅ et al., 2018)), that cannot drive diseasespecific explanation.",1,neutral
"A general approach to generate explanations for GNNs is to find a explanation graph G‚Ä≤ that has the maximum mutual information with the label distribution Y , where G‚Ä≤ can be a subgraph of G (Ying et al., 2019) or other alternations of G (Luo et al., 2020; Schlichtkrull et al., 2020; Yuan et al., 2020).",1,neutral
"To unleash the power of GNNs in brain network analysis and enable their interpretability, we propose BrainNNExplainer.",2,positive
"Recently, Graph Neural Networks (GNNs) attract broad interests due to their established power in different downstream tasks (Kipf & Welling, 2017b; Xu et al., 2019; Velickovic et al., 2018; Yang et al., 2020a).",2,positive
"Previous methods usually produce a unique explanation subgraph for each graph subject (e.g. GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), and PGM-Explainer (Vu & Thai, 2020)), or through the model-level explanation (e.g. GAT (VelicÃåkovicÃÅ et al., 2018)), that cannot drive‚Ä¶",1,neutral
"Compared with traditional shallow models such as MK-SVM, our backbone BrainNN outperforms them by large margins, with up to 11% absolute improvements on BP, which demonstrates the potential of using deep GNNs on brain networks.",2,positive
"Although several approaches have been proposed to explain the predictions of GNNs (Ying et al., 2019; Luo et al., 2020; Vu & Thai, 2020; Yuan et al., 2020), none of them is equipped with a backbone GNN specifically designed for brain networks.",2,positive
", 2019), PGExplainer (Luo et al., 2020), and PGM-Explainer (Vu & Thai, 2020)), or through the model-level explanation (e.",2,positive
"The lack of predictive original ROI features limits the power of GNNs (Cui et al., 2021).",2,positive
"Since the brain region connectivity and correlations are encoded in realvalued edge weights, which can not be handled by existing GNNs, we design an edge-weight-aware message passing mechanism.",2,positive
"‚Ä¶approach to generate explanations for GNNs is to find a explanation graph G‚Ä≤ that has the maximum mutual information with the label distribution Y , where G‚Ä≤ can be a subgraph of G (Ying et al., 2019) or other alternations of G (Luo et al., 2020; Schlichtkrull et al., 2020; Yuan et al., 2020).",1,neutral
"Compared with shallow models, GNNs are promising for brain network analysis with more powerful representation abilities to capture the sophisticated brain network structures (Maron et al., 2018; Yang et al., 2019; 2020b).",1,neutral
", 2019) or other alternations of G (Luo et al., 2020; Schlichtkrull et al., 2020; Yuan et al., 2020).",1,neutral
PGExplainer [122] can also provided an explanation for each instance with a global view of the GNN model by incor-,2,positive
"Recently, Multi-Layer Perceptrons (MLPs) have extensively been used as an explanation network in masking-based explanation methods (Luo et al., 2020; Schlichtkrull et al., 2020) to identify the edges which are affecting final predictions the most.",1,neutral
"‚Ä¶Zitnik et al., 2018), and recent research has focused on developing methods to explain GNN predictions (Baldassarre and Azizpour, 2019; Pope et al., 2019; Ying et al., 2019; Huang et al., 2020; Luo et al., 2020; Vu and Thai, 2020; Schlichtkrull et al., 2021; Chen et al., 2021; Han et al., 2021).",2,positive
"‚Ä¶end, several approaches have been proposed in recent literature to explain the predictions of GNNs (Baldassarre and Azizpour, 2019; Faber et al., 2020; Huang et al., 2020; Lucic et al., 2021; Luo et al., 2020; Pope et al., 2019; Schlichtkrull et al., 2021; Vu and Thai, 2020; Ying et al., 2019).",2,positive
"Based on the techniques they employ, these approaches can be broadly characterized into perturbation-based (Luo et al., 2020; Schlichtkrull et al., 2021; Ying et al., 2019), gradient-based (Simonyan et al.",1,neutral
"Recently, perturbation-based methods (Ying et al., 2019; Luo et al., 2020; Schlichtkrull et al., 2021) explain GNN predictions by observing the change in model predictions w.r.t. different input perturbations to study node and edge importance.",1,neutral
"To this end, several approaches have been proposed in recent literature to explain the predictions of GNNs (Baldassarre and Azizpour, 2019; Faber et al., 2020; Huang et al., 2020; Lucic et al., 2021; Luo et al., 2020; Pope et al., 2019; Schlichtkrull et al., 2021; Vu and Thai, 2020; Ying et al., 2019).",2,positive
"In contrast to GNNExplainer, PGExplainer (Luo et al., 2020) generates explanation only on the graph structure.",1,neutral
"Recently, perturbation-based methods (Ying et al., 2019; Luo et al., 2020; Schlichtkrull et al., 2021) explain GNN predictions by observing the change in model predictions w.",1,neutral
"‚Ä¶(Simonyan et al., 2014), Integrated Gradients (Sundararajan et al., 2017); perturbation-based: GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), GraphMASK (Schlichtkrull et al., 2021); and surrogate-based methods: GraphLIME (Huang et al., 2020), PGMExplainer (Vu and Thai,‚Ä¶",2,positive
"15 is intractable (Ying et al., 2019; Luo et al., 2020).",1,neutral
", 2019), PGExplainer (Luo et al., 2020), GraphMASK (Schlichtkrull et al.",2,positive
"Based on the techniques they employ, these approaches can be broadly characterized into perturbation-based (Luo et al., 2020; Schlichtkrull et al., 2021; Ying et al., 2019), gradient-based (Simonyan et al., 2014; Sundararajan et al., 2017), and surrogate-based (Huang et al., 2020; Vu and Thai,‚Ä¶",1,neutral
", 2018), and recent research has focused on developing methods to explain GNN predictions (Baldassarre and Azizpour, 2019; Pope et al., 2019; Ying et al., 2019; Huang et al., 2020; Luo et al., 2020; Vu and Thai, 2020; Schlichtkrull et al., 2021; Chen et al., 2021; Han et al., 2021).",2,positive
"Although those methods are designed to highlight important nodes or edges of the input graph for the target decision, their explanations often require additional models to be trained for generating graph masks [14, 15].",1,neutral
"Several perturbation-based methods are proposed including GNNExplainer [14], PGExplainer [15], ZORRO [30], GraphMask [16], and Causal Screening [31].",1,neutral
"[15], which divides the dataset into 80%, 10%, and 10% portions for training, validation, and testing, respectively.",2,positive
"Dataset split is taken from the PGExplainer code [15], which splits train/validation/test sets by 80/10/10%.",0,negative
This style of edge dropping based on a random graph model has also been used for parameterized explanations of GNNs [58].,1,neutral
PGM and PGE are not included in the feature sparsity because they don‚Äôt retrieve feature masks.,1,neutral
"On the other side, PGE performs worst out of all masks-based explainers.",2,positive
"In terms of node sparsity, PGE outperforms all other soft-maskbased approaches.",1,neutral
PGExplainer [24] employs a parameterized model to generate soft edge masks with node representations (extracted from target GNN) as input.,1,neutral
"Explainability approaches for explaining node level decisions include soft-masking approaches [11], [22], [24], [31], [32], [44], Shapely based approaches [8], [48], surrogate",1,neutral
"PGE performs worst
Authorized licensed use limited to the terms of the applicable license agreement with IEEE.",0,negative
"First, we compare soft-masking approaches, i.e., gradient-based approaches, PGE, andGNNExplainer.",2,positive
"Unlike our approach, PGExplainer is not model agnostic.",2,positive
"Most of the existing approaches for explaining GNNs are based on soft-masking methods [11], [22], [24], [26], [32], [44].",1,neutral
"Since PGE and GNNExplainer return soft edge
TABLE 1 Analysis of the Average Sparsity (Definition 2), RDT-Fidelity (Definition 4), and Validity (Definition 1) of the Explanations
Metric Method Cora CiteSeer PubMed
GCN GAT GIN APPNP GCN GAT GIN APPNP GCN GAT GIN APPNP
Features-Sparsity GNNExplainer 7.27 7.27 7.27 7.27 8.21 8.21 8.21 8.21 6.21 6.21 6.21 6.21 Grad 4.08 4.22 4.45 4.08 4.19 4.28 4.41 4.18 4.41 4.51 4.89 4.46 GradInput 4.07 4.25 4.37 4.08 4.17 4.29 4.33 4.17 4.41 4.51 4.92 4.47 ZORRO √∞t ¬º :85√û 1.91 2.29 3.51 2.26 1.81 1.84 3.67 1.97 1.60 1.52 2.38 1.75 ZORRO √∞t ¬º :98√û 2.69 3.07 4.34 3.18 2.58 2.60 4.68 2.78 2.55 2.58 3.21 2.86
Node-Sparsity GNNExplainer 2.48 2.49 2.56 2.51 1.67 1.67 1.70 1.68 2.7 2.71 2.71 2.71 PGM 2.06 1.82 1.66 1.99 1.47 1.59 1.10 1.54 1.64 1.16 1.62 2.93 PGE 1.86 1.86 1.78 1.94 1.48 1.40 1.36 1.41 1.91 1.81 1.85 1.92 Grad 2.48 2.34 2.25 2.35 1.70 1.61 1.55 1.60 2.91 2.76 3.11 2.73 GradInput 2.53 2.43 2.23 2.41 1.61 1.58 1.54 1.52 3.02 2.94 3.41 2.81 ZORRO √∞t ¬º :85√û 1.28 1.30 1.90 1.16 1.05 0.92 1.36 0.83 1.07 0.87 1.77 0.79 ZORRO √∞t ¬º :98√û 1.58 1.59 2.17 1.48 1.26 1.09 1.58 1.07 1.51 1.31 2.18 1.25
RDT-Fidelity GNNExplainer 0.71 0.66 0.52 0.65 0.68 0.69 0.51 0.62 0.67 0.73 0.67 0.72 PGM 0.84 0.77 0.60 0.89 0.92 0.93 0.73 0.95 0.78 0.69 0.74 0.96 PGE 0.50 0.53 0.35 0.49 0.64 0.60 0.51 0.61 0.49 0.61 0.56 0.50 Grad 0.15 0.18 0.19 0.17 0.17 0.19 0.28 0.18 0.37 0.43 0.42 0.37 GradInput 0.15 0.18 0.18 0.16 0.16 0.18 0.26 0.17 0.36 0.42 0.42 0.36 Empty Explanation 0.15 0.18 0.18 0.16 0.16 0.18 0.26 0.17 0.36 0.42 0.42 0.36 ZORRO √∞t ¬º :85√û 0.87 0.88 0.86 0.88 0.87 0.86 0.87 0.86 0.86 0.88 0.88 0.87 ZORRO √∞t ¬º :98√û 0.97 0.97 0.96 0.97 0.97 0.97 0.97 0.96 0.96 0.97 0.97 0.96
Validity GNNExplainer 0.89 0.95 0.83 0.84 0.87 0.92 0.58 0.93 0.60 0.81 0.71 0.87 PGM 0.89 0.90 0.64 0.94 0.95 0.95 0.76 0.97 0.86 0.80 0.62 0.97 PGE 0.51 0.54 0.34 0.45 0.62 0.59 0.54 0.62 0.51 0.61 0.57 0.48 Grad 0.26 0.25 0.15 0.18 0.28 0.25 0.12 0.26 0.36 0.49 0.50 0.38 GradInput 0.22 0.22 0.12 0.17 0.18 0.16 0.08 0.19 0.36 0.49 0.50 0.37 Empty Explanation 0.22 0.22 0.11 0.17 0.18 0.16 0.08 0.19 0.36 0.49 0.50 0.37 ZORRO √∞t ¬º :85√û 1.00 1.00 0.83 1.00 1.00 1.00 0.77 1.00 0.90 1.00 0.84 1.00 ZORRO √∞t ¬º :98√û 1.00 1.00 0.90 1.00 1.00 1.00 0.91 1.00 0.98 1.00 0.87 1.00
The smaller the explanation size larger is the sparsity.",0,negative
"As PGE does not produce a feature mask, in other words, it selects all features, feature sparsity is not provided.",1,neutral
"For a comprehensive quantitative evaluation we chose our baselines from the three different categories of post-hoc explanations models consisting of (i) soft-masking approaches like GNNExplainer, which returns a continuous feature and edge mask and PGE [24] learns soft masks over edges in the graph (ii) surrogate model based hard-masking approach, PGM [40], which returns a binary node mask, iii) Shapely based hard masking approach SubgraphX [48], which returns a subgraph as an explanation, and (iv) gradient-basedmethodsGrad&GradInput [34] which utilize gradients to compute feature attribu-",2,positive
"We can conclude
TABLE 3 Experiments on Faithfulness According to [30] Measured With Kendall‚Äôs tau tKendall of the Retrieved Explanation Precision and Test Accuracy
Method 1 200 400 600 1400 2000 tKendall
GNNExplainer 0.50 0.54 0.41 0.40 0.37 0.40 0:73 PGM 0.83 0.47 0.68 0.71 0.76 0.75 0.20 PGE 0.20 0.19 0.23 0.21 0.23 0.20 0.36 Grad 0.94 0.80 0.62 0.73 0.84 0.87 0.07 GradInput 0.88 0.89 0.78 0.79 0.87 0.89 0.07 ZORRO √∞t ¬º :85√û 0.00 0.92 0.88 0.93 0.94 0.94 0.73 ZORRO √∞t ¬º :98√û 0.00 0.90 0.85 0.84 0.87 0.90 0.47 To simulate different model performances, we saved the GCN model during different epochs on the synthetic dataset.",0,negative
"For a comprehensive quantitative evaluation we chose our baselines from the three different categories of post-hoc explanations models consisting of (i) soft-masking approaches like GNNExplainer, which returns a continuous feature and edge mask and PGE [24] learns soft masks over edges in the graph (ii) surrogate model based hard-masking approach, PGM [40], which returns a binary node mask, iii) Shapely based hard masking approach SubgraphX [48], which returns a subgraph as an explanation, and (iv) gradient-basedmethodsGrad&GradInput [34] which utilize gradients to compute feature attributions.",2,positive
GraphLIME and PGExplainer use sampling without any approximation guarantees.,1,neutral
"In this paper, we approach the explanation problem from a new angle, proposing a unified view that regroups existing explainers under a single framework: GNNExplainer, PGExplainer, GraphLIME, PGM-Explainer, XGNN, and the proposed GraphSVX.",2,positive
"More recently, PGExplainer [21] and GraphMask [31] generalize GNNExplainer to an inductive setting; they use re-parametrisation tricks to alleviate the ‚Äúintroduced evidence‚Äù problem [6]‚Äî i.e. continuous masks deform the adjacency matrix and introduce new semantics to the generated graph.",1,neutral
"More recently, PGExplainer [16] and GraphMask [26] generalize GNNExplainer to an inductive setting; they use re-parametrisation tricks to alleviate the ‚Äúintroduced evidence‚Äù problem [5] ‚Äî i.",1,neutral
"We compare the performance of GraphSVX to the main explanation baselines that incorporate graph structure in explanations, namely GNNExplainer, PGExplainer and PGM-Explainer.",2,positive
"Overall, this often yields explanations with a poor signification, like a probability score stating how essential a variable is [16, 26, 33].",1,neutral
"Two other explainers provide even more general explanations: XGNN, a true model-level explanation method and PGExplainer, which provides collective and inductive explanations.",1,neutral
"In terms of efficiency, our explainer is slower than the scalable PGExplainer despite our efficient approximation, but is often comparable to GNNExplainer.",2,positive
"Hence, we expect PGExplainer to perform better.",2,positive
"Perturbation methods [16,26,33] monitor variations in model prediction with respect to different input perturbations.",1,neutral
"Comparing running times with baselines [21,42], GraphSVX is still slower than PGExplainer, which is very scalable, but matches GNNExplainer thanks to our efficient approximation, especially when the data is relatively sparse (e.g., BA-Shapes or Tree-Cycles).",2,positive
PGExplainer is very similar to GNNExplainer.,2,positive
"We follow the same setting as [16] and [33], where four kinds of datasets are constructed.",1,neutral
"GNNExplainer, XGNN, GraphLIME and PGExplainer are simply selective.",1,neutral
"Regarding other baselines, PGExplainer and GNNExplainer output probability scores, PGM-Explainer a bayesian network, and XGNN a subgraph without further information.",0,negative
"It supports only graph classification, requires passing a candidate node set as input and is challenged by local methods also providing global explanations [16].",1,neutral
"PGExplainer initialises randomly the parameters of the mask generator and uses intermediate model representations, which could respectively lead to local optimum and differences across similar models.",2,positive
"For this end, PGExplainer (Luo et al., 2020) learns a multilayer perceptron (MLP) to explain multiple instances collectively.",2,positive
"While explaining graph neural networks on graphs is still a nascent research topic, a few recent works have emerged (Luo et al., 2020; Vu & Thai, 2020; Ying et al., 2019; Yuan et al., 2020), each with its own perspective on this topic.",1,neutral
"We consider the state-of-the-art baselines that belong to the unified framework of additive feature attribution methods (The proof is provided in Appendix A) (Lundberg & Lee, 2017): GNNExplainer (Ying et al., 2019) and PGExplainer (Luo et al., 2020)2.",2,positive
"We do not include gradient-based method (Ying et al., 2019), graph attention method (VelicÃåkovicÃÅ et al., 2018), and Gradient (Pope et al., 2019), since previous explainers (Luo et al., 2020; Ying et al., 2019) have shown their superiority over these methods.",2,positive
"For fair comparisons, we report the results of PGExplainer following its setting reported in (Luo et al., 2020) and compare them with the results of GNNExplainer and Gem when explaining on mutagen graphs, indicated as PGExplainer0, GNNExplainer-0, and Gem-0 in Table 2.",0,negative
", 2019), since previous explainers (Luo et al., 2020; Ying et al., 2019) have shown their superiority over these methods.",1,neutral
"For fair comparisons, we report the results of PGExplainer following its setting reported in (Luo et al., 2020) and com-",0,negative
", 2019), PGExplainer (Luo et al., 2020), DeepLIFT (Shrikumar et al.",2,positive
"For data interfaces, we consider the widely used synthetic datasets (i.e., BA-shapes, BA-Community, etc.) (Ying et al., 2019; Luo et al., 2020) and molecule datasets (i.e., BBBP, Tox21, etc.) (Wu et al., 2018).",2,positive
"We include the following algorithms: GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), DeepLIFT (Shrikumar et al., 2017), GNN-LRP (Schnake et al., 2020), Grad-CAM (Pope et al., 2019), SubgraphX (Yuan et al., 2021), and XGNN (Yuan et al., 2020a).",2,positive
"Recently, more researches focus on the interpretations of GNN models, such as GraphLIME [76], CoGE [51], Counterfactual explanations on GNNs [18] and others [20, 111, 132].",1,neutral
", 2018] popularized by PGExplainer [Luo et al., 2020] and PTDNet [Luo et al.",2,positive
"Apart from the Gumbel-Softmax trick, other implementations of end-to-end discrete sampling include the Gumbel-Max trick used by AD-GCL [Suresh et al., 2021] and hard concrete sampling [Louizos et al., 2018] popularized by PGExplainer [Luo et al., 2020] and PTDNet [Luo et al., 2021a].",2,positive
"Meanwhile, PGExplainer (Luo et al., 2020) learns a parameterized model to predict whether an edge is important, which is trained using all edges in the dataset.",2,positive
"Then we compare our SubgraphX with several baselines, including MCTS GNN, GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020).",2,positive
"While several recent studies have developed GNN explanation methods, such as GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), and PGM-Explainer (Vu & Thai, 2020), they invariably focus on explainability at node, edge, or node feature levels.",2,positive
", 2019), PGExplainer (Luo et al., 2020), and PGM-Explainer (Vu & Thai, 2020), they invariably focus on explainability at node, edge, or node feature levels.",2,positive
"Although GNNs have shown state-of-the-art results on tasks involving graph data [50; 6], existing methods for explaining the predictions of GNNs have primarily focused on generating subgraphs that are relevant for a particular prediction [48; 1; 9; 24; 28; 31; 33; 42; 46; 49].",1,neutral
", PGExplainer [56], SubgraphX [59], and Gem [60]), we",2,positive
"Contrary to most of the above interpretability methods that provide a post-hoc explanatory subgraph at the structure-level (e.g., PGExplainer [56], SubgraphX [59], and Gem [60]), we design an inherently interpretable message passing scheme for homogeneous graphs at the feature-level by identifying important input node capsules from the extracted subgraph.",2,positive
PGExplainer [56] uses parameterized neural network to identify the important subgraphs at the model-level.,1,neutral
"Concurrently to this paper, Luo et al. (2020) have also developed an interpretability technique for GNNs relying on differentiable edge masking.",1,neutral
05 to the previous baseline performance in [27].,0,negative
"We see that for the node case, explanationmethods Pope et al. (LRP), GNN-LRP, PGExplainer and GNNExplainer, all have an AUROC score above 0.9, which shows that all these explanation methods have been able to extract from the model the class-specific motif in the input graphs.",2,positive
"Some explanation techniques such as GNNExplainer [26], PGExplainer [27] and SubgraphX [32] are based solely on evaluating the function or its gradient multiple times.",1,neutral
The values for PGExplainer are extracted from [27].,1,neutral
"As a second quantitative evaluation, we consider the BA2motifs [27] benchmark that comeswith ‚Äòground-truth‚Äô explan-",2,positive
GNNExplainer [26] and PGExplainer [27] explain the model by extracting the subgraph that maximizes the mutual information to the prediction for the original graph.,1,neutral
"[26], PGExplainer [27] and SubgraphX [32] are based solely",0,negative
GNNExplainer [26] and PGExplainer [27] explain the model by extracting the subgraph that maximizes,2,positive
"Regarding the quality of the relevance features, most of the proposed methods attribute the GNN prediction to nodes or edges of the input graph [24], [26], [27], whereas GNN-LRP gives scores for higher-order features, such as sequences of edges.",1,neutral
"As a second quantitative evaluation, we consider the BA2motifs [27] benchmark that comeswith ‚Äòground-truth‚Äô explanations.",2,positive
"For example, [9] was proposed to utilize mutual information to find a subgraph with associated features for interpreting GNN models; PGExplainer [16] learns a parameterized model to predict whether an edge is important; SubgraphX [17] explains GNNs by exploring and identifying important subgraphs; GraphSVX [18] utilizes decomposition technique to explain GNNs based on the Shapley Values from game theory.",1,neutral
"Despite players‚Äô understanding of the meaning of edge weights in GNNExplainer and PGExplainer tasks, the weights sometimes confused their decisions.",1,neutral
‚Ä¢ PGExplainer: We presented players with PGExplainer‚Äôs selection probabilities of edges in subgraphs and node colors.,2,positive
"Specifically, it achieves outstanding precision and recall scores in node classification datasets and outperforms state-of-the-art methods GNNExplainer and PGExplainer.",2,positive
"Second, we performed qualitative assessments of our framework by comparing it with two state-of-the-art post-hoc explanation methods [7], [8], highlighting SCALE‚Äôs superior quality of explanations.",2,positive
"Moreover, post-hoc explanation methods [7], [8] often transform node classification problems into graph classification problems via subgraph (K-hop) sampling, which can be suboptimal when graphs contain numerous small cycles.",1,neutral
"research area with several subsequent papers [8], [23], [24] exploring the topic.",1,neutral
"Additionally, their explanation solutions are too straightforward, making it challenging to achieve significant results on other datasets [7], [8].",1,neutral
"For fair comparisons, we contacted the authors of GNNExplainer, PGExplainer, and SEGNN to request evaluation scripts for all datasets.",2,positive
"Even though GNNExplainer and PGExplainer can highlight impactful edges in
VOLUME 11, 2023 40799
Authorized licensed use limited to the terms of the applicable license agreement with IEEE.",0,negative
"Similar to previous works [7], [8], our framework aims to identify the key factors from graph structures, node features, and edge features that contribute the most to predictions.",2,positive
"Specifically, BA-2motifs (BA-2m) [8] consists of 1000 graphs with two classes constructed by adding specific motifs to BA graphs, where half contain 5-node house motifs and the other half include 5-node cycle motifs.",1,neutral
"Among them, perturbationmethods like GNNExplainer [7] and PGExplainer [8] have gained widespread acceptance, primarily because they introduce benchmark datasets for GNN explanation tasks and exhibit exceptional performance.",1,neutral
"However, adjusting the visibility threshold in GNNExplainer and PGExplainer to present explanations on multiple levels may result in outputs with multiple disconnected components, given the independence of edge selections.",1,neutral
"Explanation models, such as those proposed in [7] and [8], are students trained with hard labels provided by the original GNNs.",1,neutral
"Inspired by [8], the mask matrix is initialized via an MLP network in which inputs are edge embedding vectors constructed by concatenating embedding vectors of source and target nodes taken from the black-box GNN model.",1,neutral
"SCALE outperforms baselines on Mutag, with precision score gains of 15.54% compared to PGExplainer and 51.52% compared to GNNExplainer.",0,negative
"Our observations indicated that PGExplainer‚Äôs reparameterization trick [8] led the selection probabilities to approach 1 in most cases, making it difficult for participants to differentiate edge influences.",0,negative
"Although the average accuracy scores were almost identical for both GNNExplainer and PGExplainer tasks, PGExplainer caused more confusion in explanation weights.",0,negative
"Furthermore, its performance is comparable to that of PGExplainer on the BA-2motifs dataset.",2,positive
"As shown in Table 4, SCALE
outperforms post-hoc explanation methods by a significant margin in all experiments, with performance gains of up to 94x compared to GNNExplainer and 120x compared to PGExplainer.",0,negative
‚Ä¢ PGExplainer [8] shared the same approach as GNNExplainer [7] but initialized masks using embedding vectors from the pre-trained model.,2,positive
"B. QUALITATIVE COMPARISON WITH BASELINES For each dataset, we chose one instance and visualized explanations provided by SCALE, GNNExplainer, and PGExplainer in Figure 3.",0,negative
"plainer [7] and PGExplainer [8] have gained widespread acceptance, primarily because they introduce benchmark datasets for GNN explanation tasks and exhibit exceptional performance.",2,positive
Other baselines except PGExplainer were also executed using the same PyTorch version.,0,negative
"[8], which formulate structural explanations as binary classification tasks by including influential nodes and edges in explanations.",1,neutral
9.1 for experiments with PGExplainer.,0,negative
"As a result, user prediction accuracies varied more in the PGExplainer task than in the GNNExplainer task.",1,neutral
"We see that the objective tends to be positive or zero for Œ≥ ‚â• 0.2, although it can be negative with significant frequency, compared to the positive frequency, in BA-2motif.",1,neutral
"PGExplainer learns approximate discrete masks by training a parametric predictor, and masks out unimportant edges according to the learned masks.",2,positive
"For one graph each from the positive (Table 4) and the negative (Table 5) classes in BA-2motif, we list the top 100 most absolute relevant walks (before omitting the negative-relevant walks) found by EMP-neu.",2,positive
"BA-2motif
BA-2motif (Luo et al., 2020) is a synthetic dataset of graphs that are built by attaching one of two different motifs (either a house or a circle shape) to a random graph, which is generated by the BarabaÃÅsi-Albert (BA) model.",1,neutral
"Note that these heuristics are compatible with all edge-level explanability methods, including GNNExplainer and PGExplainer, which however are incomparably slow.",2,positive
"We also used the BA-2motif dataset, which provides the ground truth subgraphs as motifs, and evaluated how accurately explanation methods can detect the motifs.",2,positive
Table 1 shows computation time (on an M1Pro CPU) of explanation methods on the BA-2motif and Infection datasets.,2,positive
"(a) Top-1 walk search with GIN-L for L = 2, . . . , 7 on BA-2motif.",1,neutral
"These include general explanation methods, e.g., sensitive analysis (SA), guided backpropagation (GBP), class activation mapping (CAM) and excitation backpropagation (EB) (Baldassarre & Azizpour, 2019; Pope et al., 2019), adapted to the GNN structure, and novel methods specialized for GNNs, e.g., GNNExplainer (Ying et al., 2019) and PGExplainer (Luo et al., 2020).",2,positive
"This paper focuses on the instance-level explanation, for which most of the existing methods, e.g., GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020) and methods in Pope et al. (2019), only consider lower-order features, i.e., nodes and edges, ignoring higher-order interactions.",1,neutral
"(14)
Table 3 shows how often the objective of the maximization objective in the AMP-ave message passing (14) is positive, negative, or zero on randomly chosen 10 correctly classified samples from BA-2motif, MUTAG, and Graph-SST2 with different Œ≥.",1,neutral
"‚Ä¶(SA), guided backpropagation (GBP), class activation mapping (CAM) and excitation backpropagation (EB) (Baldassarre & Azizpour, 2019; Pope et al., 2019), adapted to the GNN structure, and novel methods specialized for GNNs, e.g., GNNExplainer (Ying et al., 2019) and PGExplainer (Luo et al., 2020).",2,positive
"Then, we performed approximate top-K walk search by AMP-ave for different K, and evaluated its performance in terms of precision TP/K and recall TP/K‚àó, where TP = |{Approx. top-K walks} ‚à© {True top-K‚àó walks}|, on randomly chosen samples among the correctly classified test samples from each dataset.3 Figure 3 shows the precision-recall curves on BA-2motif and Mutagenicity for different K‚àó and different Œ≥ of LRP-Œ≥ rules.",1,neutral
"We use common benchmark datasets including BA-2motif, MUTAG, Mutagenicity, and Graph-SST2 (see Appendix F for details on data and employed GNNs).",2,positive
"(1) We implement PGExplainer (PG) in (Luo et al., 2020) and adapt it for the temporal graph scenario.",2,positive
"More details about reparameterization can be found in (Luo et al., 2020).",1,neutral
"Inspired by previous parameterized explainers (Luo et al., 2020), we pretrain a navigator to provide a global understanding of the relationship among events.",2,positive
"Learning-based methods (Luo et al., 2020; Shan et al., 2021; Vu & Thai, 2020) leverage node representations generated by the trained GNN and adopt a neural network to learn crucial nodes/edges.",1,neutral
"While currently there are no methods for explaining temporal graph models, some recent explanation methods (e.g., GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020) and SubgraphX (Yuan et al., 2021)) for static GNNs are the most related.",1,neutral
", 2019), PGExplainer (Luo et al., 2020) and SubgraphX (Yuan et al.",2,positive
", PGExplainer [141]), where the aim is to train a model to interpret a pre-trained GL-GNN.",1,neutral
"Most algorithms for explaining the predictions of GL-GNNs are post-hoc (e.g., PGExplainer [141]), where the aim is to train a model to interpret a pre-trained GL-GNN.",1,neutral
PGExplainer [141] trains an MLP to determine which edges are valuable to a GNN‚Äôs prediction and then removes any irrelevant edges to form a new graph.,1,neutral
The chemical fragments -NO2 and -NH2 in mutagen graphs are labeled as ground-truth explanations [27].,1,neutral
"Mask Generation (MG) [27, 49, 29, 43] The mask generation method is to optimize a mask generator gŒ∏ to generate the edge mask M for the input graph G.",1,neutral
BA-2Motifs [27] is a synthetic dataset with binary graph labels.,1,neutral
"Explainability methods We compare non-generative methods: Saliency [6], Integrated Gradient [36], Occlusion [53], Grad-CAM [32], GNNExplainer [48], PGMExplainer [39], and SubgraphX [52], with generative ones: PGExplainer [27], GSAT [29], GraphCFE (CLEAR) [28], D4Explainer and RCExplainer [42].",2,positive
Method Generator Information Constraint Level Scenario Output PGExplainer [27] Mask Generation size instance factual E GIB [49] Mask Generation mutual information instance factual N GSAT [29] Mask Generation variational instance factual E GNNInterpreter [43] Mask Generation size model factual N / E / NF GEM [25] VGAE size instance factual E CLEAR [28] VGAE size instance counterfactual E / NF OrphicX [26] VGAE variational & size instance factual E D4Explainer Diffusion size instance & model counterfactual E GANExplainer [24] GAN instance factual E RCExplainer [42] RL-MDP size instance factual SUBGRAPH XGNN [50] RL-MDP size model factual SUBGRAPH GFlowExplainer [22] RL-DAG size instance factual SUBGRAPH,0,negative
"We follow the original setting to train PGExplainer, GSAT, and RCExplainer.",2,positive
"The graph generator is trained via a policy gradient method based on information from the trained GNNs.
PGExplainer adopts a deep neural network to parameterize the generation process of explanations, which enables PGExplainer a natural approach to explaining multiple instances collectively.",2,positive
"We drew the relationship curves of fidelity and sparsity in BBBP and tree-grids data sets by adjusting parameters, and comparing them with GNNExplainer, PGExplainer, XGNN, and SubgraphX.",2,positive
"In recent years, the main methods that have been used include XGNN, PGExplainer, and GraphSVX.",2,positive
"In recent years, several methods have been proposed to interpret GNN prediction [27, 41], such as XGNN [42], PGExplainer [23], and GraphSVX [4, 7].",1,neutral
"Independently from this categorization, a further fundamental distinction is among explainers providing explanations in terms of edge [87, 52, 66, 92] or node masks [75, 76, 55, 6, 55, 67].",1,neutral
"However, their study is limited to node classification and the three explainers under analysis [87, 52, 66] are not well representative of the diversity of explanation strategies that",1,neutral
"[91] proposed a categorization of explainers into four categories: gradient-based which exploit gradients of the input neural network [75, 76, 55]; perturbation-based where perturbations of the input graphs are aimed at obtaining explainable subgraphs [87, 52, 23, 66]; decomposition-based which try to decompose the input identifying the explanations [6, 55, 67]; and surrogate-based where a simple interpretable surrogate model is used to explain the original neural network [36, 97, 80].",1,neutral
"PgExpl: (Perturbation) [52]: The Parametrized Explainer for GNNs (PgExpl) [52] adopts a very similar formulation of the explanation problem as GnnExpl where the two major differences are: i) PgExpl provides solely explanations in terms of subgraph structures, neglecting explanations in terms of node features; ii) instead of directly optimizing continuous edge and features masks as done by GnnExpl, it uses Gradient Descend to train a MLP which, given the two concatenated node embeddings",1,neutral
"Roughly speaking, gradient-based explainers exploit gradients of the input neural network [75, 76, 55], perturbation-based models perturb the input aiming to obtain explainable subgraphs[87, 52, 23, 66], decomposition-based models try to decompose the input identifying the explanations [6, 55, 67], while surrogatebased models use a simple interpretable surrogate to explain the original neural network [36, 97, 80].",1,neutral
"Recently, although the explainability of graph neural networks (GNNs) [9], [10], [11], [12], [13], [14] has been explored as in [15], [16], [17], [18], [19], [20], and [21], they are limited to understanding",1,neutral
"models, recent explanation models for GNNs [9], [10], [11], [12], [13], [34] using gradients [35], decomposition [36], surrogates [17], [37]), and perturbation [15], [16], [38] have been proposed.",1,neutral
"Techniques have been used and implemented to create post hoc explanations of GNN models [4, 5], however, none of them can truly follow the decision process.",1,neutral
"Perturbation-based methods [16], [17], [18], [19], [20], [21] measure importance scores by masking the",1,neutral
"‚Ä¢ BA 2Motifs (Luo et al., 2020) is a synthetic graph classification dataset.",2,positive
Luo et al. (2020) follow a similar idea but emphasize finding structures that explain multiple instances at the same time.,1,neutral
"We use the Infection and Negative Evidence benchmarks from Faber et al. (2021), The BA-Shapes, Tree-Cycle, and Tree-Grid benchmarks from Ying et al. (2019), and the BA2Motifs dataset from Luo et al. (2020).",2,positive
PGExplainer [3] trains a mask predictor to generate a discrete masks for learning the importance.,1,neutral
"‚Ä¢ PGExplainer PGExplainer was proposed after GNNExplainer in order to provide a more comprehensive understanding on the predictions made by GNNs.[11] It uses an optimization framework that is similar to GNNExplainer, however it uses continuous variables in the range (0, 1) for the edge weights.",2,positive
"9
‚Ä¢ PGExplainer
PGExplainer was proposed after GNNExplainer in order to provide a more comprehensive understanding on the predictions made by GNNs.[11] It uses an optimization framework that is similar to GNNExplainer, however it uses continuous variables in the range (0, 1) for the edge weights.",2,positive
"In fact, GNNExplainer, PGExplainer, and SubgraphX can never generate explanations including only disconnected oxygen atoms but not nitrogen atoms like GStarX, because the former two solve the explanation problem by optimizing edges (as opposed to Equation 3), and the latter requires connectedness.",1,neutral
", 2019), PGExplainer (Luo et al., 2020), SubgraphX (Yuan et al.",2,positive
"GStarX is not as fast as GNNExplainer, PGExplainer, and GraphSVX, but it is about more than two times faster than SubgraphX.",2,positive
"BA2Motifs (Luo et al., 2020) is a synthetic dataset for graph classification.",2,positive
"GNNExplainer and PGExplainer choose some but not all important words, with extra neutral words appearing in the explanations as well.",0,negative
"PGExplainer (Luo et al., 2020) uses the same scoring function, but generates a discrete mask on edges by training an edge mask predictor.",2,positive
"We compare with 4 strong baselines representing the state-of-the-art methods for GNN explanation: GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), SubgraphX (Yuan et al., 2021), and GraphSVX (Duval & Malliaros, 2021).",2,positive
"We construct 3-class synthetic datasets based on BAMotif (Ying et al., 2019; Luo et al., 2020) following (Wu et al.",2,positive
"To begin with, we construct 3-class synthetic datasets based on BAMotif (Luo et al., 2020) and follow Wu et al.",2,positive
"Built upon the graph generation process, can existing methods produce a desired invariant GNN model? Using the BAMotif task (Luo et al., 2020) as Fig.",1,neutral
"2.2
To begin with, we construct 3-class synthetic datasets based on BAMotif (Luo et al., 2020) and follow Wu et al. (2022c) to inject spurious correlations between motif graph and base graph during the generation.",2,positive
"We construct 3-class synthetic datasets based on BAMotif (Ying et al., 2019; Luo et al., 2020) following (Wu et al., 2022c), where the model needs to tell which one of three motifs (House, Cycle, Crane) that the graph contains.",2,positive
"Using the BAMotif task (Luo et al., 2020) as Fig.",1,neutral
"We compare the proposed method with popular post-hoc explanation techniques including the GNN-Explainer (Ying et al. 2019), PGEExplainer (Luo et al. 2020), GradCAM (Pope et al. 2019) and SubgraphX (Yuan et al. 2021)2.",2,positive
"2019), PGEExplainer (Luo et al. 2020), GradCAM (Pope et al.",2,positive
"To this end, we use the BA2Motifs dataset (Luo et al. 2020).",2,positive
BA2Motifs is a synthetic dataset that was first introduced in Luo et al. (2020).,2,positive
"(Ying et al. 2019; Luo et al. 2020; Funke, Khosla, and Anand 2021; Loveland et al. 2021;
Schlichtkrull, Cao, and Titov 2021; Yuan et al. 2021; Perotti et al. 2022).",2,positive
"Methods like GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020) and PGMexplainer (Vu & Thai, 2020) allow importance analysis of nodes and edges within the input data sample.",2,positive
"GNNExplainer (Ying et al., 2019) and PGExplainer (Luo et al., 2020) explain GNNs by finding masks that maximize the mutual information between
the predictions of the original graph and a masked graph.",1,neutral
"BA2-Motif
BA-2motif (Luo et al., 2020) is a synthetic dataset of graphs that can be classified into two classes according to the different motifs.",1,neutral
"Understanding the relevance contribution of subgraphs in the input graph to the model prediction is a key challenge when explaining models on graphs (Yuan et al., 2021; Luo et al., 2020; Schnake et al., 2021).",1,neutral
"GNNExplainer learns soft masks for edges or node features, while PGExplainer trains a parametric predictor to determine if an edge should be masked out.",2,positive
"We used the following five popular datasets: BA-2motif (Luo et al., 2020), MUTAG (Debnath et al., 1991), Mutagenicity (Kazius et al., 2005b), REDDIT-BINARY (Yanardag & Vishwanathan, 2015), and Graph-SST2 (Yuan et al., 2020b).",2,positive
"Here, we use the BA-2motif dataset to evaluate the node ordering performance of subgraph attribution.",2,positive
"We first evaluate the node ordering performance on the BA-2motif dataset, for which the ground truth is available.",2,positive
"Most explainability techniques (Pope et al., 2019; Ying et al., 2019; Luo et al., 2020) for GNNs explains the model at the level of nodes, edges and node features, while a few of them, including SubgraphX (Yuan et al., 2021) and GNN-LRP (Schnake et al., 2021), analyze the relevance of subgraphs as‚Ä¶",2,positive
"Figure 4 shows the computation time for subgraph attribution on BA-2motif, as functions of (a) the network depth L and (b) the subgraph size |S|, respectively.",1,neutral
"In this context, the most important features are those that lead to similar predictions once retained [24,52,33,54].",1,neutral
The interpretation at the subgraph-level enables more intuitive and effective description of the GNN [134-136].,1,neutral
"‚Ä¶be developed in Section 7 belongs to the explainablility category, where the objective is to discover qualitative justifications on the learning outcome of a GCN; see (Luo et al., 2020; Lin et al., 2021; Schlichtkrull et al., 2021; Ying et al., 2019; Yuan et al., 2021) and the references therein.",2,positive
"The statistic model to be developed in Section 7 belongs to the explainablility category, where the objective is to discover qualitative justifications on the learning outcome of a GCN; see (Luo et al., 2020; Lin et al., 2021; Schlichtkrull et al., 2021; Ying et al., 2019; Yuan et al., 2021) and the references therein.",2,positive
"Numerous methods focus on identifying relevant subgraphs in the embedding space (Ying et al., 2019; Luo et al., 2020; Lin et al., 2021), but such approaches do not explain predictions symbolically‚Äîthat is, by showing how a prediction can be derived from the input KG via logical inferences of a‚Ä¶",1,neutral
"Existing methods for explaining the predictions of a GNN focus on identifying parts of the graph in the embedding space that are most relevant to a given prediction (Ying et al., 2019; Luo et al., 2020; Lin et al., 2021).",1,neutral
"Numerous methods focus on identifying relevant subgraphs in the embedding space (Ying et al., 2019; Luo et al., 2020; Lin et al., 2021), but such approaches do not explain predictions symbolically‚Äîthat is, by showing how a prediction can be derived from the input KG via logical inferences of a knowledge representation formalism.",1,neutral
"The explanation is 1365 attributable to subgraph decomposition theory [198], where 1366 it is feasible to determine whether the learned model is inter- 1367 pretable by identifying the subgraph with the most significant 1368 influence on prediction and judging whether the subgraph is 1369 faithful to general knowledge.",1,neutral
"Numerous methods focus on identifying relevant subgraphs in the embedding space (Ying et al., 2019; Luo et al., 2020; Lin et al., 2021), but such approaches do not explain predictions symbolically‚Äîthat is, by showing how a prediction can be derived from the input KG via logical inferences of a‚Ä¶",1,neutral
"Existing methods for explaining the predictions of a GNN focus on identifying parts of the graph in the embedding space that are most relevant to a given prediction (Ying et al., 2019; Luo et al., 2020; Lin et al., 2021).",1,neutral
"Numerous methods focus on identifying relevant subgraphs in the embedding space (Ying et al., 2019; Luo et al., 2020; Lin et al., 2021), but such approaches do not explain predictions symbolically‚Äîthat is, by showing how a prediction can be derived from the input KG via logical inferences of a knowledge representation formalism.",1,neutral
"Consequently, an increasing number of works are focussing on explaining [11, 12, 13, 14] the decisions of",1,neutral
"In terms of methodologies, several techniques based on input perturbations [11, 12, 13], input gradients[36, 37], causal techniques [34, 38, 33] as well as utilizing simpler surrogate models [14] have been explored.",1,neutral
"Explanations usually include the importance scores for nodes/edges in a subgraph (or node‚Äôs neighborhood in case of node-level task) and the node features [11, 12, 13].",1,neutral
"Besides, learning M for each graph G separately hinders the method from handling unseen test graphs [14].",1,neutral
‚Ä¢ PGExplainer [7] hires a neural network to learn to generate the masks for the input edges.,1,neutral
"We follow previous works [6, 7, 10, 19] and focus on the contributions of the structural features (i.",2,positive
"We combine the contrastive learning [17, 18] into class-wise generative probabilistic models [7], thereby approach coarser-grained explanations (i.",1,neutral
"Inspired by the success of generative models [7, 30, 31] in capturing the succinct structures from the graphs, we hire multiple generative probabilistic models [7] as our attribution models (short for attributor), i.",2,positive
"Thus, they lack the global understanding of the model‚Äôs workings [7, 13], which is vital to generalize to other instances being explained.",1,neutral
"We ascribe this to the limitations of PGExplainer‚Äôs global view, which is founded upon all the explained instances, but fails to differentiate the class-wise patterns.",2,positive
"However, most of current explainers focus on either on local [9, 10, 6, 11, 12] or global explainability [13, 7], thereby suffer from inherent limitations correspondingly: ‚àóXiangnan He is the corresponding author.",1,neutral
"To approximate the importance score to the discrete distribution and optimize the generator via gradient propagation, we adopt the reparameterization trick [7], where an independent random variable ‚àº Uniform(0, 1) is introduced.",1,neutral
"‚Ä¢ Although PGExplainer is also equipped with the global view of the target model, its performance is worse than that of ReFine-FT.",0,negative
"It is worth emphasizing that our attributors is different from PGExplainer [7], where only one generative probabilistic model is involved.",2,positive
The inference time [7] to explain a new instance by the pre-trained ReFine is the same as PGExplainer under the same attributor construction.,1,neutral
"To be more clear, we present the difference of PGExplainer [7], ReFine and its ablation models in Table 4.2.",0,negative
"For the parametric explanation methods (GNNExplainer, PGExplainer, PGMExplainer), we apply a grid search to tune their own hyperparameters.",2,positive
"To provide a global understanding of the model prediction, PGExplainer [7] formulates the generation of multiple explanations based on its collective and inductive property, and designs the attributor as a deep neural network whose parameters are shared across the explained instances.",2,positive
"It is also worth mentioning that, although the general understanding of GNN predictions has been considered in a recent work PGExplainer [7], it is only exploited to train a generative probabilistic model shared across all the explained instances, rather than dissecting and modeling the class-wise knowledge explicitly.",1,neutral
"Through this way, ReFine can faithfully generate multigrained explanations, and we empirically show its effectiveness as compared to some state-of-the-art explainers [9, 6, 7, 19].",2,positive
"Such multi-grained explainability flexibly and reliably inspects the decision-making process of the GNN [4, 5], which is critical to the applications on safety, fairness, and privacy [6, 7].",2,positive
"Graph Neural Networks (GNNs) have emerged as powerful tools for effectively representing graph structured data, such as social, information, chemical, and biological networks.",1,neutral
Explainability in Graph Neural Networks (GNNs).,1,neutral
"These are gradient-based methods: Gradients [41], Integrated Gradients [45]; perturbation-based methods: GNNExplainer [50], PGExplainer [30], GraphMASK [39]; and surrogate models: GraphLIME [17], PGMExplainer [47].",1,neutral
GraphLIME [17] is a local interpretable model explanation for GNNs that identifies a nonlinear interpretable model over the neighbors of a node that is locally faithful to the node‚Äôs prediction.,1,neutral
"With the reparameterization, the objective function of PGExplainer becomes:
min ‚Ñ¶
E ‚àºUniform(0,1) H(Y |Gu = GÃÇS), (19)
where H is the conditional entropy when the computational graph for Gu is restricted to GS .",1,neutral
"A variety of GNN architectures have been proposed in the literature [31, 52, 54], and recent research has focused on on developing GNN explanation methods [3, 8, 17, 28, 30, 33, 39, 47, 50].",1,neutral
Notation: Graphs and GNNs.,1,neutral
"In contrast to GNNExplainer, PGExplainer [30] generates explanation only on the graph structure.",1,neutral
"GNNs specify non-linear deep transformation functions that map graph structures (i.e., nodes, edges or entire graphs) into compact vector embeddings [26].",1,neutral
"Based on the techniques they employ, these approaches can be broadly characterized into perturbation-based [30, 39, 50], gradient-based [41, 45], and surrogate model based [17, 47] methods [51].",1,neutral
"To this end, several approaches have been proposed in recent literature to explain the predictions of GNNs [3, 8, 17, 28, 30, 33, 39, 47, 50].",1,neutral
"In the context of GNNs, an explanation Eu corresponding to a node u is considered stable if the explanations corresponding to u (i.e., Eu) and its perturbation u‚Ä≤ (denoted by E‚Ä≤u) are similar.",1,neutral
"Due to the discrete nature of GS , PGExplainer employs the reparameterization trick where they relax the edge weights from binary to continuous variables in the range (0, 1) and then optimize the objective function using gradient-based methods.",1,neutral
These bounds need only information about the general message passing form of GNNs [10] and do not make any assumptions about the GNN architecture.,1,neutral
"More recently, perturbation-based methods [30, 39, 50] explain GNN predictions by finding small subgraphs that are most influential for the prediction w.",1,neutral
"||(1 ‚àí ru) ‚ó¶ xu||2, where Œ≥11 is a constant comprising
of the product of the Lipschitz constant for GNN‚Äôs activation function, weights of the final node classification layer, and self-attention weight of node u across all layers of the GNN.",1,neutral
"While several approaches have been proposed to explain the predictions of GNNs, evaluating the quality of the resulting explanations is non-trivial.",1,neutral
PGExplainer.,0,negative
"As GNNs are increasingly being employed in critical applications in domains such as financial lending and criminal justice, it becomes important to ensure that the explanations generated by state-of-the-art GNN explanation methods preserve the fairness properties of the underlying model.",2,positive
"Thus, PGExplainer consider a relaxation by assuming that the explanatory graph GS is a Gilbert random graph, where selections of edges from the input graph Gu are conditionally independent to each other.",1,neutral
"Formally, GNNExplainer determines the importance of individual node attributes and incident edges for node u by leveraging Mutual Information (MI) using the following optimization framework:
max GS
MI(Y, (GS ,XS)), (17)
where GS ‚äÜ Gu is a subgraph and XS is the associated node attributes that are important for the GNN‚Äôs prediction yÃÇu.",1,neutral
"Further, Œ≥ denotes the product of the Lipschitz constants for GNN‚Äôs activation function and GNN‚Äôs weight matrices across all layers in the GNN, and ‚àÜ is an explanation method-specific term.",1,neutral
"[30] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang.",0,negative
"Taking the classifying mutagenic property of a molecular graph (Luo et al., 2020) as an example, G is a molecular graph where the nodes are atoms and the edges are the chemical bonds between atoms, and Y is the class label, e.g., whether the molecule is mutagenic or not.",1,neutral
"‚Ä¶causes Z such as the factors to generate nitrogen dioxide (NO2) group, which has an determinative effect on the mutagenicity of molecule, and the effect of irrelevant variable M , such as the carbon ring which exist more frequently in mutagenic molecule but not determinative (Luo et al., 2020).",1,neutral
"Taking the classifying mutagenic property of a molecular graph (Luo et al., 2020) as an example, G is a molecular graph where the nodes are atoms and the edges are the chemical bonds between atoms, and Y is the class label, e.",1,neutral
", 1991), as we could explain the results based on the knowledge used in (Luo et al., 2020).",2,positive
"StableGNN correctly identifies chemicalNO2 andNH2, which are known to be mutagenic (Luo et al., 2020) while baselines fail in.",2,positive
"The whole molecular graph G is an effect of relevant latent causes Z such as the factors to generate nitrogen dioxide (NO2) group, which has an determinative effect on the mutagenicity of molecule, and the effect of irrelevant variable M , such as the carbon ring which exist more frequently in mutagenic molecule but not determinative (Luo et al., 2020).",1,neutral
"Moreover, we also conduct the experiments on a commonly used graph classification dataset, MUTAG (Debnath et al., 1991), as we could explain the results based on the knowledge used in (Luo et al., 2020).",2,positive
"Similarly, PGExplainer (Luo et al., 2020) uses a generative probabilistic model to learn succinct underlying structures from the input graph data as explanations.",1,neutral
"Similarly, PGExplainer (Luo et al., 2020) uses a generative probabilistic",1,neutral
", 2019), PGExplainer (Luo et al., 2020) and PGMExplainer (Vu and Thai, 2020).",2,positive
"As GNNExplainer and PGExplainer provide continuous masks, we report, for fair comparisons, the performance with both continuous and discrete masks built with the k best edges.",2,positive
"According to the literature, the best competitors are GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020) and PGM-Explainer (Vu and Thai, 2020).",0,negative
"GNNExplainer, PGExplainer, and PGM-Explainer are the methods that report the best performance on many datasets.",2,positive
"INSIDE-GNN obtain similar scores or outperforms the other competitors (i.e., PGExplainer, PGM-Explainer, Grad) at equal sparsity on most of the datasets.",2,positive
It remains slightly slower than PGExplainer (6ms to 20ms).,0,negative
"Methods based on perturbation (Luo et al., 2020; Ying et al., 2019) aim to learn a mask seen as an explanation of the model decision for a graph instance.",1,neutral
", 2019), PGExplainer (Luo et al., 2020), and GraphMask (Schlichtkrull",2,positive
"‚Ä¶our FlowX with eight baselines, including GradCAM (Pope et al., 2019), DeepLIFT (Shrikumar et al., 2017), GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), PGMExplainer (Vu & Thai, 2020), SubgraphX (Yuan et al., 2021), GNN-GI (Schnake et al., 2020), GNN-LRP (Schnake et al., 2020).",2,positive
", 2019), PGExplainer (Luo et al., 2020), and SubgraphX (Yuan et al.",2,positive
", 2019), PGExplainer (Luo et al., 2020), PGMExplainer (Vu & Thai, 2020), SubgraphX (Yuan et al.",2,positive
"‚Ä¶methods have been proposed to explain the predictions of GNNs, such as GraphLime (Huang et al., 2020), GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), PGMExplainer (Vu & Thai, 2020), SubgraphX (Yuan et al., 2021), XGNN (Yuan et al., 2020b), and GraphSVX (Duval & Malliaros, 2021).",2,positive
"Recently, several techniques have been proposed to explain GNNs, such as XGNN (Yuan et al., 2020b), GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), and SubgraphX (Yuan et al., 2021), etc.",2,positive
"Second, several existing methods, such as GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), and GraphMask (Schlichtkrull
et al., 2021), explain GNNs by studying the importance of different graph edges.",1,neutral
"Following [17], we use the cross-entropy function to replace the conditional entropy function minS H(Y |S) with N given instances.",1,neutral
", GNNExplainer [34] and PGExplainer [17], attempt to solve the optimization problem with continuous relaxation.",1,neutral
"For fairness, we follow the experimental setup in [17, 12], i.",1,neutral
Then we compare RG-Explainer with two state-of-the-art baselines GNNExplainer [34] and PGExplainer [17] in both qualitative and quantitative evaluations.,2,positive
"Following the problem setting, PGExplainer [17] leverages the representations generated by the trained GNN and adopts a deep neural network to learn the crucial nodes/edges.",1,neutral
"We use the trained GNN model in [12], whose architecture is given in [17, 34].",1,neutral
"To enhance the interpretability of GNNs, a line of works [34, 17, 30, 37, 31] focused on developing GNN explainers.",1,neutral
"For example, GNNExplainer [Ying et al., 2019] and PGExplainer [Luo et al., 2020] identify compact subgraph structures that are crucial in prediction as the explanation for the GNN models.",1,neutral
", 2019] and PGExplainer [Luo et al., 2020] identify compact subgraph structures that are crucial in prediction as the explanation for the GNN models.",1,neutral
"It would be beneficial - and is considered future work - if GraphLIME was also trying to find important graph substructures instead of just features, if it was compared with other methods like PGExplainer [52], PGMExplainer [83], GNN-LRP [72], and if it were extended to multiple instance explanations.",2,positive
PGExplainer can explain multiple instances collectively and also works in an inductive setting.,1,neutral
"We shall discuss and compare 19 recent GNN interpretability methods from the aforementioned categories: GNNExplainer [36], PGExplainer [37], GraphMask [38], SubgraphX [39], PGMExplainer [40], RelEx [41], GraphLime [42], RCExplainer [43], DnX [44], GCFExplainer [45], CF2 [46], SA [26], GuidedBP [26], CAM [21], Grad-CAM [47], LRP [48], GNNLRP [49], ExcitationBP [21], and XGNN [50].",2,positive
"We shall discuss and compare 19 recent GNN interpretability methods from the aforementioned categories: GNNExplainer [36], PGExplainer [37], GraphMask [38], SubgraphX [39], PGMExplainer [40], RelEx [41], GraphLime [42], RCExplainer [43], DnX [44], GCFExplainer [45], CF(2) [46], SA [26], GuidedBP [26], CAM [21], Grad-CAM [47], LRP [48], GNNLRP [49], ExcitationBP [21], and XGNN [50].",2,positive
"PGExplainer performs a similar process as GNNExplainer that maximizes the mutual information between the predictions of the input graph and that of the evidence subgraph; however, it only focuses on the graph structure by using a deep neural network to parameterize the generation of the evidence subgraph.",2,positive

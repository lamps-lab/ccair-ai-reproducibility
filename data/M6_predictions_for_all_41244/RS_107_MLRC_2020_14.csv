text,target_M6_predict,target_predict_M6_label
"Finally, we retrain our model from scratch each round to prevent warm starting [4].",0,negative
"Finally, we retrain our model form scratch each round to prevent warm starting [22].",0,negative
"For example, if a pipeline will be executed on the interval [1, 11), all of the vertices in the interval DAG have the interval [1, 11).",1,neutral
"Vertex for P1 only
Vertex for P2 only
Vertex for P1 and P2
S3
S2 S1
S4
Figure 5: The process of materialization DAG construction.
algorithm removes ùëù2 from all of its ancestors, i.e., ‚ü®Var-f, [0, 10)‚ü©, ‚ü®Scale-t, [0, 10)‚ü©, ‚ü®Scale-f, [0, 10)‚ü©, and ‚ü®FG-t, [0, 10)‚ü©.",1,neutral
"For example, the vertex ‚ü®Var-f, [1, 11)‚ü© of ùêºùê∑2 is split into ‚ü®Var-f, [1, 10)‚ü© and ‚ü®Var-f, [10, 11)‚ü© in Figure 6c. S2 There exists a matching vertex with full or partial interval overlap.",1,neutral
"For example, the second execution of p1 can reuse the generated features in the interval [1, 30).",1,neutral
"For example, for the second execution of p1 in interval [1, 31), we can reuse the statistics artifacts (i.",1,neutral
", mean and variance) and the feature artifacts of the interval [1, 30).",1,neutral
"Therefore, for p2, the interval for training is [1, 11) (the last 10 days) and for p1, the interval is [0, 11), since the scheduled interval of p1 (30 days) is larger than the available data (Figure 6a).",0,negative
"For example, the vertex ‚ü®Var-f, [1, 11)‚ü© of ID2 is split into ‚ü®Var-f, [1, 10)‚ü© and ‚ü®Var-f, [10, 11)‚ü© in Figure 6c.",1,neutral
"Therefore, reusing a model does not replace its computation, but reduces the training time [1, 52].",1,neutral
"On the second execution (on the interval [1, 31)), since mean and variance can be computed incrementally, we reuse the mean and variance of the interval [1, 30) and only compute the mean and variance of [30, 31).",1,neutral
"For example, if ‚ü®Var-f, [0, 10)‚ü© is materialized, we set its compute cost to zero before computing the cost of ‚ü®Var-t, [0, 10)‚ü© and ‚ü®DNN, [0, 10)‚ü©.",1,neutral
"Since ‚ü®Var-f, [1, 10)‚ü© and ‚ü®Scale-t, [0, 10)‚ü© are materialized, we prune their incoming edges.",1,neutral
"Note that materializing a vertex such as ‚ü®Var-f, [0, 10)‚ü© does not break the dependency of its descendants (e.g., ‚ü®Var-t, [0, 10)‚ü©) from its ancestors (e.g., ‚ü®Scale-t, [0, 10)‚ü©), since there are more than one path connecting the ancestors to the descendants.",1,neutral
"(Ash & Adams, 2019) considered an extreme transfer scenario, where an agent is pretrained on data from the same distribution as the target task, and reported a negative generalisation gap.",0,negative
"We build on (Ash & Adams, 2019) and study the generalisation gap induced by pretraining the model on the same data distribution.",2,positive
"We start with the same setup as in (Ash & Adams, 2019) training deep residual networks (He et al., 2016) to classify the CIFAR 10 data set.",2,positive
"A similar experiment was reported in (Ash & Adams, 2019).",1,neutral
The selection and initial setting of these hyperparameters critically impacts the performance of deep learning networks in terms of quality of solution and training time required [32].,1,neutral
The idea of core-set can also be related to Warm-Starting Neural Network Training [16].,1,neutral
"If the initial training sample set is selected by any measurement, then we also call it warm start or core set method [16]; otherwise, it is a good star method.",2,positive
[3] compares the performance between warmstarting and fresh random initialization.,2,positive
"At the same time, there are also multiple works that have suggested that fine-tuning is not applicable in all scenarios [2] and re-training from scratch is more beneficial.",1,neutral
ZORB could also provide a new direction for warm starting techniques [3].,1,neutral
"[Ash and Adams, 2019] takes this a step further and shows that warm starting a network might result to poorer generalization although the training losses converge to the same value.",1,neutral
Ash and Adams [10] takes this a step further and shows that warm starting a network might lead to poorer generalization although the training losses may be the same.,1,neutral
"Being based on the Tustin and Euler discretization methods (AÃästroÃàm and Wittenmark (2013)), they will be named Tustin-Nets (TN).",2,positive
"We avoid warm-starting and retrain models from scratch every time new samples are queried (Ash and Adams, 2019).",2,positive
"Generalization impairment due to non-stationarity has already been well studied in both Supervised Learning (Ash and Adams, 2019) and Reinforcement Learning (Igl et al., 2021; Fedus et al., 2020; Lyle et al., 2022; Steinparz et al., 2022).",1,neutral
"Generalization impairment due to non-stationarity has already been well studied in both Supervised Learning (Ash and Adams, 2019) and Reinforcement Learning (Igl et al.",1,neutral
The idea of core-set can also be related to Warm-Starting Neural Network Training [16].,1,neutral
"If the initial training sample set is selected by any measurement, we also call it warm start or core set method [16], other wise it is a cool star method.",2,positive
"Warm starting Ash & Adams (2019) allows models that have been partially trained on a subset of a dataset (in an online setting, for example) to be efficiently updated without sacrificing generalization performance.",2,positive

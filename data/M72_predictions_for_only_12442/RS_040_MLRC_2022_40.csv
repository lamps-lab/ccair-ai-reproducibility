text,target_predict,target_predict_label,target_model_6_predict,target_predict_model_6_label
"We use three self-supervised learning algorithms including SimCLR, MoCo v2 and MAE to pre-train the encoder and use ResNet-18 [35], ResNet-34, ResNet-50, VGG-16 [36], DenseNet-121 [37], ViT-B [38], ViT-L and ViT-H networks as model structures of pre-trained encoder.",1,related,1,positive
"We have selected three representative algorithms for each of the two lines in this work, SimCLR [1], MoCo v2 [2] for the former, and MAE [3] for the latter.",1,related,1,positive
Implementation details: We experiment with ViT-B and ViT-L architectures with supervised and self-supervised (MAE) initialization.,1,related,1,positive
"In this paper, we adopt a novel perspective to explain what contributes to ‚Äúa rich hidden representation inside the MAE‚Äù (He et al., 2022), focusing on analyzing its decoder‚Äôs behaviors.",1,related,1,positive
"Masked Autoencoders (MAE) (He et al., 2022) is a straightforward yet efficacious self-supervised method for pretraining Vision Transformers (ViT)(Dosovitskiy et al.",1,related,1,positive
"To verify this, we computed the attention distance of MAE (He et al., 2022), DINO (Caron et al.",1,related,1,positive
L1 or mean absolute error (MAE) has been calculated to train ReconResNet.,1,related,0,negative
"Di ff erent types of variational auto-encoder(Makhzani et al., 2015) methods can be used, such as Factorised Variational Auto-encoder (FactorVAE) (Kim and Mnih, 2018), Vector Quantised Variational Auto-encoder (VQ-VAE) (Van Den Oord et al., 2017), Masked autoencoders (MAE)(He et al., 2022) etc.",1,related,1,positive
"Therefore, we did not use MAE and its variants (He et al., 2022; Xie et al., 2022; Huang et al., 2022) for comparisons.",1,related,0,negative
"Inspired by the latest art MAE [45], we consider auto-encoder-based models are likely to contain unlimited potential.",1,related,0,negative
", 2020) and MAE (He et al., 2022), our model demonstrates superior efficiency by utilizing fewer parameters (11.",1,related,1,positive
"On a different note, the masked autoencoder (MAE) [22] introduces a different way to tackle self-supervised learning.",1,related,1,positive
"Interestingly, we find these quantities can be powerful tools in understanding and improving existing selfsupervised methods regardless of whether they are contrastive, feature decorrelation-based, or masking-based [22].",1,related,1,positive
We shall briefly introduce MAE [22] as an example.,1,related,1,positive
"We employ commonly utilized augmentations, such as resizing, crop, rotation, color jitter, translation, and horizontal flip, in conjunction with the masked autoencoder [11] to alter the feature space.",1,related,1,positive
"Inspired by MaskAE [13] approaches, we demonstrate that we can maintain a comparable fine-4 tuning model performance by retaining only a small group of vision tokens.",1,related,1,positive
"Inspired by MaskAE [13] approaches, we demonstrate that we can maintain a comparable fine-",1,related,1,positive
"In this work, we train masked autoencoders (MAEs) [24] on progressively larger HCS image sets and show that these models are scalable learners of cellular morphology, outperforming previous SOTA methods at inferring known biological relationships in whole-genome HCS screens.",1,related,1,positive
"As MAE is a self-supervised method, we use all three datasets as well as Cityscapes [58] as external data for the pre-training process.",1,related,0,negative
"Additionally, we incorporate Masked Autoencoders (MAE) [28], a self-supervised pre-training algorithm, to foster enhanced fine-tuning of the ViT [19] on facade-centric data.",1,related,1,positive
We thus adopt an MAE [28] pre-training to initialize ViT methods.,1,related,0,negative
"Notably, we introduced MAE pre-training specifically tailored for facade segmentation, marking the first application of this approach in the context of facade-related tasks.",1,related,0,negative
We thus adopt an MAE He et al. (2022) pre-training to initialize ViT methods.,1,related,0,negative
"Unlike the traditional Masked AutoEncoders[31] that can only learn the data structure by reconstructing from raw sensor data, our FMAE can recover cross-modality latent features inside the original multimodal model.",1,related,1,positive
We initialize the encoder weights with the self-supervised ImageNet pre-training [33].,1,related,1,positive
"More notably, our approach improves over MAE [33] which is also pre-trained with ImageNet and ScanNet (+2.",1,related,1,positive
"Following [33], we first normalize the output patches as well as target patches, and then used compute the MSE loss between the ground truth and the predicted pixels.",1,related,1,positive
"Inspired by masked autoencoders (MAE) [8], a promising generative SSL approach in the computer vision domain, we propose Spatial-Temporal Masked AutoEncoders (STMAE), a versatile framework that is able to elevate the capability of existing spatial-temporal models in MTS forecasting.",1,related,1,positive
"Following [2, 10], we use a shared, learnable mask token as the initial embedding of each masked value.",1,related,1,positive
"In this paper, we present REMASKER, a novel method that extends the masked autoencoding (MAE) framework [2, 10] to imputing missing values of tabular data.",1,related,1,positive
"By extending the siamese form of MAE [15], we show that R E M ASKER encourages learning missingness-invariant representations of input data, which requires a holistic understanding of the data even in the presence of missing values.",1,related,1,positive
"In this paper, we present R E M ASKER , a novel method that extends the masked autoencoding (MAE) framework [2, 10] to imputing missing values of tabular data.",1,related,1,positive
"However, unlike conventional MAE, as the data in the imputation task is inherently incomplete ( i.e. , naturally masked), we employ a ‚Äúre-masking‚Äù approach that explicitly accounts for this incompleteness in applying masking and reconstruction.",1,related,1,positive
"Similar to [10], we use an asymmetric design with a deep encoder and a shallow decoder (e.",1,related,1,positive
"The REMASKER imputer extends the MAE framework [3, 1, 10] that reconstructs masked components based on observed components.",1,related,1,positive
The vanilla ViT-Base [15] model pre-trained with MAE [15] is adopted as the backbone for our encoder.,1,related,1,positive
"As described in lines 101-113 in our main manuscript, and we would like to clarify again here: most existing studies of adopting masking operations (together with self-reconstruction objective) to realize self-supervised learning are based on the transformer backbone thanks to the to-kenized input (where the masking is simply to block out some tokens), and the prior works (e.g. SemMAE [22], MST [23], BEiT [1], iBOT [41], MAE [17], and Sim-MIM [37]) are designed for transformers as well.",1,related,1,positive
"To pretrain the ViT, we adopt the MAE training scheme [He et al., 2022].",1,related,0,negative
"Our method is compared with the State-Of-The-Art (SOTA) class-agnostic SVR method MAGE [21] that uses spatial-masking MIM, and with the SOTA class-dependent SVR method AdaCode [23] that uses a dense weight map.",1,related,1,positive
"Our M-AdaCode can also be seen as a method of Masked Image Modeling (MIM) [14,21].",1,related,1,positive
"From another perspective, our M-AdaCode can be seen as an MIM method.",1,related,1,positive
"In order to match dimension with audio and text embeddings, we pass the output from SATMAE encoder to a ReLU activation followed by a 512-dimension linear layer.",1,related,1,positive
"For overhead imagery, we adopt the same data augmentation as SATMAE [7].",1,related,1,positive
"In our work, we start with the pre-trained weights of Vision Transformer (ViT) [11] encoder of SATMAE [7] as the overhead-image encoder for GeoCLAP.",1,related,1,positive
"For encoding overhead image, we use the pre-trained vit_base_patch16 encoder of SATMAE [7].",1,related,1,positive
"In our work, we adopt a method similar to MAE [17], where the mask tokens are not passed to the encoder.",1,related,1,positive
"Concurrently to our work, [30], [31] used MAE to pre-train the encoder.",1,related,0,negative
We adapt MAE [17] into a self-supervised pre-training framework for trajectory forecasting.,1,related,0,negative
"Further, in self-supervised pretraining, we substitute the supervised pre-training task with self-supervised masked reconstruction tasks [17] tailored to trajectory forecasting, i.",1,related,1,positive
"Inspired by Masked Image Modeling (MIM), we design a new pre-training strategy for both spectral and spatial transformers.",1,related,1,positive
We also apply MAE [22] and BEiT [1] as teacher networks.,1,related,1,positive
"We first pre-train a MIM model, which could be MAE [22] or BEiT [1] as the teacher model.",1,related,1,positive
"For setting a solid baseline, we enhanced OSTrack by substituting its MAE [19] pretrained weights with those of CAE [20], the outcome of which is enumerated in Tab.",1,related,1,positive
"Our approach is an extension of the masked autoencoder [1], [2] for time-sequential trajectory data and aims to",1,related,1,positive
"Demandconditioned features are concatenated with a bounding box and logits and input into a Transformer Encoder, then passed into a Transformer Decoder with Demand BERT features and global visual features (encoded by a pre-trained Vision Transformer [57, 58]).",1,related,1,positive
"Image Encoder: Vision Transformer We use the mae_vit_large_patch16 version of Vision Transformer [57, 58] as the Image Encoder to obtain global visual features.",1,related,1,positive
We adopt a commonly used linear probing protocol [8] for evaluation.,1,related,1,positive
"We use the Masked Autoencoder (MAE) architecture [18], which enables efficient pretraining.",1,related,1,positive
"We use the assymetric encoder-decoder architecture proposed in [18], with a lightweight decoder and a larger encoder.",1,related,1,positive
"We choose YOLO over a masked autoencoder model [15] since the latter focuses more on image reconstruction, which is not aligned with our pressure map data in signal format.",1,related,1,positive
We initially selected the MAE trained solely on the image modality as our baseline model.,1,related,0,negative
We then incorporated CLIP into MAE (denote as +CLIP) to achieve granular alignment.,1,related,1,positive
"Here, we adopt MAE proposed by [4] as our primary image representation extractor.",1,related,1,positive
MAE achieved AUC of 81.3% on the RSNA dataset with 1% annotations.,1,related,0,negative
"For both the pre-training and fine-tuning of the image classification task, we warmed up the network by linearly increasing the learning rate to the set value, and then decreased the learning rate using the cosine decay schedule, following the approach reported in MAE.",1,related,1,positive
"For mask image modeling, we explore several state-of-the-art methods as baselines,
including MAE, Maskfeat, SimMIM, ConvMAE, BEiT v2, and BootMAE.",1,related,1,positive
"The initial masking ratio, denoted as œÉ0, is set at 25%, a value determined based on experimental findings in MAE.",1,related,0,negative
"To investigate the effectiveness of AMLP, we conduct experiments on two datasets and compare its performance with two state-of-the-art baselines: Fully Supervised Baseline (i.e., Fully Supervised) and Self-Supervised Baselines (i.e., SimCLR, BYOL, SwAV, MAE, Maskfeat, SimMIM, ConvMAE, BEIT V2, BootMAE).",1,related,1,positive
"For mask strategy, we follow the setting of MAE [11], and only the unmasked token is used during pre-train.",1,related,0,negative
"In detail, we use a pre-trained ViT-b/16 (with MAE [11] on ImageNet 1K for 1600 epochs) as initial parameters and refine on LAIONFACE-cropped for 16 epochs with our Mask Contrastive Face.",1,related,1,positive
"Instead of minimizing the pixel loss as previous methods [11, 25], we minimize the feature map difference between the original face image I and the pseudo image outputted byM0.",1,related,1,positive
"(1) L2, a mask image modeling with L2 loss, which is close to MAE [11], and in order to learn the globe face identity, we remove the random resize crop operation during data augmentation; (2) L2 + Lmim , which directly adds theM1 decoder at the end of MAE decoder; (3)Lmim , a mask image model with 8-layerM0 and aM1 decoder; (4) Lmim , Table 7: Comparison with previous representation learning.",1,related,1,positive
"In addition, with the popularity of [40, 41], We will further investigate how to pre-train the Transformer-based model on unlabelled medical images by a self-supervised approach and combine it with our proposed methods.",1,related,1,positive
"S1), we used a diverse set of training dataset and employed the Masked Autoencoders (MAE) method [He et al., 2022] as the Pre-training method.",1,related,0,negative
"Upon adopting this strategy, various methods are available for its implementation, and in this study, we chose the Masked Autoencoders (MAE [He et al., 2022]) method (Supplementary Fig.",1,related,1,positive
"NNCLR along with other methods, e.g. MoCo Chen et al. [2020b], has adopted memory banks in their scheme to maintain the support set of nearest neighbors.",1,related,1,positive
"we reconstruct the missing feature embeddings rather than the raw pixels of the masked areas, which is different from traditional MIM methods like SimMIM [19] and MAE [5].",1,related,1,positive
"Note that
we reconstruct the missing feature embeddings rather than the raw pixels of the masked areas, which is different from traditional MIM methods like SimMIM [19] and MAE [5].",1,related,1,positive
Backbone APbox APmask Params with Mask R-CNN: MAE-B [8] 49.,1,related,1,positive
"At the lowest level of the last top-down column, we use mean-square error (MSE) loss to reconstruct raw images, similar to [8].",1,related,1,positive
"3% top-1 accuracy outperforms ConNeXt V2 [12], MAE [8], and CAE [14] counterparts.",1,related,1,positive
"We initialize weights using MIM pre-trained models, and fine-tune on ImageNet-1K with class label, similar to [8, 12, 9].",1,related,1,positive
"We benchmark four representative methods‚ÄîMoCo [6], DINO [7], MAE [8], and data2vec [9]‚Äîon the proposed dataset.",1,related,1,positive
"SELF-SUPERVISED PRETRAINING We performed pretraining using four representative SSL methods: MoCo-v2/v3 [15], [16], DINO [7], MAE [8], and data2vec [9].",1,related,1,positive
"For viewport reconstruction, we choose the MAE architecture [17], which is a well-known encoder-decoder architecture implemented by Vision Transformers (ViT) [47], to reconstruct the viewport frames.",1,related,1,positive
"We conduct nearest neighbor, linear probing and fewshot linear probing experiments on five downstream image classification tasks, comparing the representation learning capabilities of MAE [23] and CL-MAE, upon selfsupervising both models on ImageNet [39].",1,related,1,positive
‚Ä¢ We introduce curriculum learning into the MAE framework [23] to learn robust representations.,1,related,1,positive
"To this end, we propose a novel masking module, which is trained in an end-to-end fashion along with the MAE backbone [23].",1,related,1,positive
"We introduce curriculum learning as the core element of our proposed method, while using MAE [23] as the underlying backbone for representational learning.",1,related,1,positive
"Specifically, we adopt the state-of-the-art human pose estimation model ViTPose [75] trained on COCO [38], with the ViT-L backbone [14] initialized with MAE [19] pretrained weights.",1,related,1,positive
"Inspired by the content reasoning ability of masked autoencoders (MAE) [9], our framework endows the holistic learning process of deep unfolding with the explicitly integrated with inherent physical mechanism underlying the pan-sharpening task.",1,related,1,positive
"‚Ä¢ We fine-tuned DINO-ViT, DINO-ResNet-50 [30] and ViT-MAE models [31] 108 within their respective self-supervised frameworks, using CheXpert [32] X-ray 109 image dataset.",1,related,1,positive
"For a grid with all-zero pixels, we use the one mask embedding p[M ] ‚àà R(3) [20] as a replacement:",1,related,1,positive
", relative position embedding (MSA-RP [17]), learnable position embedding (MSA-LP [7]), and 2D sine-cosine position embedding (MSA-SP [8]).",1,related,1,positive
"Fine-tuning on AudioSet-20K, achieving 34.8%, CMMixer-Solo significantly outperforms concurrent MAE-AST [59], which trained with an additional 1,000 hours of speech in Librispeech while we only fine-tune without off-domain pretraining.",1,related,0,negative
"However, we observe that we are still a little short of the best model AudioMAE, which is both pre-trained and fine-tuned on AudioSet and has a larger size of the model.",1,related,0,negative
"Second, because of the heavy spatial redundancy in images [10], we randomly drop part of the data to improve encode efficiency after mixing them to gain a mixture.",1,related,1,positive
"Therefore, we draw lessons from the successes of BERT [9] in NLP and MAE [10] in CV, innovating an effective pre-processing method of multimodal data and leveraging a mixer to emphasize the frame-level synchronization.",1,related,1,positive
"On the one hand, the CMMixer-Solo achieves 0.6%, 0.2%, and 0.4% higher accuracy than BEVT, VideoMAE-B and ST-MAE-B respectively.",1,related,0,negative
"‚Ä¢ We develop a new multimodal pre-training framework based on Masked
Autoencoder (MAE) that uses a single encoder-decoder structure.",1,related,1,positive
"Based on VideoMAE [33] and AudioMAE [31], we divide the video into n clips (with a default setting of 8) and extract one frame and 2.6 seconds waveform with a 16,000 sampling rate from the middle of each clip.",1,related,1,positive
"We compare Mean Squared Error (MSE), a loss function previously used in masked autoencoders [5], against Mean Absolute Error (MAE) and the recently proposed Barlow Twin Loss [52].",1,related,1,positive
‚Äôs Masked Autoencoder for image classification [5].,1,related,1,positive
"From Table 3, we can first conclude that it is necessary to retrain the image encoder when applied to the nuclei segmentation task as retrained Tiny-ViT outperforms pre-trained MAE ViT.",1,related,1,positive
"To reduce the parameters of image encoder, the MAE ViT of SAM is replaced with a TinyViT [16].",1,related,1,positive
"Our unlabeled dataset might also facilitate general unsupervised representation learning [2, 9, 11, 24, 29].",1,related,1,positive
"With respect to distinct backbones, we adopt the widely used residual networks including ResNet(R)-50/-101/-152 and ResNeXt(RX)-101/-152 [60] pretrained by InfoMin [55], as well as the vision transformers including ViT-Base (ViT-B)/-Large (ViT-L) [14] pretrained by MAE [22].",1,related,1,positive
"We implement a common pretext task that reconstructs pixels for masked regions (Atito, Awais, and Kittler 2022; He et al. 2022).",1,related,1,positive
We adopt the Vision Transformer (ViTbase) [34] model pre-trained using the MAE [17] method as the backbone for extracting visual features.,1,related,1,positive
"We employ the checkpoints of ViT-small/-base (Dosovitskiy et al. 2020) and HiViT-base (Zhang et al. 2022c), which are all pre-trained using the MAE (He et al. 2022) selfsupervised strategy.",1,related,1,positive
We discover that employing the imTED structure (Zhang et al. 2022b) alone (i.e. MAEBBoxHead only) struggles to surpass the baseline performance.,1,related,1,positive
"Please note that the ViT-small backbone is obtained from the MAE pretrained encoder, and the 4-layer Transformer block is derived from the pre-trained decoder, which forms the MAEBBoxHead module.",1,related,1,positive
"Therefore, we adopt a design inspired by the imTED (Zhang et al. 2022b) detector and substitute the backbone as well as head modules of the two-stage detector with Vision Transformer blocks pre-trained using the MAE method.",1,related,1,positive
We primarily draw upon the training recipes in MAE (He et al. 2022) for stable training.,1,related,0,negative
"Moreover, we distinctly remove the strategies of random erasing and exponential moving average (EMA) in MAE.",1,related,1,positive
"Motivated by the Masked Auto-Encoder (MAE) [21], which recovers images with only some patches of them, we drop less important patches to reduce the number of pixels utilized for describ-",1,related,1,positive
"At the training stage, we reconstruct training images via important patches and a pre-trained MAE [21] model.",1,related,0,negative
"At the training stage, we employ a strong pre-trained MAE decoder to reconstruct the dropped patches and the original images.",1,related,0,negative
The samples are originally selected by ResNet-18 and reconstructed with MAE.,1,related,0,negative
"Motivated by the Masked Auto-Encoder (MAE) [21], which recovers images with only some patches of them, we drop less important patches to reduce the number of pixels utilized for describ-
Algorithm 1 Data bin generation.",1,related,1,positive
"Specifically, we experiment with a ViT-B/16 model pre-trained unsupervisedly by Masked Auto-encoder [13].",1,related,1,positive
"For the audio encoder and the visual encoder, inspired by the excellent performance and generalization capability of recent vision/audition selfsupervised pre-trained models [17, 31, 32, 36, 85], we apply",1,related,1,positive
"We provide the adversarial robustness results to l2-norm attack for ViTs that are equipped with different training methods, including the supervised baseline, MoCo v3 [8], BEiT [3], PeCo [16] and MAE [25].",1,related,1,positive
"From the quantitative results listed in Table 7 and Figure 3, we can surprisingly find that, though MAE [25] pretrained model performs better or comparably on clean images, it encounters much larger performance drop on adversarial samples.",1,related,0,negative
"Self-supervised learning and revised the masked autoencoder Following the MAE [14], we auto-encode the images by reconstructing the original images with only partial observations.",1,related,1,positive
"Following [13, 15, 37], we implement end-to-end fine-tuning, semi-supervised learning, and transfer learning to evaluate the pre-trained MaST-Pre.",1,related,1,positive
"Drawing inspiration from the effective representation learning of masked image autoencoders [16], we introduce masked motion modeling, a technique that involves temporally masking a random portion of the input motion sequence at a ratio r , and subsequently requiring the model to reconstruct the entire motion sequence.",1,related,1,positive
"First, a traffic patch encoder is pre-trained by the data of source cities in the fashion of the Masked Autoencoder [14, 37].",1,related,1,positive
"Here, we propose a traffic patch encoder pre-training framework based on SOTA methods STEP [37] and MAE [14].",1,related,1,positive
"We set the mask ratio to 75% according to the original papers [14, 37].",1,related,0,negative
We randomly mask out 75% of total image patches following MAE[23].,1,related,0,negative
We follow the default finetuneing parameters of the MAE[23].,1,related,1,positive
Encoder Only visible patches xp = {xp|M i = 1} are fed to the encoder following[23] and mapped to the high resolution patches xe across a stack of transformer blocks.,1,related,1,positive
Only the loss generated by the parts replaced by xl is calculated[23].,1,related,1,positive
"Comparing to previous popular methods based on masking/denoising autoencoding like masked autoencoder (MAE) [25] and denoising autoencoder (DAE) [56], which are mainly designed for model pre-training, we treat the denoising and masking prediction as auxiliary tasks to aid the primary fully-supervised motion prediction task and jointly learn all the tasks together.",1,related,1,positive
"1) Comparisons with Prior Arts: First, we use the 3D-Unet [37] as the segmentation network backbone, our MLM based on [29].",1,related,1,positive
Inspired by the popular masked image modeling (MIM) methods [29]‚Äì[32] we introduce the MLM for modeling shape.,1,related,1,positive
"We implement the above two pipelines into our method with the following experiments: 1) meaning the output representations Er ‚àà RN√óD into a vector and projecting it to regress the distortion parameters, and 2) reshaping the output Er to form a reconstructed image, following MAE [28].",1,related,1,positive
"Before the official training, an MAElike [32] unsupervised warming-up phase is deployed to upgrade robustness as described in Appendix B.",1,related,0,negative
"We follow the encoder-decoder design in MAE [17], where the transformer encoder focuses on representation learning, while the decoder is responsible for the implementation of the pre-training pretext.",1,related,1,positive
"Mask Sampling Strategy: In our approach, we employ the vanilla transformer as the backbone network, where embedding features at any spatio-temporal location can be freely masked as in MAE [17].",1,related,1,positive
"C V
] 1
4 A
ug 2
02 3
the recent success of masked autoencoders in reconstructing images (He et al. 2021; Bao, Dong, and Wei 2021), and videos (Tong et al. 2022), we adapt the motion synthesis as a reconstruction problem: to recover a sequence of masked human skeletons regardless of the masking‚Ä¶",1,related,1,positive
"Unlike previous works (He et al. 2021; Tong et al. 2022; Li et al. 2022), we do not neglect the masked patches in the encoder phase, as all patches E have relevant information for the model (due to the adaptation of X into Xfill).",1,related,1,positive
"We employ MAE [5], a representative masked modeling method.",1,related,1,positive
"MS-COCO
tr@1 ir@1
real 58.1 44.2
VQGAN [64] 35.6 32.0
DALL-E2 [65] 44.5 38.6
Stable [34] 52.3 40.9
w/ ours 54.9 (+2.6) 43.8 (+2.9)
mix w/ ours 60.8 46.2
Table 4: MAE [5] downstream results.",1,related,1,positive
"For classification tasks, we utilize the Masked Auto Encoder (MAE) [5].",1,related,1,positive
"‚Ä¶and thus, provides direct comparisons with NLA, 2) LFADS has been frequently employed in this field, and 3) NDT is chosen since the masked autoencoder has been proposed for a de facto standard of representation learning in many domains (Devlin et al., 2018; He et al., 2022; Tamkin et al., 2022).",1,related,1,positive
"The rationale for this selection is that 1) SeqVAE shares the backbone autoencoder with NLA, and thus, provides direct comparisons with NLA, 2) LFADS has been frequently employed in this field, and 3) NDT is chosen since the masked autoencoder has been proposed for a de facto standard of representation learning in many domains (Devlin et al., 2018; He et al., 2022; Tamkin et al., 2022).",1,related,1,positive
"[11], we leverage a self-supervised learning strategy based on masking.",1,related,1,positive
"We considered the following three pretext tasks: our custom 1) Vertex Normal Prediction (VertNormPred), 2) ¬µCT volume Reconstruction (ReconCT), and 3) Masked ¬µCT volume Reconstruction (MaskReconCT) tasks.",1,related,1,positive
"3(b) for ReconCT and MaskReconCT tasks, where we minimize the L1 loss between the generated and original 5√ó5√ó5 ¬µCT node-wise subvolumes.",1,related,1,positive
"In the MaskReconCT [8] task, we focus on reconstructing pixel-wise randomly masked 5 √ó 5 √ó 5 ŒºCT subvolumes using an encoder-decoder network shown in Fig.",1,related,1,positive
"In the MaskReconCT [8] task, we focus on reconstructing pixel-wise randomly masked 5 √ó 5 √ó 5 ¬µCT subvolumes using an encoder-decoder network shown in Fig.",1,related,1,positive
"Even contrastive implementations using augmentations tailored to 12-lead ECG data, such as CLOCS [11], are outperformed by the MAE, highlighting our decision to integrate MDM into our proposed MMCL.",1,related,0,negative
2) Training Strategy: We first pre-train our unimodal signal encoder fs(¬∑) using the MAE [16].,1,related,0,negative
"(1)
We train the MAE by optimizing the mean squared error (MSE) loss, which can be lower bounded by a global alignment loss with respect to the embeddings fs(xv) and fs(x‚Ä≤v), under the assumption that g(¬∑) is L-bi-Lipschitz [36]:
LMSE = Ep(x)Ep(xv|x)||g(fs(xv))‚àí x|| 2 (2a)
‚â• ‚àí 1 2L Ep(xv,x‚Ä≤v|x)fs(xv) ‚ä§fs(x ‚Ä≤ v)‚àí œµ+ const.",1,related,1,positive
We also employ a MAE during the initial step of our framework and further introduce unstructured masking similar to [18] to learn temporally encoded local and global ECG features.,1,related,1,positive
"With heavy masking ratios, we find the MAE able to eliminate redundancy present in the 12-lead ECG across signal channels and time.",1,related,0,negative
"Let fs(¬∑) and g(¬∑) be the signal encoder and decoder, respectively, such that the MAE can be represented as h(¬∑) = (g ‚ó¶ fs)(¬∑), where
xÃÇ = h(xv) = g(fs(xv)).",1,related,1,positive
We base our MDM implementation on the MAE [16].,1,related,1,positive
"Furthermore, to evaluate the effectiveness of the MAE [16] for learning rich ECG embeddings, we compare it to a fully supervised model and multiple unimodally pre-trained contrastive models.",1,related,1,positive
"We note that the masking ratio is set to 50% since it achieves the best BP estimation performance in our experiments when it ranges from 15% to 75% following the existing experience [48, 50].",1,related,0,negative
We adopt the vanilla ViT-Base [16] model pre-trained with MAE [25] on ImageNet-1k to initialize the backbone of our ROMTrack.,1,related,1,positive
"Since OSTrack [57] also adopts MAE pretrain, we would like to compare with it.",1,related,0,negative
We adopt the vanilla ViT-Base [16] model pretrained with MAE [25] on ImageNet [15] as the backbone of our ROMTrack.,1,related,1,positive
"Inspired by the masked autoencoder [13], Wu et al.",1,related,0,negative
"We use masked autoencoders (MAEs) as our SSL algorithm of choice (He et al., 2022).",1,related,1,positive
"I would like to thank the authors of the MAE paper (He et al., 2022) for making their code available.",1,related,0,negative
"We choose MoCo-v3 [9] and MAE (Masked AutoEncoder) [16] for contrastive learning and masked image modeling, respectively.",1,related,1,positive
"For network architecture, we use ViT-Base [15] as feature extractor containing lm = 11 MSA blocks and lt = 1 TSA block, where the parameters are initialized via [22].",1,related,1,positive
"From Table 4, we can see that directly fine-tuning the vanilla ViTMAE pre-trained model with 16√ó 16 patch numbers has lower accuracy.",1,related,1,positive
"Motivated by ViTMAE, we adopt a masked image modeling (MIM) task to help the diagram encoder effectively extract geometry features and learn geometry primitives.",1,related,1,positive
"Similar to ViTMAE [11], we calculate the mean squared error (MSE) between the reconstructed and original image patches at pixel-level as the loss function Lmim for the MIM task, which can be formulated by",1,related,1,positive
"In addition, we enhance the geometry diagram understanding ability via a self-supervised learning method with the masked image modeling auxiliary task [11].",1,related,1,positive
"To verify the effectiveness of our proposed auxiliary tasks for the training diagram encoder, we conduct a series of experiments: (1) training the encoder by themasked imagemodeling (MIM) task from a pre-trainedmodel with the vanilla ViTMAE configuration (i.e., 16 √ó 16) on ImageNet; (2) training the encoder by the masked image modeling (MIM) task from random initialization with 8 √ó 8 patching; (3) training the encoder by the multi-label classification (MLC) task from random initialization; (4) training the encoder by a combination of MIM and MLC tasks.",1,related,1,positive
"Inspired by ViTMAE, we utilize a masked image modeling task to pre-train our Transformer-based diagram encoder, where we specifically design the mask percentage as our diagram images mainly contain straight lines and annotated characters without those sufficient textures in natural images.",1,related,1,positive
"Similar to ViTMAE [11], we calculate the mean squared error (MSE) between the reconstructed and original image patches at pixel-level as the loss function Lùëöùëñùëö for the MIM task, which can be formulated by
Lùëöùëñùëö = ‚àëùëÅ ùëñ=1 ùêøùëñ ¬∑ùëÄùëñ‚àëùëÅ ùëñ=1ùëÄùëñ ,where ùêøùëñ = 1 ùê∂ ùê∂‚àëÔ∏Å ùëó=1 ( ùëÉùëñ ùëó ‚àíùëáùëñ ùëó )2 .",1,related,1,positive
"where xvs , xvt are data samples from source and target cities,LMaskedAE denotes the loss of masked autoencoding [14], and Œªd is a hyperparameter.",1,related,1,positive
The source feature network is pre-trained with masked autoencoding [14] to reconstruct masked patches using unmasked ones in a long input sequence x ‚àà R ¬∑P .,1,related,1,positive
"We use the default setup, described in Section 5.4 of the main text, to adopt a MAE [20] pre-trained ViTBase as the backbone and train the model for ‚àº50 epochs.",1,related,1,positive
"For the fair comparison, We use a MAE [20] pretrained ViT-Base as the backbone and train the object detector for ‚àº50 epochs.",1,related,1,positive
"Therefore, we investigate the influence of model scale on social biases, using iGPT [8] and ViT-MAE [13], as both have been trained using selfsupervised methods and are available in three different model sizes.",1,related,1,positive
"In contrast, we observe the opposite effect on ViT-MAE, where it comes with a small increase in gender bias.",1,related,0,negative
"Models In our experiments, we use BEiT [3], ViTMoCo [10] and ViT-MAE [13], which use a standard Transformer as the backbone network (12 layers, 12 attention heads, 768 hidden size).",1,related,1,positive
"To this end, we adopt the standard contrastive learning objective for ViT-MoCo [10] and masked image modeling training objective for BEiT and ViT-MAE with a masking ratio of 40 % [3] and 75 % [13], respectively.",1,related,1,positive
"To evaluate whether the observed effects on ViT-MoCo and ViT-MAE are a result of their pre-trained checkpoints, we train them from scratch on ImageNet-1k and our counterfactual data (2-sided CDA).",1,related,0,negative
"Generative methods [3, 10, 19, 65] reconstruct the original input sample from the corrupted one.",1,related,1,positive
"2, we apply MFF to two MIM baselines, namely MAE [14] and PixMIM [30], and show the improvements brought by such design.",1,related,1,positive
"We evaluate it on MAE[14] and PixMIM[30], and more detailed results are shown in Section 5.",1,related,0,negative
"In order to investigate whether being biased towards low-level features is the sole and inherent drawback of pixel-based MIM, we introduce multi-level feature fusion to EVA[10] and supervised ViT[14].",1,related,1,positive
"We employ multi-level feature fusion to enhance MAE [14], resulting in MFFMAE.",1,related,1,positive
"3, we apply the first pilot experiment introduced in Section 1 to EVA [10] and supervised ViT [14], to investigate whether they too require lowlevel features and to confirm that the bias towards low-level details is the unique and inherent drawback of pixel-based MIM.",1,related,0,negative
"As shown in Figure 5, the expected hessian max eigenvalue of MFFMAE is smaller than that of MAE[14].",1,related,1,positive
We choose the MSE loss instead of MAE to implement better robustness.,1,related,1,positive
"7, which is usually composed of MSHA; besides, we remove the mask operation, and the
decoder is retained in MAE.",1,related,0,negative
"To extract the global feature information of the image, we employ the encoder and decoder in the Masked Autoencoders Are Scalable Vision Learners (MAE) [18], as shown in Fig.",1,related,1,positive
"Notably, we use MAE to refer to the method in [He et al., 2022] not as shorthand for masked autoencoder to avoid confusion.",1,related,1,positive
"In other words, the success of masked autoencoder in vision paves a path: SSL in vision‚Äúmay now be embarking on a similar trajectory as in NLP‚Äù [He et al., 2022].",1,related,1,positive
"Therefore, this section starts with introducing BEiT with its improved variants and then discusses the seminal work MAE [He et al., 2022] .",1,related,1,positive
"Despite high similarity regarding pretext task, the masked autoencoder introduced in [He et al., 2022] differs from early denoising autoencoder [Vincent et al.",1,related,1,positive
"In our initial experiments, we found all these models to perform better than MAE [35], which only has token-level selfsupervision.",1,related,1,positive
"initial experiments, we found all these models to perform better than MAE [35], which only has token-level selfsupervision.",1,related,1,positive
"In this work, we take the visiontransformer model (ViT) as the backbone model, which is pre-trained in ImageNet-1K by masked auto-encoder (MAE) [31].",1,related,1,positive
"The gating part of Gate-DAP is:
ZIt = ViT|MAE(It), (4) Z‚Ä≤It = SpaG(Z I t ), (5)
HIt = MemoG(H I t‚àí1, [Z ‚Ä≤I 1, ...,Z ‚Ä≤I t ]), (6)
[M‚Ä≤It ,M ‚Ä≤F t ,M ‚Ä≤S t ,M ‚Ä≤D t ] = MU-InfoG(H I T ,H S T ,H F T ,H D T ), (7)
M‚Ä≤t = Stack[M‚Ä≤ I t ,M ‚Ä≤F t ,M ‚Ä≤S t ,M ‚Ä≤D t ].",1,related,1,positive
"In our approach, we adopt the masked autoencoder [19] to improve the channel estimation performance.",1,related,1,positive
"Clevr/Count Clevr/Dist
MIM methods, without view data augmentations data2vec [3] ViT-L/16 72.7 53.0
MAE [24] ViT-B/16 86.6 70.8 ViT-L/16 92.1 73.0
I-JEPA [1] ViT-B/16 82.2 70.7 ViT-L/16 85.6 71.2
Low-level vision.",1,related,1,positive
"Epochs Top-1
MIM methods, without view data augmentations data2vec [3] ViT-L/16 1600 53.5
MAE [24] ViT-B/16 1600 68.0 ViT-L/16 1600 76.0
I-JEPA [1] ViT-B/16 600 72.9 ViT-L/16 600 77.5
Method Arch.",1,related,1,positive
"CIFAR100 Places205 iNat18
MIM methods, without view data augmentations data2vec [3] ViT-L/16 59.6 36.9 10.9
MAE [24] ViT-B/16 68.1 49.2 26.8 ViT-L/16 77.4 54.4 33.0
I-JEPA [1] ViT-B/16 69.2 53.4 43.4 ViT-L/16 83.6 56.5 48.4
Dense prediction.",1,related,1,positive
"J-Mean F-Mean J&F Mean
MIM methods, without view data augmentations MAE [24] ViT-B/16 49.4 52.6 50.9 ViT-L/16 52.5 54.3 53.4
I-JEPA [1] ViT-B/16 56.1 56.2 56.1 ViT-L/16 56.1 55.7 55.9
Method Arch.",1,related,1,positive
"Following past works, we focus on evaluating the (target) encoder representations [24, 1], and use the standard VISSL [21] evaluation protocol like in [1].",1,related,1,positive
"To further explore the efficacy of the learned features, we employ a partial fine-tuning method based on the protocol proposed in [26].",1,related,1,positive
"To accomplish this, we employ a Masked Autoencoder (MAE) model [30], a self-supervised learning approach that utilizes the ViT model as its backbone.",1,related,1,positive
"During the exploration, we found an innovative application of MAE on the limited dataset, which is not studied by the previous work [30].",1,related,0,negative
"For the former stage, we resort to masked autoencoders (MAE) [31] and image mixing techniques to enhance representation learning of self-supervised depth estimation models.",1,related,1,positive
"In this challenge, we directly load a pre-trained MAE model [31] for image reconstruction of the input image x.",1,related,1,positive
We initialized its parameters with ImageNet-1k MAE [11] pre-training; 2) a simple feature pyramid [18] network to introduce multiscale supervision; 3) a morphology-based edge loss strat-,1,related,1,positive
MAE pre-train We initialize the ViT with parameters pretrained on ImageNet-1k [5] with Masked Auto Encoder (MAE) [11].,1,related,1,positive
"We initialized its parameters with ImageNet-1k MAE [11] pre-training; 2) a simple feature pyramid [18] network to introduce multiscale supervision; 3) a morphology-based edge loss strat-
egy is proposed to ensure edge supervision.",1,related,1,positive
"As shown in w/o MAE aspects in Table 3, the use of Xavier initialization to train the model resulted in complete non-convergence.",1,related,1,positive
"We initialize ViT-B with MAE pre-trained weights on ImageNet1k and used the AdamW optimizer [21] with a base learning
1We noticed some resolution errors in the public CASIAv2 dataset.",1,related,1,positive
"For initialization, besides full setup with MAE pre-training on ImageNet-1k, we test Xavier initialization and ordinary ViT pre-training on ImageNet-21k by classification.",1,related,1,positive
"In conclusion, our findings are:
MAE pretrain is mandatory.",1,related,0,negative
", resulting from unsupervised learning [58], [59], [60]) will be ablated in Section 5.",1,related,0,negative
"2) initializations from unsupervised pre-training, including MoCo [58], MAE [59], and CLIP [60].",1,related,0,negative
Here we compare MAE and MSE with the default PLCC.,1,related,1,positive
"Specifically, we initialize ResNet-50 with weights supplied by MoCoV2 and CLIP, ViT-B by MAE and CLIP, and Swin Transformer-B by ImageNet-22k.",1,related,1,positive
"Here, we further explore other possibilities: 1) initializations from pre-training on larger computer vision datasets (e.g., ImageNet-22k) and
2) initializations from unsupervised pre-training, including MoCo [58], MAE [59], and CLIP [60].",1,related,1,positive
"We first arrange the features in descending order based on their Fisher Score, and then select a predetermined proportion of top-ranked features to retrain a classifier to probe the quality of features [30], [31], [32].",1,related,1,positive
"the authors perform a 2D-sine-cosine linear embedding on the patches which are fed as input to the multimodal ViT encoder which operates only on the visible tokens, tremendously reducing the cost of computation [179].",1,related,1,positive
"MAEs [179] are asymmetric encoderdecoder models in which the encoder only operates on a small
portion (about 15",1,related,1,positive
MAEs [179] are asymmetric encoderdecoder models in which the encoder only operates on a small,1,related,1,positive
"This functional copy, which we call the model EMA, has a number of desirable properties: i) the model EMA inhabits wider minima than the target model, reducing overfitting and improving generalization (Huang et al., 2017; Izmailov et al., 2018; He et al., 2022); ii) compared to the target model, the model EMA moves slowly, making it useful as a stabilizer for networks governing Bellman updates in reinforcement learning, (Lillicrap et al.",1,related,1,positive
"Besides DeiT-Base, we also initialize MGP-STR with various recent pre-trained ViT backbone models (i.e. , DINO [96], DINOv2 [97], MAE [98] CLIP [14] and BLIP [99]) to verify
the effectiveness of our method.",1,related,1,positive
"(3) We verify various recent ViT backbone models (such as those from MAE, DINO and BLIP) and show that, once adequately trained with enough domain-specific data (text images in this work), these ViT models can work equally well on the task of scene text recognition (see Sec.",1,related,1,positive
We also apply E(2)VPT to two self-supervised objectives: MAE [24] and MoCo v3 [10].,1,related,1,positive
"We respectively examine the performance and robustness of E(2)VPT on ViT [12], Swin [54], and two selfsupervised objectives ‚Äî MAE [24] and MoCo v3 [10].",1,related,1,positive
"We conducted experiments with two self-supervised objectives, MAE [24] and MoCo v3 [10], on backbones pretrained without labeled data, following the approach of VPT [34].",1,related,0,negative
"Following [11], we adopt Mean-SquareError(MSE) to compute the reconstruction loss:",1,related,1,positive
"Following [11], we divided the 3D images into sub-volumes of the same size and randomly masked a portion of them, as demonstrated in Figure 2.",1,related,1,positive
"Furthermore, We consider an additional baseline for model adaptation based on the patch reconstruction objective in [13] on pooled normal and unlabeled images, denoted as (AMAE - Stage 2 (Mask Rec.",1,related,1,positive
"As for Stage 1 of our method, (i) AMAE creates synthetic anomalies from only normal training images, and the usefulness of pre-trained MAE [13] is evaluated by training a lightweight classifier using a proxy task to detect synthetic anomalies.",1,related,0,negative
"For that, we use a ViT/MAE [5,11] encoder E consisting of alternating blocks of multi-head selfattention and multi-layer-perceptrons.",1,related,1,positive
"To do this, we employed a pre-trained feature extractor model, either a supervised model such as a pre-trained CNN (e.g., EfficientNet (Tan & Le, 2019)) or a pre-trained model in an unsupervised fashion such as a masked autoencoder (e.g., ViTMAE (He et al., 2022) and MultiMAE (Bachmann et al., 2022)).",1,related,1,positive
"‚Ä¶do this, we employed a pre-trained feature extractor model, either a supervised model such as a pre-trained CNN (e.g., EfficientNet (Tan & Le, 2019)) or a pre-trained model in an unsupervised fashion such as a masked autoencoder (e.g., ViTMAE (He et al., 2022) and MultiMAE (Bachmann et al., 2022)).",1,related,1,positive
"Given the final video frame-level representations X and text word-level X, we first randomly replace a noun phrase or verb phrase representations with mask embeddings [75], where each mask token is a shared, learned vector.",1,related,1,positive
"For self-supervised objectives (i.e., MAE and CLIP), we use ViT-Base/16, ViT-Large/14, ViT-Large/16 and ViT-Huge/14 as basic backbones.",1,related,1,positive
"To explore the effect of different pre-training strategies, we conduct experiments by using various backbones under supervised (SUP [10]) and self-supervised (MAE [15], CLIP [39]) settings.",1,related,1,positive
"Additionally, we employ ViT-Base/16, ViT-Large/14, ViT-Large/16 and ViT-Huge/14 for assessing generalization to different pre-training strategies (e.g., MAE [15] and CLIP [39] ).",1,related,1,positive
"For fine-tuning models pre-trained by MAE [15], we follow its official configurations of LP on ImageNet-1K, which adopt a linear scaling rule [13].",1,related,1,positive
"B.1 Video Recognition
For video recognition, we follow VideoMAE [111] to modify the tokenizer by replacing the 2D embedding layer with a 3D embedding layer to simultaneously encode the spatial-temporal information from input frames.",1,related,1,positive
"Method Modality UCF101 Params OPN [109] V 59.6 - SimCLR [110] V 88.9 86.9M VideoMAE V1 [111] V 96.1 86.9M VideoMAE V2 [112] V 99.6 86.9M ViT [13] (from scratch) V 51.4 86.9M Meta-Transformer-B16F V 46.6 1.1M
cantly reduced trainable parameter count, suggesting the potential benefit of unified multi-modal learning and less architectural complexity.",1,related,1,positive
"We choose MoCo-v3 [24] and MAE [14] as archetypes of contrastive and restorative SSL respectively, using a ViTB architecture (with 12 layers) for both methods to ensure a fair comparison.",1,related,1,positive
"restorative pre-training approach is the masked autoencoder (MAE) [14], which employs an asymmetric encoder-decoder architecture and masks 75% of the image.",1,related,1,positive
"In this section, we set up strong contrastive and restorative self-supervised baselines for medical imaging based on the robust self-supervised learning methods MoCo-v3 [24] and MAE [14].",1,related,1,positive
"Here, we provide a brief overview of the MoCo-v3 [24] and MAE [14] methods, which we use to build strong contrastive",1,related,1,positive
We adopt the same fine-tuning settings as MAE [14] and fine-tune on a single V100 GPU.,1,related,1,positive
"In this paper, we leverage the MAE method [14] to develop optimal fine-tuning strategies that effectively utilize restorative pre-trained features for medical imaging analysis.",1,related,1,positive
"‚Ä¶Supervised is pre-trained using supervised learning on ImageNet-1k, SWAG is Supervised Weakly from hashtAGs (Singh et al., 2022), CLIP is Contrastive Language-Image Pretraining (Radford et al., 2021), and MAE is for masked-autoencoder (He et al., 2022), a generative pre-training technique.",1,related,1,positive
The first is the limited precision of the ViTMAE tokenization of the patches; here we have adopted 256 dimension latent vector.,1,related,1,positive
"We thereby construct Enki, a ViTMAE model trained on SST ocean model outputs that may then be applied to actual remote sensing data.",1,related,1,positive
"While [8] advocates t%=75 to insure generalization, we generated Enki models with t%=[10,20,35,50,75].",1,related,1,positive
"In this study, we use the fine-scale (1/48‚ó¶, 90-level) ocean simulation from the Estimating the Circulation and Climate of the Ocean (ECCO) project and referred to as LLC4320 to train an implementation of the ViTMAE.",1,related,0,negative
"We have designed and trained a machine learning model, inspired by ViTMAE [8], named Enki to reconstruct masked SST fields [12].",1,related,1,positive
"In this manuscript, we introduce a novel approach, inspired by the vision transformer masked autoencoder (ViTMAE) model of [8] to reconstruct masked pixels in satellite-derived fields.",1,related,1,positive
"As described in Section 3, we trained Enki with a range of training mask percentiles expecting best performance with t%=75 as adopted by [8].",1,related,0,negative
"A primary hyperparameter of the ViTMAE is the training percentage (t%), i.e. the percentage of pixels masked during training (currently a fixed value).",1,related,0,negative
"When no context frame is used, we essentially utilize only patch-level representations to perform reconstruction with the temporal transformer (simulating a perframe MAE followed by a temporal transformer).",1,related,1,positive
"The pretraining objective is inspired by masked autoencoding (MAE) for unlabeled video frames, where the aim is to reconstruct a subset of ‚Äúmasked‚Äù image patches given the ‚Äúunmasked‚Äù image patches as context.",1,related,1,positive
"Masked Autoencoders (MAE) [27], on the other hand, simply regress to the pixel values of these tokens.",1,related,1,positive
"Our MAE baselines are pretrained with the same hyper parameters (e.g. optimization and mask ratio) as IV-CL, which we have observed to be optimal based on the validation set performance.",1,related,1,positive
Our reimplementation of image and video MAEs achieve very similar performances on their original benchmarks.,1,related,0,negative
"However, for video-based MAE, we observe that the ‚Äúun-factorized‚Äù backbone leads to training collapse on CATER.",1,related,0,negative
"As most of the prior work require explicit object detection and are not end-to-end trained, we reimplement an image-based MAE [27] and a video-based MAE [53] baseline and analyze the impact of inductive biases (using slot tokens or not) as well as pretraining objectives (predictive coding given compressed context, or autoencoding the original inputs) on the reasoning performance.",1,related,1,positive
"We use the same number of layers, hidden size, and other hyperparameters as recommended by [27].",1,related,1,positive
We confirm empirically that the proposed method outperforms MAE and its video extension by large margins.,1,related,0,negative
Image Decoder for Pre-training: We use the same image decoder as in [27].,1,related,1,positive
"Masked Autoencoder (MAE) [He et al., 2021] is a generative-based SSL method with the training objective of reconstructing masked images.",1,related,1,positive
"Method Accuracy
Supervised 82.5% MoCo v3 84.1% MAE 84.9% MAGE 84.3% GD (Linear, pool 2√ó2) 73.17% GD (Linear, pool 4√ó4) 73.50%
Table 4: Stable Diffusion linear probe results.",1,related,1,positive
We adopt MAE [13] framework to pre-train the ViT backbone in MAERec.,1,related,0,negative
We choose Vision Transformer (ViT) [8] as the default backbone for its effortless applicability in masked image modeling [13].,1,related,1,positive
We adopt the framework of MAE [13] with minor modifications.,1,related,1,positive
The ViT encoder is ViT-B/16 and we load pretrained weights from MultiMAE [3].,1,related,1,positive
"We adopt ImageNet-V+ to evaluate 40 different models pre-trained on ImageNet, including models with different structures (the CNN-based VGG [46], ResNet [21], Inception [48, 47], DenseNet [26], EfficientNet [49], MobileNet-v2 [43], the transformer-based: ViT [14], DeiT [51], Swin Transformer [33], and the MLP Mixer [50]), different training paradigms (adversarial training [42] and mask-autoencoder [20]), different augmentation methods (AugMix [23], DeepAugment [22]).",1,related,1,positive
"On the other hand, inspired by the masked learning trend (He et al. 2021), we employ the masked multihead attention mechanism to obtain object features of the tth iteration Hi,j(t), which are highly responded to class probabilities:",1,related,1,positive
"On the other hand, inspired by the masked learning trend (He et al. 2021), we employ the masked multihead attention mechanism to obtain object features of the tth iteration Hci,j(t), which are highly responded to class probabilities:
Hci,j(t) = Attention(WqryM(Fi,j(t)) ,W‚ä§keyM(Ei,j(t))‚ä§)‚Ä¶",1,related,1,positive
"R O
] 1
4 Ju
l 2 02
3
In this paper, we propose a tactile representation method based on the Masked Autoencoder [8], named TacMAE, to simulate the contact area‚Äôs absence of incomplete tactile data caused by partial contact.",1,related,1,positive
"Specifically, our TacMAE receives two components as the input: latent features from the encoder and trainable vectors that illustrate the existence of the missing patches for reconstruction [8].",1,related,1,positive
"based on the Masked Autoencoder [8], named TacMAE, to simulate the contact area‚Äôs absence of incomplete tactile data caused by partial contact.",1,related,0,negative
"In order to train the MAE framework, we use the official PyTorch implementation1 provided by the authors [13].",1,related,1,positive
"Following common practice in the literature [28, 58], we pre-train image backbones unsupervised on ImageNet-1k.",1,related,1,positive
"Comparison with sota pretraining methods: To further demonstrate the superiority of our customized ‚Äùpre-training‚Äù approach for few-shot medical image segmentation, we present a comparison between our method and three stateof-the-art pre-training methods: Swin-SimMIM (Xie et al. 2022), ViT-MAE (He et al. 2022), and Res50-SimCLR (Chen et al. 2020b) in Table 3 and Table 4.",1,related,0,negative
"6, we visualize the learned features of DenseMP, Swin-SimMIM, ViT-MAE, and Res50-SimCLR.",1,related,1,positive
"‚Ä¶of our customized ‚Äùpre-training‚Äù approach for few-shot medical image segmentation, we present a comparison between our method and three stateof-the-art pre-training methods: Swin-SimMIM (Xie et al. 2022), ViT-MAE (He et al. 2022), and Res50-SimCLR (Chen et al. 2020b) in Table 3 and Table 4.",1,related,1,positive
"We further expanded the experiment by utilizing visual foundation models such as SimCLR-ResNet-50, BeiT-ViT-B16, MAE-ViT-B16, and MoCo-ViT-B16, as well as multimodal models CLIP-ResNet-50, and CLIP-ViT-B16.",1,related,1,positive
"We follow [34] to choose the most representative 2D and 3D pretraining baselines: AttrMask [28], ContextPred [28], InfoGraph [54], MolCLR [63], GraphCL [70], as well as recently published method Mole-BERT [65] and GraphMAE [26] as 2D baselines.",1,related,1,positive
"Unlike the previous methods [10, 22], we do not utilize reconstruction loss [23], but use this only as a data augmentation method.",1,related,1,positive
It then appends a learnable embedding for the masked audio tokens to g and passes it through a shared audio-visual transformer decoder [28].,1,related,1,positive
"Given an egocentric video clip with binaural audio, we mask segments of it and train a model based on a new form of masked autoencoding (MAE) [16, 28, 16, 72, 34, 6] to predict the missing segments on the basis of the video and the unmasked segments in the audio.",1,related,1,positive
"To solve our pretext task of binaural audio inpainting in egocentric videos, we propose an approach based on the masked autoencoding framework [28], which has been shown to learn meaningful semantic features from audio-visual data [26, 33, 24].",1,related,1,positive
"Therefore, we expect that our method can be combined with Masked Autoencoders [16] or other scalable self-supervised method.",1,related,1,positive
"We evaluate a representative sample of vision foundation models based on their pre-training algorithms: (1) a masked autoencoder (MAE) (He et al., 2022), a canonical masking-based method that fills image portions during pre-training, (2) SimCLR (Chen et al.",1,related,1,positive
"However, when we reduce the dataset size significantly, such as in the case of the masked autoencoder [22] trained on 400 times less data from ImageNet [11], its performance significantly declines.",1,related,1,positive
"Opting for MAE over alternatives like CLIP is informed by two primary considerations: 1) Our scenario lacks textual annotations, and 2) The MAE model is widely used with its robust image inpainting abilities.",1,related,1,positive
"Despite the input image being part of a generated composite action, we encourage the model to reconstruct the original sub-actions, denoted as vÃÇimae, vÃÇ j mae = fmae(vÃÇ
i, vÃÇj), where fmae denotes the MAE model.",1,related,1,positive
"3D to 2D rendering: In this study, we utilize the pretrained MAE [10] as our self-supervised model.",1,related,1,positive
"Finally, we propose Decoupling Refinement, which leverages a self-supervised pre-trained model MAE to ensure semantic consistency between the sub-actions and compositional actions.",1,related,1,positive
"We then compute the value for each pixel in the 2D attention map A by aggregating these decayed attention scores from all N joint points:
Ai(pix) = N‚àë n=1 Ein ||pix‚àí pixin||2 , (6)
where pix denotes each pixel of the rendering image v. Lastly, we divide the image into segments of 16√ó16 regions, following the MAE model‚Äôs configuration.",1,related,1,positive
"Regarding the MAE model, we utilize a larger vision that also incorporates a GAN loss to enhance the in-painting capabilities.",1,related,0,negative
We then employ a self-supervised pre-trained model Mask AutoEncoder (MAE) [10] to reconstruct the complete image from sub-segments.,1,related,1,positive
"We see these methods as they are learning trajectory representations and the policy jointly, with training objectives inspired by MAE [22] and GPT [37].",1,related,1,positive
"To accelerate convergence, we initialize our backbone with MAE-pretrained weights [12].",1,related,1,positive
"To evaluate its effectiveness, we replace the ViT model pretrained by DINO with MocoV3 [107] and MAE [106] pretraining model.",1,related,0,negative
The structure of the FCMAE applied to a time series is displayed in figure 3.,1,related,1,positive
"Definitions of MAE andRMSE are as follows, where RULi denotes the actual value of RUL at time i, and ‚àß
RULi denotes the predicted value of RUL at time i:
MAE= 1 n n‚àë i=1 ‚à£‚à£‚à£‚à£RULi‚àí ‚àßRULi‚à£‚à£‚à£‚à£ (4)
RMSE= ‚àö‚àö‚àö‚àö1 n n‚àë i=1 ( RULi‚àí ‚àß RULi )2 .",1,related,1,positive
"We assess the performance of the previous state-of-the-art approach (UNet [19]) and fine-tune the latest pre-training methods based on vit (MAE [31], BEiT [32]) on our AxonCallosumEM dataset.",1,related,0,negative
"The input size for the pre-trained ViTs from MAE [31] and BEiT [32] was 224x224, while for EM-SAM [9], it was 1024x1024.",1,related,0,negative
"We compared the publicly available pre-trained ViTs, including MAE [31] and BEiT [32].",1,related,0,negative
"We are using a ViT pre-trained on ImageNet-21k using the generative, self-supervised learning method of Masked Autoencoders(MAE) [14] that has exhibited major amounts of effectiveness in generalization.",1,related,1,positive
"MAE [18] is leveraged as our self-supervision module, specifically for performing the augmentive modeling.",1,related,1,positive
"For the augmentive modeling, we leverage MAE [18] method into our SelfFed framework.",1,related,1,positive
"In this work, we propose the use of Swin transformer along with MAE [18] to perform augmentive modeling as self-",1,related,1,positive
"We hypothesize that while being more powerful than ResNet in general, U-Net tends to perform better when trained on high-resolution data [67], and ViT often suffers from overfitting when trained from scratch [26, 50].",1,related,0,negative
"We consider pre-trained classifiers with architectures such as Vision Transformers (ViT) [17], ResNet-18/50 [21] trained with different pretraining methods including supervised training [21], adversarial training [38], SimCLR [8], MoCo [20], SwAV [7], CLIP [35], and MAE [19].",1,related,1,positive
The image encoder Ev is an MAE [53] pre-trained ViT backbone [35].,1,related,1,positive
The image encoder E v is an MAE [53] pre-trained ViT backbone [35].,1,related,1,positive
"We extract similarity matrices for the visual features, obtained through a pretrained MAE [18] or our fine-tuned one, and for the text features, produced using a pretrained BERT [6] or fine-tuned one.",1,related,1,positive
"To provide the attention maps for IDM, we suggest the Contrastive Attentional Masked Auto-encoder (CAMAE) framework.",1,related,1,positive
We found that the PLB tends to capture noisy attention maps with large reconstruct loss in the MAE framework without CL.,1,related,1,positive
"For this process, we use the same regularization strategies as [4] and set weight decay as zero.",1,related,1,positive
"For a fair comparison, our pre-training and linear classification experiments are conducted on the ImageNet100/1K dataset, following the same protocol as [4].",1,related,0,negative
"However, we find the PLB is not helpful for MAE framework.",1,related,0,negative
"Inspired by MAE [46], we then introduce masked filter modeling to construct PCA-like knowledge by aligning the outputs between the intermediate features of the pre-trained teacher and the decoder added to the student, which guides the filter sampling based on the Straight-Through Gradient Estimator.",1,related,1,positive
"In addition, different from MAE [46] through reconstructing the masked image patches to learn feature representation, our PCA-like knowledge is extracted by reconstructing the features of the student to approach those of the pre-trained teacher.",1,related,1,positive
MAE is a new type of autoencoder model [23].,1,related,0,negative
"In series association [6], we mask [8] the points themselves to ensure that the reconstruction does not rely on their own information.",1,related,0,negative
"In Figure 12 (b), we report the results of stitching different pretrained weights based on the base and large variants of ViTs, including MAE [20], SAM [26], AugReg [41] and BEiTv2 [34].",1,related,1,positive
ImageNet-1K finetuned MAE-B4 and MAE-L5.,1,related,1,positive
"Different from SkexGen and previous work on masked learning (He et al., 2022), we apply masking on a skip-connection from the encoder input to the decoder input.",1,related,1,positive
"SAM employs a pre-trained Masked AutoEncoder (MAE) [60] based on Vision Transformer (ViT) [80] to process images into intermediate features, and encodes the prior prompts as embedding tokens.",1,related,1,positive
"SAM utilizes the MAE pre-trained ViT as its image encoder, which is available in three versions: base, large, and huge.",1,related,0,negative
1: The overall framework of our masked compression model (MCM) which unifies pre-trained MAE [23]-based MIM and LIC for extremely low-bitrate image compression.,1,related,1,positive
"After that, receiving the visible tokens by the tokenization on the visible patches pv, similar to MAE [23], we also apply a pre-trained standard ViT [32] as the encoder E to learn latent representations, resulting in encoded tokens x of pv:",1,related,1,positive
"1, our MCM unifies pre-trained MAE [23]-based MIM and LIC for extremely lowbitrate image compression.",1,related,1,positive
The representative method of MIM is masked autoencoder (MAE) [23] which applies a standard ViT [32] as the,1,related,1,positive
We use two masked image modeling objectives: Masked Autoencoders [24] and CroCo [49].,1,related,1,positive
"To understand the potential of MIMIC for the high-level classification tasks, we evaluate MAE [24] and CroCo [49] pretrained with MIMIC-3M on ImageNet-1K [18].",1,related,0,negative
"In this experiment, the features we use are: 1) the RGB values of image itself, the most basic feature of pixels, 2) the CNN features obtained from supervised CNN (ResNet (He et al. 2016)), and self-supervised CNN (MoCov3 (Chen, Xie, and He 2021)), 3) transformer features obtained from supervised transformer (ViT (Dosovitskiy et al. 2020)), and self-supervised transformers (DINO (Caron et al. 2021) and MAE (He et al. 2022) which is known to outperform DINO in down-stream tasks).",1,related,1,positive
One notable point is that we need to set œÑ to high value when we use the features of ViT and MAE.,1,related,1,positive
"We train RetinaNet (Lin et al. 2017b) detectors with ResNet as backbones, and explore vision transformer detection and segmentation quantization using ViT and Swin Transformer (Liu et al. 2021a) pretrained with the newly proposed selfsupervised method MAE (He et al. 2022).",1,related,1,positive
"We tried ViT (Dosovitskiy et al. 2021) and Swin Transformer (Liu et al. 2021a) pretrained on ImageNet1k and ImageNet21k, respectively, using the self-supervised learning methods MAE (He et al. 2022).",1,related,1,positive
2021a) pretrained with the newly proposed selfsupervised method MAE (He et al. 2022).,1,related,0,negative
"To convey the fundamental knowledge of the SSL‚Äôs efficacy to clients, we select SimCLR with ResNet50 (Chen et al. 2020) and masked autoencoders (MAE) with Vision Transformer base model (ViT-B) (He et al. 2022) as the representative contrastive and generative methods of SSL, respectively.",1,related,1,positive
"Inspired by masked autoencoders [19], we create a reconstruction task that cannot be easily addressed by the model with a high masking ratio.",1,related,1,positive
"Unlike masked autoencoders [19], the mask in our method is not completely empty but contains Gaussian noise.",1,related,1,positive
"Standard SSL protocols is to either learn a linear classifier on frozen features (Chen et al., 2020a; He et al., 2020) or to finetune on downstream tasks (He et al., 2021; Chen et al., 2022).",1,related,1,positive
"We conduct self-supervised pre-training on the ImageNet-1K (Deng et al., 2009) training set, as commonly used in SSL methods for both MIM (He et al., 2021) and contrastive learning (Chen et al., 2020a).",1,related,1,positive
"Differences. iBOT learns the local information by adding MIM objective, which is now widely developed in MAE (He et al., 2021), SimMIM (Xie et al., 2021d).",1,related,1,positive
"iBOT learns the local information by adding MIM objective, which is now widely developed in MAE (He et al., 2021), SimMIM (Xie et al.",1,related,0,negative
The image encoder employs an MAE [25] pre-trained ViT network [6] to extract image features.,1,related,1,positive
The image encoder uses the Vision Transformer (ViT) (10) as its backbone and is pre-trained using the masked strategy from the masked autoencoder (MAE) (11).,1,related,1,positive
"For masked image modeling, we follow the setting of SimMIM (Xie et al., 2022b) and MAE (He et al., 2022) using their official code repositories12 where the masking ratio is 0.6 and 0.75, respectively.",1,related,1,positive
"We simply adopt l1 regularization to minimize the distance between predicted patches and the targets, followed by earlier reconstruction-based works (Xie et al., 2022b; He et al., 2022), and after completing a sequential training, the obtained encoder hŒ∏ can be utilized for many different downstream tasks.",1,related,1,positive
"Inspired by our above observations, we propose a new UCL framework based on Masked Image Modeling (MIM) (Xie et al., 2022b; He et al., 2022) that improves task-generic representation across all layers during training.",1,related,1,positive
"We follow Siamese network structure by Madaan et al. (2022) and
implement a MIM-based continual self-supervised learning framework under SimMIM (Xie et al., 2022b) and MAE (He et al., 2022) for UCL.",1,related,1,positive
"‚Ä¶a new UCL framework for a better generalizable representation model across all layers, we survey Masked Image modeling (MIM) (Pathak et al., 2016; He et al., 2022) that self-trains input representation by minimizing regression loss to predict RGB pixel values in randomly zeroed patches in‚Ä¶",1,related,1,positive
"To build a new UCL framework for a better generalizable representation model across all layers, we survey Masked Image modeling (MIM) (Pathak et al., 2016; He et al., 2022) that self-trains input representation by minimizing regression loss to predict RGB pixel values in randomly zeroed patches in patchified input images.",1,related,1,positive
"Lastly, we describe how to create training videos with synthetic anomalies and train the masked AEs to jointly predict the anomaly maps and overlook (not reconstruct) the anomalies from training frames.",1,related,1,positive
"We elaborate the connection to seem-
ingly related masked AEs in the supplementary [10, 88].",1,related,0,negative
"With the same purpose in mind, we propose to employ masked auto-encoders [29] in anomaly detection, introducing new ways to regulate their generalization capacity.",1,related,1,positive
Our masked AE pursues the architectural principles proposed in [29].,1,related,0,negative
"Moreover, we go beyond applying standard masked AEs, proposing several modifications leading to superior performance levels: emphasizing tokens with higher motion, augmenting training videos with synthetic anomalies, and employing self-distillation.",1,related,0,negative
"[29], we replace the ViT [19] blocks with CvT blocks [83], aiming for higher efficiency.",1,related,1,positive
"Transfer Learning on Downstream Tasks To further evaluate the transferability of our proposed PatchMix, we conduct transfer learning experiments on downstream tasks: object detection and instance segmentation on COCO [23] by Mask RCNN [17] with FPN [22] as MAE [15].",1,related,1,positive
"To further evaluate the transferability of our proposed PatchMix, we conduct transfer learning experiments on downstream tasks: object detection and instance segmentation on COCO [23] by Mask RCNN [17] with FPN [22] as MAE [15].",1,related,1,positive
"We implement AugMask using MAE style token drop [13], allowing us to inherit the computational cost reduction by skipping network computation for the masked region.",1,related,1,positive
"We utilize three finetuning recipes: MAE [13], BEiT v2 [29], and Finetune CLIP [30].",1,related,1,positive
"We construct AugSub utilizing three in-network drop-based techniques: dropout [1], drop-path [6, 7], and input masking [13, 17].",1,related,1,positive
"We select three drop-based techniques for AugSub: dropout [1], drop-path [6], and random masking [13].",1,related,1,positive
"We compare RemoteCLIP with a variety of baselines, including the vanilla CLIP model (ViT-Base32 and ResNet-50), Self-supervised Learning (SSL-based) foundation visual models (SwAV, Barlow Twins, VICReg),
ImageNet pretrained models (ViT-Base-32 and ResNet-50), and existing remote sensing foundation models (ViTAE and SatMAE).",1,related,1,positive
"We use the pre-trained models from [7] which were trained via MAE [3] on a collection of 4.5M images from Ego4D [29], Epic [30], Something-Something [31], 100 Days of Hands [32], and ImageNet [33].",1,related,1,positive
We use the pre-trained models from [7] which were trained via MAE [3] on a collection of 4.,1,related,0,negative
"Consistent with prior work [3], we find that fine-tuning is more effective than linear probe evaluation.",1,related,1,positive
"We instantiate this idea through a masked reconstruction task, similar to the BERT [5] and MAE [3] counterparts in language and computer vision.",1,related,1,positive
"In the case that x represents generated images, a valid choice is a deep kernel built upon a pre-trained NN-based image encoder h (e.g., a ViT trained by the objective of MAE [47] or CLIP [48]).",1,related,1,positive
We feed the obtained masked image into the image encoder and employ the same decoder structure as MAE to reconstruct the full image area based on the unmasked area.,1,related,1,positive
Ours w/o HAM; the HIAR module is then rather similar to a typical MAE module.,1,related,1,positive
"We propose a hard instrument area reinforcement module intertwined with the popular image reconstruction approach, masked autoencoder (MAE) [14].",1,related,1,positive
We seek to reinforce its performance on the hard-predicted area in an image by utilizing a MAE-like structure [14] to reconstruct the image especially on hard-predicted area for representation enhancement.,1,related,1,positive
"Our decoder has the same architecture with MAE [17], except that we add the adaptive layer norm blocks for conditioning on the time and class embeddings [33].",1,related,1,positive
"Similar to MAE [17], we apply an asymmetric encoder-decoder architecture: 1) the encoder has the same architecture as the original DiT except without the final linear projection layer, and it only operates on the unmasked patches; 2) the decoder is another DiT architecture adapted from the lightweight MAE decoder, and it takes the full tokens as the input.",1,related,1,positive
"12 True _ False GroundedSAM [29, 24] GroundingDINO[29],MAE[14]+ViT[11] 834.",1,related,1,positive
Masked autoencoders (MAE) [86] provide a selfsupervised pre-trained backbone for developing fully trained models with a small labeled dataset.,1,related,1,positive
"Among various types of SSL methods, we identify reconstruction-base learning with masked autoencoders (MAE) [He et al., 2022] as one of the most suitable SSL approaches for training DP foundation vision models.",1,related,1,positive
"For (Syn)-ViP pre-training, we follow the training setup outlined in [He et al., 2022]: we apply the training parameters specified in Table 8 of He et al. [2022] and pre-train pre-train (Syn)-ViP on the S21k dataset developed in Baradad et al. [2022], which comprises of 1,300,000 training samples,‚Ä¶",1,related,0,negative
"To partially demonstrate it, we use the MAE pretrained model [99] as the teacher for local spatial feature representation.",1,related,1,positive
"We also predict the visual modality by reconstructing the image crops for the masked tokens in MMLM, in a way similar to MAE (He et al., 2022).",1,related,1,positive
"2 Masked Crop Modeling (MCM) We also predict the visual modality by reconstructing the image crops for the masked tokens in MMLM, in a way similar to MAE (He et al., 2022).",1,related,1,positive
"Our work aims to systematically evaluate such representations for perceptual similarity, also including OpenCLIP (an open-source implementation of CLIP) [38] and pre-trained masked autoencoders (MAE) [30].",1,related,1,positive
"Following standard practice, distance D ( x, Àú x ; f Œ∏ ) = 1 ‚àí cos f Œ∏ ( x ) , f Œ∏ (Àú x ) is taken as the cosine distance between the CLS tokens taken from the last layer for DINO and MAE (before and after the layer normalization, respectively), and the embedding vector for CLIP and OpenCLIP.",1,related,1,positive
"We present an algorithm for mining a dictionary of Rosetta Neurons across several popular vision models: Class Supervised-ResNet50, DINO-ResNet50, DINO-ViT, MAE, CLIP-ResNet50, BigGAN, StyleGAN-2, StyleGAN-XL.",1,related,1,positive
"We search for Rosetta Neurons across eight different models: Class Supervised-ResNet50 [13], DINOResNet50, DINO-ViT [4], MAE [12], CLIP-ResNet50 [24], BigGAN [3], StyleGAN-2 [15], StyleGAN-XL [29].",1,related,1,positive
"In addition, leveraging the mask point transformer architecture, we explore two distinct baseline methodologies for PIC, which encompass separating inputs and targets akin to the Painter strategy [33] and concatenating inputs and targets in a manner analogous to the MAE approach [11] for reconstruction.",1,related,1,positive
"We apply the loss function to all patches, rather than the visible ones only [24], to optimise for reconstruction.",1,related,1,positive
"Following the principles of reconstruction-based detection, we directly use the well-designed training pipeline of MAE [24] and serve the reconstruction error to assign an anomaly score.",1,related,1,positive
"We argue that such dynamically masking at time intervals can break the original dependency of the discriminator on some local features that are important to distinguish historical samples, inspired by [65, 22, 14, 43].",1,related,1,positive
Masked Auto-Encoder (MAE) [18] is another unsupervised training regime that uses asymmetric auto encoders to train Visual Transformers on unlabeled data collections.,1,related,0,negative
"We build upon masked autoencoders (MAEs) (He et al., 2022) and train vision transformers (ViTs) (Dosovitskiy et al.",1,related,1,positive
"We build upon masked autoencoders (MAEs) (He et al., 2022) and train vision transformers (ViTs) (Dosovitskiy et al., 2020) to reconstruct masked LiDAR data from fused LiDAR and camera features.",1,related,1,positive
"Our proposed fusion method builds upon masked autoencoding (He et al., 2022), which is a recent form of denoising autoencoding.",1,related,1,positive
"To encode a richer geometry in our interpolation, we embed the datasets using a masked auto encoder (MAE) [He et al., 2022] and learn the OTDD map in this (‚àº200K dimensional) latent space.",1,related,1,positive
"datasets using a masked auto encoder (MAE) [He et al., 2022] and learn the OTDD map in this (‚àº200K dimensional) latent space.",1,related,1,positive
We use ResNet-18 as the model architecture and pre-train the model on decoded MAE images (interpolated dataset) or original images (single dataset).,1,related,1,positive
"Inspired by Mae [16] and BERT [9], we mask random patches of the input image and labels at the training stage.",1,related,1,positive
"47.8 - - - 42.6 IN-1k DINO 48.9 32.9 52.2 62.4 43.7 IN-1k MAE 51.2 34.9 54.7 66.0 45.5 IN-1k FLSL 53.1 36.9 56.2 67.4 47.0
Table 2: VITDET-B/16 WITH MASK R-CNN ON COCO
Pretrain Backbone APVOC IN-1k DINO ViT-S/16 48.9 IN-1k DINO ViT-B/16 49.1 IN-1k DINO ViT-S/8 51.1 IN-1k FLSL ViT-S/16 53.1 IN-1k FLSL ViT-B/16 53.5 IN-1k FLSL ViT-S/8 55.2
Table 3: FASTER R-CNN FPN ON UAVDT
Protocol for hyperparameter tuning Standard instance-level SSL evaluation protocols typically utilize one of the two approaches: employing a k-NN classifier or training a linear classifier on fixed features.",1,related,1,positive
FLSL-pretrained ViT on ImageNet-1k (IN1k) demonstrates superior performance compared to the state-of-the-art ADCLR-IN1k [61] and MAE [33] pretrained counterparts.,1,related,0,negative
"FLSL also outperforms the SOTA generative approach, MAE, by +1.7% and +1.4% in the two tasks, respectively.",1,related,1,positive
"Baselines We compare FLSL with various existing SSL approaches that are based on the ResNet [27] and ViT [19] architectures: (a) self-supervised ResNet: MoCo-v2 [12], DetCo [55], DenseCL [51], BYOL [23], and SCRL [43]; and (b) self-supervised ViT: MoCo-v3 [13], MoBY [57], DINO [8], MAE [24], SelfPatch [60], and ADCLR [61].",1,related,1,positive
"FLSL pseudo-code, complete training details, and settings of augmentation pipeline are provided in Appendix D. Baselines We compare FLSL with various existing SSL approaches that are based on the ResNet [27] and ViT [19] architectures: (a) self-supervised ResNet: MoCo-v2 [12], DetCo [55], DenseCL [51], BYOL [23], and SCRL [43]; and (b) self-supervised ViT: MoCo-v3 [13], MoBY [57], DINO [8], MAE [24], SelfPatch [60], and ADCLR [61].",1,related,1,positive
We show that the naive adaptation of MAE scheme to cost volume does not work due to the redundant nature of cost volumes and the incurred pretraining-finetuning discrepancy.,1,related,0,negative
"We are inspired by the recent success of masked autoencoding, such as BERT [21] in NLP and MAE [22] in computer vision.",1,related,1,positive
"To address these challenges, we propose to use masked autoencoders (MAE) (He et al., 2021) as a pre-training strategy",1,related,0,negative
"As shown in Figure 1, our method is an extension of MAE (He et al., 2021) to 3D electron microscopy image data.",1,related,1,positive
"The best masking ratio we observed for 3D MAE (He et al., 2021) on SEM images can reach 90%.",1,related,0,negative
"Unlike the 2D MAE (He et al., 2021) design, due to the different spatial resolutions during imaging, we do not use downsampling in the z-direction, which ensures the 3D resolution of the voxel is close to a cube.",1,related,1,positive
"Deviating from prior practices [7, 32], we develop RAE and R-MAE by pre-training on COCO train2017 [45].",1,related,0,negative
"We begin with MAE [32] as a representative baseline, and explore the use of pre-computed regions [25] in an MAE-style.",1,related,1,positive
"Different from prior practices [7, 32], we develop R-MAE by pre-training on COCO train2017 [45], for its scenecentric images and ground-truth regions as potential oracles.",1,related,0,negative
"Using the default fine-tuning recipe from ViTDet [43] and MAE [32], our RAE shows significant improvement (47.",1,related,0,negative
Masked autoencoder (MAE): A state-of-the-art self-supervised ViT-based model for ImageNet[21].,1,related,1,positive
"According to the setup of the proposed models, we also evaluate SSAST and MAE-AST with a temproal resolution of 40 ms per frame, by applying average pooling to the frame-level representations.",1,related,1,positive
"Supervised Methods PANN [42] 81M 43.9 27.8 - - - PSLA [47] 14M 44.4 31.9 - - 55.4 AST [25] 86M 45.9 34.7 98.1 - - HTS-AT [48] 31M 47.1 - 98.0 - - PassT [49] 86M 47.1 - - - 65.3 KD-AST [33] 86M 47.1 - - - 62.9
Self-supervised Methods SSAST-PATCH [6] 89M AS+LS - 31.0 98.0 64.2 - SSAST-FRAME [25] 89M AS+LS - 29.2 98.1 80.8 - Conformer [8] 88M 67K hours * 41.5 27.6 - - - MAE-AST-PATCH [7] 86M AS+LS - 30.6 97.9 - - MAE-AST-FRAME [7] 86M AS+LS - 23.0 98.0 63.3 - ASiT [30] 85M AS - 35.2 98.8 63.1 - data2vec [24] 94M AS - 34.5 - - - MaskSpec [26] 86M AS 47.1 34.7 97.6 - - MSM-MAE [14] ‚Ä† 86M AS - 36.7 98.4 95.3 - Audio-MAE (local) [9] 86M AS 47.3 37.0 98.3 94.8 - BEATsiter3 [10] 90M AS 48.0 38.3 98.3 - - BEATsiter3+ [10] ** 90M AS 48.6 38.9 98.1 - - M2D [15] 86M AS - 37.4 98.5 94.4 -",1,related,1,positive
"Data augmentation is critical for adjusting the matching difficulty; ii) data2vec constructs the teaching representation by taking the average of the last eight transformer blocks, while our ATST-Frame uses the asymmetric structure of the BYOL [16], where an extra predictor network is set for the student branch. iii) M2D organizes spectrograms patch-wisely and uses a MAE structure
3 in the student branch, while ATST-Frame adopts a frame-wise strategy and uses a standard transformer encoder architecture.",1,related,1,positive
"1) Comparison Methods: We compare with six SSL pretrained models: BYOL-A-v2 [11], SSAST [6], MAE-AST [7], Audio-MAE [9], BEATs [10] and M2D [15].",1,related,1,positive
We pretrain all models from scratch for 400 epochs using ResNet50 [20] (except ViT-B [12] for MAE [17]) as our backbone with synchronized batch normalization [18] during pretraining.,1,related,0,negative
"For MAE, we strictly follow its original pre-training settings [17].",1,related,0,negative
"Since our cropping strategy is orthogonal to SSL methods, we now validate its adaptability to the non-contrastive SSL method MAE [17].",1,related,1,positive
"ViT-Base pretrained (800ep) on COCO with MAE [17], and finetuned on COCO using Mask RCNN FPN (‚ÄòMask‚Äô) and Cascade RCNN FPN (‚ÄòCas.",1,related,1,positive
"Figure 18 shows the results across 4 encoders -
Inception, CLIP, DINOv2, and MAE.",1,related,0,negative
We find that SimCLR ranks worst on the perceptual score while the masked methods of MAE and data2vec score the highest.,1,related,1,positive
"In this work, we choose masked autoencoder (MAE) [12] and contrastive learning [3, 11] to pre-train models, as they generalize well on multiple tasks and show an impressive performance in previous research.",1,related,1,positive
"Instead of using a pre-training strategy specific to one downstream task, we apply masked autoencoder (MAE) [12] and MoCoV2 [3], a successful method for contrastive learning, to evaluate the generated data.",1,related,1,positive
Then He generalizes this method to pre-train an encoder for an image task [21].,1,related,1,positive
We leave other structures such as Masked Autoencoder [21] adopting Vision Transformer [15] for further exploration.,1,related,1,positive
"We use the powerful pipeline of MAE [53] with the following modifications: (1) Patches are not dropped, (2) The loss function used is Gumbel-Softmax",1,related,1,positive
"We use a recently proposed powerful Transformerbased framework, MAE [53] (tiny), to recover the original clean images from the images encrypted by MI.",1,related,1,positive
"We adapt MAE with two modifications: (1) Patches are not dropped and (2) The linear patch embedding is replaced by a nonlinear patch embedding, which is consistent with the patch embedding used in PEYOLOS.",1,related,1,positive
"We use the powerful pipeline of MAE [53] with the following modifications: (1) Patches are not dropped, (2) The loss function used is Gumbel-Softmax
proposed in [54], and (3) The positional encoding is removed, which is necessary as the patch orders are randomly shuffled by RS.",1,related,1,positive
"Our work is only a small step in this direction; relative to the model sizes and dataset sizes in NLP and CV, ours are tiny.",1,related,1,positive
"We call it as non-unique target issue, which substantially limits the flexibility of the MIM models.",1,related,0,negative
"Inspired by the above observations, we propose a novel asymmetric patch sampling strategy, to introduce more asymmetry for contrastive learning and alleviate the nonunique target issue suffered by the existing MIM methods at the same time.",1,related,1,positive
"For the first view, we conduct sparse patch sampling [17] to obtain highly sparse patch sequences, which only contain small portion of patches from the original image, e.",1,related,1,positive
"Second, compared to MIM methods, we replace the reconstruction objective with contrastive one, which provides more flexible targets for training.",1,related,0,negative
"They utilize a masked autoencoder (MAE) backbone [38] to encode the image into a latent space, before using an attentionbased decoder to generate segmentation masks from a learned encoding of prompts, in contrast to our work, which operates clustering directly on the encoder outputs.",1,related,1,positive
"Here we consider three pretrained models: ResNet34, MAE[9] and SimCLR[4]",1,related,1,positive
"Various feature extractor selections setting: In addition to the WideResNet28 model (used as feature extraction module in this paper), we also consider three additional pre-trained models: ResNet34, SimCLR, where we use ResNet18 as backbone and MAE, where we use ViT-tiny as backbone, as a supplement to prove the effectiveness of the online version.",1,related,1,positive
"From CIFAR-100, we observe that across all models, the representations of the Original images are most similar to the Foreground variants with similarity scores ranging from 0.24 for the MAE model, to the 0.4 for the Supervised model.",1,related,1,positive
Mixed Autoencoder (MixedAE) [100] studies the mixing augmentation for MAE.,1,related,0,negative
"Following the recent masked target prediction-based SSL model (He et al. 2022), we build an SSL model suitable for video summarization tasks with an adaptation of asymmetric encoder-decoder design.",1,related,1,positive
"Our autoencoder is built as a variant of masked autoencoders (He et al. 2022; Bao, Dong, and Wei 2021).",1,related,1,positive
"C V
] 2
J un
2 02
3
We further show that the NN scene understanding capabilities of canonically-pretrained vision transformers (such as MAE [30] and DINO [15]) vary greatly, despite similar finetuned performance.",1,related,0,negative
We follow the finetuning protocol of MAE [30] and use UperNet [76] as a decoder.,1,related,1,positive
"We also note that Hummingbird scales well with increasing the dataset size from ImageNet-1k to ImageNet-22k, which does not hold for all other methods (e.g. MAE, consistent with [51]).",1,related,1,positive
"‚Ä†indicates results are taken from [30], using UperNet [76] as the decoder.",1,related,0,negative
"For ViTDet, we use the ImageNet-1K MAE pretrained checkpoint [19] with the layerwise lr decay [34]; the pixel decoder is the simple feature pyramid inside ViTDet, whose outputs are upsampled and added together to get the high resolution feature map F .",1,related,1,positive
"We empirically evaluate the pretrained weight from ImageNet-1K [40], ImageNet-22k [42] and recent proposed MAE [21, 41] method.",1,related,1,positive
"However, we also observe that VideoMAE [41] pretraining does not perform well which only achieves 64.1% AO, comparing to the 70.4% AO from
image classification pretraining.",1,related,0,negative
"Similar to [28], we further evaluate the robustness of classification performance on the four ImageNet variants, i.",1,related,1,positive
"Our experiments follow the same architecture, settings, and pre-training recipe as MAE [28], and we find that the simple incorporation of a discriminator consistently outperforms MAE in variant models, e.",1,related,1,positive
"The original MAE pretraining recipes (He et al., 2022; Feichtenhofer et al., 2022) explicitly do not use drop path (Huang et al.",1,related,0,negative
"Following (He et al., 2022), we adjust learning rate, training epochs on each dataset.",1,related,1,positive
"ablated here, we find the defaults in (He et al., 2022; Feichtenhofer et al., 2022) to be appropriate.",1,related,1,positive
"In the main paper, we show that we can replace the spatial biases offered by specialized modules in a hierarchical vision transformer with a strong pretext task like MAE (He et al., 2022), thereby teaching these spatial biases instead.",1,related,1,positive
"Hiera consistently outperforms ViT pretrained with MAE (He et al., 2022), indicating that our Hiera-L and Hiera-H architectures backbone pretrain acc.",1,related,0,negative
"As an example, our default configuration yields np = 250, and thus the window sizes for each MW-MHA module in all decoder blocks will be [2, 5, 10, 25, 50, 125, 250, 250] for a total of 8 attention heads, which is a reasonable number of attention heads inline with previous research [23, 37].",1,related,1,positive
"Encoder: In line with previous work [23, 31, 37], we use a Vision Transformer (ViT) [17] based encoder, which only processes non-masked patches (20% in this work).",1,related,1,positive
"Additionally, to improve the efficiency of training, our model also incorporates a masked autoencoder [21].",1,related,0,negative
"We build our ZeroSeg model based upon the recent masked autoencoder (MAE) work [21], which aims to learn semantically meaningful representations through reconstructing masked-out image pixels.",1,related,1,positive
Our ZeroSeg also builds upon the success of MAE and incorporates a masked autoencoder to improve the training efficiency and semantic representation for those segments.,1,related,0,negative
"For our study, we choose SimCLR [10] from the former family and MAE [26] from the latter due to their simplicity and strong performance.",1,related,0,negative
"When fine-tuning pre-trained MAE models on ImageNet, we found synthetic images are still able to outperform real images.",1,related,0,negative
"Following the default hyperparameters in MAE [26], we train a ViT-B/16 model for each guidance scale w.",1,related,1,positive
"Following the default hyperparameters in MAE [26], we train a ViT-B/16 model for each guidance scale w. Figure 2(right) reports the linear probing results.",1,related,1,positive
"To leverage the power of large model, we adopt VideoMAE [24] as the based model of our clip-level distracted action classifier.",1,related,1,positive
"More specifically, our backbone is ViT-L/16 and we initialize the model with learned VideoMAE on Kinetics-710 [13].",1,related,1,positive
"Our model is a Vision Transformer (ViT) [10] pretrained using a self-supervised learning approach, which involves the masking-then-reconstruct procedure from Masked Autoencoder (MAE) [13].",1,related,1,positive
"As suggested by [13], we use a masking ratio of 75%, which speeds up the pretraining process since only 25% of the patches need to be processed by the MAE encoder.",1,related,0,negative
"Following MAE [6], our model is trained via the mean squared error (MSE) loss between the reconstructed image and the original image on masked patches 1 D ‚àëD i=1(xi ‚àí yi)(2) where D is the total number of reconstructed pixels in the FOV.",1,related,1,positive
"Furthermore, taking inspiration from the Masked Autoencoder (MAE) [6] learning paradigm, we incorporate the surrogate task of multi-pixel patch masking and reconstruction via a light weight ViT decoder for each sampled FOV of a given image to simultaneously learn semantically meaningful token representations of all patches in the FOV.",1,related,1,positive
"For object detection, we adapt the ViT to take the place of the vanilla FPN backbone [25] in Mask R-CNN [20] following [18] and fine-tune the model for 15 epochs.",1,related,1,positive
"Masked encoder: Similar to the original MAE work [18] our positional embedding, Pos (.",1,related,1,positive
We employ an asymmetric design as outlined in Figure 2 inspired by [18] in that it differs from classical encoder-decoder designs.,1,related,1,positive
We tabulate comparison results with MAE [22] pre-trained weights in Table 10.,1,related,0,negative
Methods Pre-train Train DF FS MAE [22] 99.,1,related,0,negative
"Inspired by successes in Masked AutoEncoders (MAE), where the pretraining model on ImageNet can efficiently adapt to the high-level representative vision benchmarks such as recognition and detection [18, 57], we argue that pretraining is still a potential solution for BID task.",1,related,1,positive
"MAE-based Features MAE-based features are extracted by an MAE model pre-trained on DFEW [14], Emo-tionet [3], FERV39k [41] datasets.",1,related,1,positive
Our framework starts with pre-training a masked autoencoder (MAE) [9] from a set of face databases.,1,related,1,positive
We adopt ViT-S [13] pre-trained on ImageNet-1K [12] with MAE method [19] as the backbone network.,1,related,1,positive
"We find that the proposed method works quite well in conjunction with trans-former feature backbones, such as MAE [17], DINO [5], and iBOT [50], so we call the proposed method asymmetric correspondence transformer, written as ACTRans-former, and train it end-to-end.",1,related,1,positive
"We find that the proposed method works quite well in conjunction with transformer feature backbones, such as MAE [17], DINO [5], and iBOT [50], so we call the proposed method asymmetric correspondence transformer, written as ACTRansformer, and train it end-to-end.",1,related,1,positive
We follow the setup in MAE [7] to evaluate linear classification performance.,1,related,1,positive
"Inspired byMasked AutoEncoder [13], we leverage a shared learnable token ùëí ‚àà Rùëë0 as embeddings for target nodes, while using an embedding layer with the parameter ùëä ‚àà Rùëëùë¶√óùëë0 to get embeddings for context nodes.",1,related,1,positive
"Inspired byMasked AutoEncoder [13], we leverage a shared learnable token e ‚àà R0 as embeddings for target nodes, while using an embedding layer with the parameter W ‚àà Rdy√ód0 to get embeddings for context nodes.",1,related,1,positive
"In this section, we conduct a series of experiments on ImageNet-1k [6] using DeiT [30], MAE [13], and LVViT [16].",1,related,1,positive
"This is because DeiT classifies solely based on the class token, while MAE classifies based on the average of all image tokens.",1,related,1,positive
"For the second type, we use the official opensource code of ATS and ToMe to implement them on various pre-trained Transformer backbones, including DeiT [16], LV-ViT [18], MAE [25], AugReg [26], and SWAG [27].",1,related,1,positive
"In contrast, our method achieves better quantitative results.. Compared to the VSD methods, our approach outperforms them in terms of MAE, FŒ≤ , IoU, and BER metrics.",1,related,1,positive
"Table 1 summarizes the quantitative comparison results between our method and other state-of-the-art methods in terms of MAE, FŒ≤ , IoU, and BER.",1,related,1,positive
"Following previous methods [4, 10, 20], we use four commonly used validation metrics to evaluate the effectiveness of the method, including: Mean Absolute Error (MAE) [4], F-measure (FŒ≤) [4, 19], Intersection over Union (IoU) [4], and Balaence Error Rate (BER) [4, 65].",1,related,1,positive
"Specifically, we improve the MAE by 27.3% compared to TVSD [4].",1,related,0,negative
"Our rationale is as follows: for the image encoder, it is pre-trained on millions of natural images using MAE [15], and the trained encoder is capable of extracting good natural image representations, which are completely suitable for video shadow detection.",1,related,1,positive
"We train a selfsupervised model using a modified version of the Masked-Auto-Encoder (MAE) [38], for details see appendix B.",1,related,1,positive
"We train a selfsupervised model using a modified version of the Masked-Auto-Encoder (MAE) [38], for details see appendix B.5.",1,related,1,positive
"We train a self-supervised model using a modified version of the Masked-Auto-Encoder (MAE) [38], a self-supervised model trained on ImageNet for object detection.",1,related,1,positive
"We define the Mean-Squared-Error between the ground truth image and reconstruction of masked patches as loss, equivalent to [38].",1,related,1,positive
"In phase 2, we use the pre-trained ViT-MAE-base [47] model as the image auto-encoder.",1,related,1,positive
"Decoder Layers Following [6, 47, 51], for both the fMRI and the image auto-encoder, we build asymmetric architectures where the decoder is much smaller than the encoder.",1,related,1,positive
"For instance, we compare the performance of FINOLA+MobileFormer with MAE+ViT in the context of ImageNet classification.",1,related,1,positive
Fine-tuning on ImageNet-1K: Table 7 compares FINOLA with MAE and MoCo-V3 on finetuning results.,1,related,1,positive
The encoders are initialized with MAE-Base[38] pretrained weights.,1,related,0,negative
"In this work, we tend to evaluate the data scaling ability under MAE [3], which is the simplest framework of masked image modeling, with only the reconstruction target changed.",1,related,1,positive
"We follow the design of MAE [3], except for the reconstruction target.",1,related,0,negative
"The batch size is always set as 4096 during pre-training, and the masking ratio is set as 75% following [3].",1,related,0,negative
"In this work, we investigate various target encoders, including MAE [3], DINO [28], CMAE [13], CLIP [29], etc.",1,related,1,positive
"In our experiments, we adopt the MAE [3] configurations, with the exception of the reconstruction target, which is produced by a target encoder.",1,related,1,positive
"initialization, we initialize ViTMatte-S and ViTMatte-B using DINO [6] and MAE [22] pretrained weights respectively.",1,related,1,positive
We observe that ViTMatte gets best results with self-supervised pretraining DINO [6] and MAE [22].,1,related,0,negative
"For the model
initialization, we initialize ViTMatte-S and ViTMatte-B using DINO [6] and MAE [22] pretrained weights respectively.",1,related,1,positive
"Therefore, to alleviate the above shortcomings of visual representations in ZSL models, we consider pre-trained Masked Autoencoders (MAE) [14] to capture more discriminative semantically rich visual features of images for classification.",1,related,1,positive
‚Ä¢ We propose a pre-trained Masked Autoencoders (MAE) [14] based multi-head self-attention model for solving the ZSL task.,1,related,1,positive
We initialize the backbone with MAE [14] pre-trained on ImageNet-1k without labels.,1,related,1,positive
"We train embedding models with three different visual SSL algorithms: DINO (Caron et al., 2021), Mugs (Zhou et al., 2022), and masked autoencoders (MAE) (He et al., 2022).",1,related,1,positive
"The DINO algorithm performs the best in our evaluations, with Mugs coming in second and MAE third.",1,related,0,negative
"As recommended (He et al., 2022), we use a large masking ratio of 75% during training, i.",1,related,0,negative
"To answer it, we conduct a rigorous comparative study for the adapter-based and fine-tuning based TransRec on two item modalities (i.e., texts and images) with two popular recommendation architectures (i.e., SASRec [21] and CPC [43]) and four powerful ME (i.e., BERT, RoBERTa [27], ViT and MAE [14]).",1,related,1,positive
"Specifically, we run experiments on eight combinations: {SASRec+BERT, CPC+BERT, SASRec+RoBERTa, CPC+RoBERTa, SASRec+ViT, CPC+ViT, SASRec+MAE, CPC+MAE}, where BERT, RoBERTa, ViT, and MAE are the most
6The Food, Cartoon, and Dance vertical channel dataset are denoted by Bili_F, Bili_C, and Bili_D, respectively 7Bili: https://www.bilibili.com/; TN: https://news.qq.com/; KS: https://www.kuaishou. com/new-reco; DY: https://www.douyin.com/.",1,related,1,positive
Our work studies a simple extension of MAEs [24] to videos.,1,related,1,positive
Our training settings follow [24] and we build on the open-source implementation of MAEs (https://github.,1,related,0,negative
"To that end, we study a simple extension of MAE [24] to video data (Fig.",1,related,1,positive
"Similar to the findings in the image [24] and video [27] domains, we find that SiamMAE does not require extensive data augmentation to achieve competitive performance.",1,related,0,negative
"To explore which contrastive paradigm is most suitable, we evaluate three famous paradigms, namely, MoCo [44], MAE [43], and CLIP [65], on their abilities to extract generalizable Choice Textual Labels",1,related,1,positive
"The results obtained by Tian et al. (2023) indicate that ViTs, which have been considered a prerequisite for MIM, are not irreplaceable and that CNNs can still compete with ViTs in generative SSL.",1,related,0,negative
"With a unique take on MIM, He et al. (2022) proposed MAE, an asymmetric autoencoder framework that directly learns to reconstruct image patches.",1,related,1,positive
We also conduct comparison experiments on CLIP [RKH+21] ViT-L/14 and MAE [HCX+22] ViT-H/14.,1,related,0,negative
We also conduct comparison experiments on CLIP [RKH21] ViT-L/14 and MAE [HCX22] ViT-H/14.,1,related,0,negative
"With tokenizers pretrained on relative smallscale dataset (i.e., ImageNet-1k [35] with 1.28M images), DeiT demonstrates better image captioning performance (65.8 CIDEr) than self-supervised models DINO (45.0) and MAE (37.3), without jointly tuning the visual tokenizer.",1,related,1,positive
"Similar to object counting benchmarks, we report Mean Absolute Error (MAE) and Root Mean Square Error (RMSE).",1,related,1,positive
"On GVTBench, we evaluate visual tokenizers with the same architecture (ViT-B [34]) but different pretraining strategies, including fully-supervised (DeiT [16]), self-supervised (DINO [19], DINOv2 [20], MAE [18]) and text-guided weakly supervised (CLIP [17]) pretraining.",1,related,1,positive
"Based on this benchmark, we comprehensively evaluated existing visual tokenizers with same architecture but different pretraining methods, including fully supervised (DeiT [16]), text-guided weakly supervised (CLIP [17]) and self-supervised (MAE [18], DINO [19], DINOv2 [20]) models (Section 2).",1,related,1,positive
"Based on the above analysis, we propose a novel SSL framework for point clouds, called PointGPT.",1,related,1,positive
"Our main contributions can be summarized as follows: (I) A novel GPT scheme, termed PointGPT, is proposed for point cloud SSL. PointGPT leverages a point cloud auto-regressive generation task while mitigating positional information leakage, outperforming other single-modal SSL methods.",1,related,1,positive
"Consequently, our PointGPT surpasses other single-modal SSL methods with comparable model sizes.",1,related,1,positive
This is in line with the previous SSL methods [37; 64; 66; 24] to allow for a direct comparison with these prior approaches.,1,related,1,positive
"(2)
Embedding: Following Point-MAE [37], a PointNet [38] network is employed to extract rich geometric information for each point patch.",1,related,1,positive
"The experimental results are presented in Table 1, our PointGPT-S surpasses other single-modal SSL methods.",1,related,1,positive
"The results are presented in Table 1, our PointGPT-S, which has similar capacities and training data to previous methods like Point-MAE, outperforms other single-modal SSL methods.",1,related,1,positive
"The video classifier based on VideoMAE [34] is trained on Kinetics-400 [35], an annotated video dataset with 400 classes, including motions, human interactions, etc.",1,related,1,positive
"2.7 Contrastive learning and attentive pooling improve performance We also conducted a comparative analysis of two self-supervised learning models, namely masked autoencoder (MAE) and adversarial contrastive learning (AdCo), with respect to their performance in extracting tile-level features.",1,related,1,positive
"For feature extraction, we tested two types of self-supervised learning, adversarial contrastive learning (AdCo) [30] and masked autoencoder (MAE) [31], and compared their performance.",1,related,1,positive
"Following [2], we only input the unmasked units to the modality fusion encoder to reduce computation costs and save memory.",1,related,1,positive
"For the Semi-ViT model [2], we also use the same MAE ViT-Base model.",1,related,1,positive
"In the ViT architecture, we have relied on Masked Autoencoders (MAE) [9] ViT-Base model.",1,related,1,positive
"Our image encoder is a MAE [22] pretrained Vision Transformer (ViT) [12], following SAM [29].",1,related,1,positive
"Masked Image Token Prediction Similar to text infilling, we build an image denoising method to model image patches inspired by previous studies (Bao et al., 2021; He et al., 2022).",1,related,1,positive
"Unlike 2D MIM methods which adopt image classification task as the benchmark to evaluate the effectiveness of their pre-training methods, we do not have a classification task for scene-level 3D point clouds.",1,related,1,positive
"Based on evidence from 2D masked modeling methods [18], we choose a high mask ratio (70%) when removing tokens.",1,related,1,positive
"Moreover, we can leverage standard architectures such as those used in masked language modeling [9] and masked image modeling [15], for which the scalability and stability have been tested.",1,related,1,positive
"We apply random masking with uniform distribution to the latent vector z, following the same protocol used in MAE [15].",1,related,1,positive
"Following MAE [12], we obtain the image representation by masking a large ratio of the image (i.",1,related,1,positive
"A. Image Reconstruction
Following MAE [12], we obtain the image representation by masking a large ratio of the image (i.e., 75",1,related,1,positive
"We insert a cls token as the feature vector for downstream classification tasks, as it will automatically aggregate the features from other tokens when fine-tuned [1, 14].",1,related,1,positive
The results also demonstrate that our model is a successful application of masked autoencoding from CV and NLP domains to the brain network.,1,related,0,negative
2) Masked Autoencoder: Masked Autoencoder (MAE) [22] is a pre-training method that has been proven to be effective on the image domain.,1,related,1,positive
"2) Rather than the transformer module used as the encoder and decoder in MAE, only local spatial convolution is unitized on the adjacent matrix in our masked graph autoencoder.",1,related,1,positive
"5) We compared the self-supervised learning methods of GMAE [19], BrainGSL-AE and BrainGSL, all of which involve two branches: pre-training through self graph reconstruction and fine-tuning on downstream graph classification.",1,related,1,positive
showed that transformers are able to confidently reconstruct from the context up to 95% of hidden data in a masked autoencoder fashion [10].,1,related,0,negative
"Density/masking is the most frequent data augmentation method adopted in mask autoencoder (MAE) type SSL research (He et al., 2021; Pang, Wang, Tay, Liu, Tian and Yuan, 2022; Yu, Tang, Rao, Huang, Zhou and Lu, 2021).",1,related,1,positive
"Since the ViT pre-trained with MAE [6] contains 12 layers attention layers, we experiment with n = 3, 6, 9, 12 and report their results in Table 5.",1,related,1,positive
"Our feature extractor, pre-trained with MAE [6], consists of 12 transformer encoder blocks with a hidden dimension of 768, and each multihead self-attention layer contains 12 heads.",1,related,1,positive
"However, 1) we exclude the self-supervised [6] pre-training on the FSC147 dataset compared with CounTR, which leads to a complex training strategy, and 2) we do not require an additional CNN to extract features of exemplars, which will cause varied feature spaces.",1,related,1,positive
"We use the ViT-Base (ViT-B) and ViTHuge (ViT-H) as our backbone, which is proposed by [70] and pre-trained as MAEs [71] on ImageNet-1K [72].",1,related,1,positive
We employ the weights trained on ImageNet [34] as initialization and used the OCT2017 dataset [29] to fine-tune the model for 300 epochs.,1,related,1,positive
"For a fair comparison, in the implementation of the transfer learning, we use the vision transformer [32] trained on the ImageNet and fine-tuned on the OCT2017 dataset as the encoder.",1,related,1,positive
"Although the original OCT data is collected in gray-scale (C = 1), we use the default RGB channels used in the vision Transformer models [32,34] for simplicity.",1,related,1,positive
The output of the MAE decoder is reshaped to form a reconstructed image.,1,related,1,positive
"The advantages of our method can be summarized below: We leverage the emerging self-supervised generative learning [26, 27], which has been demonstrated to be effective in transferring to general computer vision tasks [34,35,49], here we use it to address the annotation efficiency problem in OCT segmentation.",1,related,1,positive
"We leverage the modeling capacity with self-attention of the Transformers [31‚Äì33], and the scalability and generalization capacity of masked self-supervised generative learning [34, 35].",1,related,1,positive
"We follow the learning strategy of the masked autoencoders (MAE) [34], in which the objective is to reconstruct missing pixels after randomly masking patches of input images.",1,related,1,positive
"The global tokens, denoted by w ‚àà R1√ó(ta¬∑d¬∑ea) and w ‚àà R1√ó(tv¬∑h¬∑w¬∑ev), can be thought of as similar to the [CLS] tokens proposed in [24].",1,related,1,positive
"The parameters of the optimizer, similar to [24], are Œ≤2 = 0.",1,related,1,positive
"Notably, we restrict the capacity of the decoder, e.g., using a shallow decoder (Lu et al., 2021; He et al., 2021), to decrease the dependencies among generated tokens as well as improve the correlation between span representations and their associated knowledge.",1,related,1,positive
"We observe that other methods, e.g., DINO (Caron et al., 2021), BEiT (Bao et al., 2022), and MAE (He et al., 2022), have consistent properties (See Figure C.1).",1,related,1,positive
We note that the optimal masking ratio in [25] is 75% for natural images.,1,related,1,positive
"In Stage I, we introduce the MAE-style training paradigm [25], a recently proposed novel self-supervised learning method, to train the latent representation extractor with unlabeled samples.",1,related,1,positive
"Inspired by MIM in CV [10], we devise three feature-level data augmentation operators as shown in Figure 2: (1) Random Mask, masking each feature in the augmented group with randomly sampled positions; (2) Span Mask, masking each feature in the augmented group with continuous positions beginning from the sampled starting position; (3) Uniform Noise, adding imperceptibly small noises to each feature in the augmented group.",1,related,1,positive
"Borrowing the idea of mask image modeling (MIM) in CV [10], we devise three feature-level data augmentation operators ‚Äî randommask, spanmask, and uniform noise ‚Äî in order to perturb song representations in different manners.",1,related,1,positive
"To perform more comprehensive feature embedding, we design the GPT-Net based on anomaly-specific datasets and masked autoencoder [37] to pre-train the encoder.",1,related,1,positive
We use the pre-trained MultiMAE (RGB + Depth + SemSeg) made publicly available by Bachmann et al. [3].,1,related,1,positive
We show that fine-tuning a pre-trained MultiMAE model can significantly increase VO performance using only 5% of the training data amount of previous methods [35].,1,related,0,negative
Our results show that MultiMAE pre-training provides useful multi-modal features for VO that fine-tuned outperform the ConvNet baselines.,1,related,0,negative
"The image encoder was, with the backbone of ViT, pre-trained by the masked autoencoder (MAE [83]) technique.",1,related,1,positive
"In the self-supervised learning step, as described above, we use Masked Autoencoder (MAE) He et al. (2022) as the self-supervised learners.",1,related,1,positive
"We decouple seismic data p(x, t) and velocity maps c(x, z) and train individual masked autoencoders (MAE) He et al. (2022) for each domain (shown in Figure 1-b).",1,related,1,positive
"In the selfsupervision step, we use the same hyper-parameters and the training schedule with the original MAE paper He et al. (2022), except we change the batch size to 512 and remove the pixel normalization.",1,related,1,positive
"In particular, we train two masked autoencoders (MAE) He et al. (2022) separately, i.e. one for seismic data and one for velocity maps (see Figure 1- b).",1,related,1,positive
"In particular, we train two masked autoencoders (MAE) He et al. (2022) separately, i.",1,related,1,positive
"In particular, we train two masked autoencoders (MAE) He et al. (2022) separately, i.e. one for seismic data and one for velocity maps (see Figure 1b). Surprisingly, we observe a linear correlation between the two latent spaces. This means the self-pretrained encoder and decoder can be frozen, and we only need to learn a linear converter to connect them from the paired seismic data and velocity map. This introduces an interesting insight into FWI: the self-consistent representation within each domain is associated with simpler mapping across domains. We name this method SimFWI, as it simplifies the mapping (linear) in FWI between seismic data and velocity map via domain-independent self-supervised learning. Furthermore, SimFWI provides a better understanding of the relationship among multiple FWI datasets with different subsurface structures. We found that these datasets can share both encoders and decoders, but have different linear mappings between the latent spaces of two domains (i.e. seismic data and velocity map). Essentially, the two domains have a piece-wise linear relationship over multiple datasets. In addition, we found a correlation between the linear layer‚Äôs singular values and the complexity of the dataset. SimFWI achieves solid performance on multiple FWI datasets. It has comparable results to the InversionNet Wu & Lin (2019), a jointly trained model that uses paired data as supervision, with only half the model size (12.",1,related,1,positive
"In particular, the encoder and decoder can be learned separately in their own domains via MAE He et al. (2022), and the two corresponding latent spaces are linearly correlated. Table 1 lists notations of our method. 3.1 Domain-Independent Self-Supervised Learning We decouple seismic data p(x, t) and velocity maps c(x, z) and train individual masked autoencoders (MAE) He et al. (2022) for each domain (shown in Figure 1-b).",1,related,1,positive
"Our approach is based on the masked autoencoding framework [16], in which the network architecture includes both an encoder (f ) and a decoder (g).",1,related,1,positive
"We fill in the missing values with a shared and learnable embedding [10, 18] to indicate the absence of a user-specified motion in these pixels.",1,related,1,positive
"For image masking, unlike MAE [12], we aim to reconstruct the invisible patches features with visible image, text, and entity features to facilitate multi-modal information and knowledge fusion.",1,related,1,positive
The encoders are initialized with the MAE [18] pre-trained parameters.,1,related,0,negative
"Following previous works(He et al., 2022; Xie et al., 2022; Wei et al., 2022a; Bao et al., 2022; Huang et al., 2022), we use ImageNet-1K(Russakovsky et al.",1,related,1,positive
"Instead of using an 8-layer decoder in MAE(He et al., 2022), we use a lighter 2-layer decoder for ViT-Base and a 4-layer decoder for ViT-Large, respectively.",1,related,1,positive
"Following MAE(He et al., 2022), we employ Mask RCNN(He et al., 2017) with
FPN(Lin et al., 2017) as the detector.",1,related,1,positive
"Main settings follow the MAE(He et al., 2022).",1,related,0,negative
"MAE(He et al., 2022) with this head achieves 51.7% bbox mAP and 45.9% mask mAP. iBOT adopts Cascade Mask R-CNN(Cai & Vasconcelos, 2018), which is also unfair.",1,related,0,negative
"We adopt the same setting of MAE(He et al., 2022) to do linear probing.",1,related,1,positive
"Following previous works(He et al., 2022; Xie et al., 2022; Wei et al., 2022a; Bao et al., 2022; Huang et al., 2022), we use ImageNet-1K(Russakovsky et al., 2015) as the pretraining and fine-tuning dataset.",1,related,1,positive
"Same as the settings of MAE(He et al., 2022), we turn on relative position bias(Raffel et al., 2020) during transfer fine-tuning.",1,related,1,positive
"Img2Vec achieves 85.1% top-1 accuracy with ViT-B, which surpasses MAE(He et al., 2022) by 1.5% absolutely.",1,related,0,negative
"After obtaining the predictions and targets, we adopt the patch loss to optimize the model, which is similar to MAE(He et al., 2022).",1,related,1,positive
"Specifically, we employed a ViT pretrained on ImageNet-21k using the generative, self-supervised learning method of masked autoencoders (MAE) [39], which exhibited major amounts of effectiveness in generalization.",1,related,1,positive
"Full Fine-tuning The Masked Auto-encoders (MAE) paper [He et al., 2022] re-introduced fine-tuning as the main evaluation metrics.",1,related,1,positive
"Interestingly, the authors point out that simply pretraining a ConvNextV2 with the MAE framework is subpar.",1,related,0,negative
"In particular, extended from MAE [17], we develop a masking strategy during the training to adaptively mask out visual tokens and learn strong pixel representations by reconstructing clean signals from corrupted inputs.",1,related,1,positive
"We compare our strategic masking with block-wise masking [1], random masking [17] and uniform masking [28].",1,related,1,positive
"Specifically, we learn a score map for patch selection to choose the most informative patches as masked tokens under a determined ratio, as opposed to randomly masking in [17] or uniformly masking in [28].",1,related,1,positive
"Following MAE [30], we ablate the decoder design in Tables 1a and 1b.",1,related,1,positive
"Following the ViT-B decoder in MAE [30], we set the number of blocks as 4.",1,related,1,positive
"First of all, we compare our VSA with recently proposed MAE [30].",1,related,1,positive
We also verified the generality of FreMIM on RGB images dataset namely ISIC 2018 compared with the other seven well-6 [25] ViT-B/16 [20] MAE [26] 75.18 (-0.10) 88.95 ( +0.53 ) 78.47 (+2.14) 80.87 (+0.86) UNETR [25] ViT-B/16 [20] DINO [9] 75.22 (-0.06) 88.33 (-0.09) 75.89 (-0.44) 79.81 (-0.20) UNETR [25] ViT-B/16 [,1,related,1,positive
"In comparison with MAE on UNETR and SimMIM on Swin UN-ETR, our FreMIM greatly improves model performance with the benefit of exploiting MIM in the frequency domain for global representation learning.",1,related,1,positive
"MAE cannot be adapted to Swin Transformer back-bone due to the token-dropping operation), for other meth-ods we kept their original backbone as in their papers to achieve a fair comparison, which implicitly demonstrates our method‚Äôs superior versatility to various backbones.",1,related,0,negative
We follow MSN [14] for linear-probing and MAE [13] for full fine-tuning of standard models (see Sec.,1,related,1,positive
"From works that demonstrate robustness in the fullshot regime, we seem to arrive at the following conclusions for robustness to natural distribution shifts in the fullshot regime: (1) Amongst ImageNet pre-trained initializations, SSL ViTs are more robust than their supervised and CNN counterparts, with the more recent ones being better [13, 14].",1,related,1,positive
"We aim to address this gap in our work by evaluating some of the most recent SSL ViTs on a variety of datasets and distribution shifts, also comparing with CNNs and the supervised counterparts.",1,related,1,positive
"We assess the quality of the curve fit via mean absolute error (MAE) and coefficient of determination ( R 2 ) of the curve on these data points, as shown in table 4.",1,related,1,positive
"For full fine-tuning in the full-shot regime, we use the MAE codebase [13] and fine-tune for 20 epochs.",1,related,1,positive
"RobustViT [68] RobustViT uses an unsupervised localization method such as TokenCut [69] to dump offline segmentation maps and then optimizes a supervised ViT‚Äôs saliency maps [70] to resemble the offline ones while maintaining its classification accuracy to its improve robustness on the OOD shifts for ImageNet [7, 8, 9, 10, 11].",1,related,1,positive
"Despite being amenable to low-shot finetuning, we find that it is non-trivial to implement RobustViT for self-supervised ViTs that perform better on OOD shifts and downstream tasks [13, 14].",1,related,1,positive
This article focuses on self-supervised SER with the masked autoencoder (MAE) approach [10].,1,related,1,positive
"The parameters of the optimizer, similar to [10], are Œ≤2 = 0.",1,related,1,positive
"of high resolution, which is not available in CTs), we proposed several additional loss terms in the objective function of VoxelMorph as well as a random masking strategy to greatly improved the quality of the synthetic CT images (a similar idea has been adopted by He et al.(93) to significantly",1,related,1,positive
"CLIP outperforms ViT-B/MAE-ViT-B, thanks to a large amount of diverse multi-modal pre-trained data (i.e., learning the visual concept from language supervision).",1,related,1,positive
"We hypothesize that selfsupervised learning (MAE-ViT-B) plays a positive role compared to supervised learning (ViT-B), as it does not overfit the training classes and instead learn a holistic understanding of the input images beyond low-level image statistics aided by a well-designed loss (e.g., the masking and reconstructing strategy in MAE).",1,related,1,positive
We follow MAE [37] and use the standard ViT architecture [28].,1,related,1,positive
Critical to our sequential approach are the two components MAE [37] and NNCLR [29] on which we build upon.,1,related,1,positive
"We find that all MAE models, all MAE‚ÄìCT models and the huge models of MAE‚ÄìCTaug benefit from more regularization in this setting and therefore use the protocol from the original MAE publication [37], which uses Mixup [93], Cutmix [91], RandomErase [94], DropPath [44] (0.",1,related,1,positive
"MIM methods, like Masked Autoencoders (MAE) [37] and others [7, 87, 6] first mask out areas ar X iv :2 30 4.",1,related,1,positive
"We follow common linear probing protocols and sweep over learning rates [95, 12, 4] and insert a non-affine BatchNorm [48] layer after the fully frozen encoder [26, 37, 3].",1,related,1,positive
"The objectives with 1) MSE and 2) the CS loss are defined respectively as follows:
arg min Œ∏
1
n n‚àë i=1 ‚à•‚à•x(i) ‚àí hŒ∏ (x(i))‚à•‚à•22 and arg minŒ∏ 1n n‚àë i=1 `CS ( x(i), hŒ∏ ( x(i) )) ,
where `CS is given in Eq.",1,related,1,positive
"4.5.1 Setting in Expt3
Using ESC-50 [50] dataset, we compare 1) MSE, 2) CS, 3) N2V, and 4) dCS.",1,related,1,positive
"In this experiment, using Noisy-MNIST, we compare 1) MSE, 2) CS loss, 3) N2V loss, 4) SURE loss, and 5) dCS loss.",1,related,1,positive
"Note that N2N and N2V are self-supervised denoising methods, i.e., the minimization of these losses is equivalent to that of an MSE-based loss defined via the clean data; for the detailed mathematical arguments
1This study is an extension of the denoising method proposed in Sanada et al. [55]; see the last paragraph of Section 2.3 for the comparison with Sanada et al. [55].
of N2N, see Zhussip et al. [71].",1,related,1,positive
"Inspired by the Mask Auto-Encoder (MAE), we also design a Masked Skeleton Cloud Repainting task to pretrain the designed auto-encoder framework, aiming for learning more discriminative and informative self-supervised representations.",1,related,1,positive
"Motivated by BERT [52] and MAE [53], we study the masked skeleton cloud modeling strategy for skeleton representation learning based on the introduced auto-encoder framework.",1,related,1,positive
"75, which is consistent with the value reported in MAE [23].",1,related,0,negative
"To model the spatial relationships in weather data, we employ the Vision Transformer (ViT) architecture [22] as the backbone network and apply the widely-used pre-training scheme, Masked AutoEncoder (MAE) [23], to propose a pre-trained weather model named W-MAE for multi-variable weather forecasting.",1,related,1,positive
"We initialize image/text encoders in the same style as CLIP, except for one change: we use a sinecosine position embedding in ViT, like (Chen et al., 2021; He et al., 2022), and keep it frozen while training.",1,related,1,positive
"For pre-training the MRI encoder, we explore the selfsupervised masked auto-encoder (MAE) method [22] us-",1,related,1,positive
75 in our experiments) of input embeddings and trains models to reconstruct them [23].,1,related,1,positive
"‚Ä¢ Autoencoders: these are trained to reconstruct their inputs by learning an efficient representation of the data (He et al., 2022).",1,related,1,positive
"First, we run our evaluations for MAE (He et al., 2021), DINO (Caron et al.",1,related,0,negative
"First, we run our evaluations for MAE (He et al., 2021), DINO (Caron et al., 2021), SEERv2 (Goyal et al., 2022a), MSN (Assran et al., 2022), EsViT (Li et al., 2022a), Mugs (Zhou et al., 2022) and iBOT (Zhou et al., 2021).",1,related,1,positive
"Interestingly, our evaluation using +ms is on par with fully finetuning MAE with an Upernet decoder (53.0 versus 53.6 mIoU).",1,related,0,negative
"This property of MAEs has been further validated on video (Tong et al., 2022), audio (Xu et al., 2022), and across other modalities (Girdhar et al., 2022).",1,related,1,positive
"Specifically, considering that the saturated regions can be regarded as masking the short LDR input patches, inspired by [6], we randomly mask a high proportion of the short LDR input and expect the model to reconstruct a no-saturated HDR image from the remaining LDR patches in the first stage.",1,related,1,positive
"1) Batch Random Mask Block: In order to obtain more robust features and make the prediction of the network adapt to complex and changeable environments, we propose a batch random mask (BRM) block inspired by the Masked AutoEncoders (MAE) [50], in which the random patches of the image are masked to reconstruct these missing regions by the model.",1,related,1,positive
"Our results in Section 5.2 shows that while off-the-shelf
CLIP representations can be poor (especially for RGB-stacking see Figure 6 Right), adapting them through our proposed adapters results in similar performance as other adapted representations (such as MAE ones).",1,related,1,positive
", 2020) and masked auto-encoder (MAE) for ViT (He et al., 2022).",1,related,1,positive
", 2020) and for ViT we use masked auto-encoder (MAE) (He et al., 2022).",1,related,1,positive
"For NFNet we use CLIP pretraining (Radford et al., 2021), for ResNet we use BYOL (Grill et al., 2020) and for ViT we use masked auto-encoder (MAE) (He et al., 2022).",1,related,1,positive
"In addition to imagenet pretraining across all three architectures, we also evaluate using ALIGN (Jia et al., 2021) for NFNet, BYOL for ResNet (Grill et al., 2020) and masked auto-encoder (MAE) for ViT (He et al., 2022).",1,related,1,positive
"To demonstrate that the performance of the foundation model improves with the increase in the number of model parameters when pretrained using the same number of datasets in the remote sensing field, we pretrain models with different
KEUMGANG CHA et al.: A BILLION-SCALE FOUNDATION MODEL FOR REMOTE SENSING IMAGES 5
numbers of parameters using MAE [5] and the large-scale remote sensing imagery dataset, MillionAID [44].",1,related,1,positive
CV BYOL [1] ResNet200 2x 375 Million SimCLR v2 [2] ResNet152 3x w sk 795 Million DINO [3] ViT Base 84 Million iBOT [4] ViT Large 307 Million MAE [5] ViT Huge 632 Million ALIGN [6] EfficientNet-L2 800 Million CLIP [7] ViT Large 307 Million SEER [8] RegNety-256gf 1.,1,related,1,positive
"In the original MAE, pretraining is applied with 1600 epochs [5].",1,related,0,negative
"In this section, we discuss the details of the model architecture (vision transformer) [43], pretraining dataset (MillionAID) [44], and pretraining method (MAE) [5].",1,related,1,positive
"For our default model, we re-use weights from the publicly available MAE model.",1,related,1,positive
Stage 1: We follow settings from MAE [28].,1,related,0,negative
"Following [28], we adopt an encoder-decoder architecture during training and ask the model to reconstruct patches that are randomly masked out, at the pixel level.",1,related,1,positive
"We use masked input prediction[20, 28, 5, 75] objective for unimodal stages.",1,related,1,positive
"The difference between our approach and [20, 49] is that we follow the encoder-decoder structure in [28], where masked tokens are removed for the encoder and are reconstructed through a separate decoder.",1,related,1,positive
"‚Ä¢ RGB mask Following MIM [41, 11, 36, 14, 3], we randomly mask blocks of 2D input images and take an additional UNet-like decoder after the feature extractor to reconstruct the masked regions.",1,related,1,positive
"Following (He et al., 2022), during linear probing we include BatchNorm without the affine transformation before the final classifier.",1,related,1,positive
"For audio, we use a fixed 2D sinusoidal positional encoding as described in (Huang et al., 2022) and (He et al., 2022).",1,related,1,positive
"In many cases, we have access to large-scale unlabeled data containing such corruptions and noise, and would like to exploit them to pre-train a VT that can be effectively adapted to a wide range of downstream tasks [2, 11,20, 61].",1,related,0,negative
"Compared to MAE, MB1 outperforms MAE by 2% in both UF1 and UAR, approximately.",1,related,0,negative
"We utilize the encoder and decoder parts of Œº-BERT (without DMA and POI) to train previous SSL methods (MoCo V3 [4], BEIT [1], MAE [9]) and then continue learning the MER task on the large-scale database CASME3 [14].",1,related,1,positive
"We utilize the encoder and decoder parts of ¬µ-BERT (without DMA and POI) to train previous SSL methods (MoCo V3 [4], BEIT [1], MAE [9]) and then continue learning the MER task on the large-scale database CASME3 [14].",1,related,1,positive
", [9] presented a Masked Auto Encoder (MAE) that utilizes the BERT framework.",1,related,1,positive
"6% top-1 [34], which aligns with our observations on fine-tuning the pre-trained ADM.",1,related,0,negative
"Inspired by the work of SimpleClick [35], we employ large models for feature encoding, such as the widely used MAE-pretrained Vision Transformer (ViT) [21].",1,related,1,positive
"As our ViT backbones are pre-trained on 224 √ó 224 pixel images by MAE [21], we resize the pre-trained absolute positional embeddings to match the size of our images.",1,related,1,positive
"In the global contrast branch, we use the SGD optimizer to pre-train a ResNet-50 [19] (for MAE [16], we use ViT-base [11].) encoder Eg for 200 epochs.",1,related,1,positive
"In the global contrast branch, we use the SGD optimizer to pre-train a ResNet-50 [19] (for MAE [16], we use ViT-base [11].",1,related,1,positive
"‚Ä¢ MAE: Masked Autoencoders [15]: ViT-B16, ViTL16.",1,related,0,negative
"Motivated by scalability and powerful pretraining methods, we use an MAE [47] pre-trained Vision Transformer (ViT) [33] minimally adapted to process high resolution inputs [62].",1,related,1,positive
"Unless otherwise specified: (1) SAM uses an MAE [47] pre-trained ViT-H [33] image encoder and (2) SAM was trained on SA-1B, noting that this dataset includes only automatically generated masks from the final stage of our data engine.",1,related,0,negative
"Motivated by scalability and access to strong pre-training, we use an MAE [47] pre-trained Vision Transformer (ViT) [33] with minimal adaptations to process high resolution inputs, specifically a ViT-H/16 with 14√ó14 windowed attention and four equally-spaced global attention blocks, following [62].",1,related,1,positive
We initialize SAM from an MAE [47] pre-trained ViT-H.,1,related,0,negative
"While our model is initialized with a selfsupervised technique (MAE [47]), the vast majority of its capabilities come from large-scale supervised training.",1,related,0,negative
"For a comprehensive comparison, we also compare TDMR with MRKD which means a simple combination of model reprogramming (MR) and knowledge distillation (KD), and transfer learning method linear probing [7], [80].",1,related,1,positive
We compare finetuned ViT-B models trained with MoCo-v3 and MAE in Table 5.,1,related,0,negative
"We find that MoCo-v3 defended with PatchSearch and i-CutMix is better than MAE both in terms of Acc and FP for 1% labeled
finetuning data, but MAE quickly catches up in the 10% regime.",1,related,0,negative
"Restrictions apply.
from [44] indicate that MAE is robust against backdoor attacks.",1,related,0,negative
"Following previous pre-training approaches [14, 25], we use the default image input size of 224√ó224.",1,related,1,positive
Comparison with Hiera [52]: We show class-level performance (average precision and relative gain) of Hiera [52] (pre-trained on using MAE [24]) and ours.,1,related,1,positive
"Overall our method has a gain of +2.8 mAP compared to Video MAE [15, 49].",1,related,0,negative
"To further explicitly supervise motion feature generation, inspired by MAE [18], we design a decoder (Figure 3(b)) to reconstruct pixel motion dynamics.",1,related,1,positive
"The detailed pre-training hyper-parameters are in Table 7, which mainly follows the training settings used in the original MAE [37].",1,related,0,negative
The masked autoencoder (MAE) model [37] consists of an encoder and a decoder.,1,related,1,positive
"As proven in [10], a narrow decoder is enough for the MAE task, so we set L‚Ä≤‚Ä≤ to 1.",1,related,1,positive
"In this work, we use one such pre-training algorithm (MAE (He et al., 2021)) to explore scaling and adapting pre-trained visual representations (PVRs).",1,related,1,positive
"We now ask: how much does the relevance and diversity of the pre-training dataset and the model size matter? To study this, we fix the pre-training objective ‚Äì MAE (He et al., 2021) ‚Äì and then vary the composition of the pre-training dataset and the size of the visual backbone (ViTB with 86M parameters and ViT-L with 307M parameters).",1,related,1,positive
"Experiment Details of Training PVRs
To train the MAE models, we use the official codebase released by the authors on GitHub (He et al., 2021) and use the default hyperparameters provided by the repo to train the ViT-B and ViT-L models.",1,related,0,negative
"Specifically, in MAE adaptation we continue training the backbone network with the MAE (He et al., 2021) pre-training objective on task-specific data.",1,related,0,negative
"We train vision transformers (ViT-B and ViT-L) (Dosovitskiy et al., 2020) on these 4 datasets using Masked AutoEncoding (MAE) (He et al., 2021), and systematically analyze their performance on CORTEXBENCH.",1,related,1,positive
"To study this, we fix the pre-training objective ‚Äì MAE (He et al., 2021) ‚Äì and then vary the composition of the pre-training dataset and the size of the visual backbone (ViTB with 86M parameters and ViT-L with 307M parameters).",1,related,1,positive
"As the final step in this paper, but as a first step towards this open problem, we study adapting VC-1 with either task-specific training losses or datasets (via MAE (He et al., 2021)) to specialize VC-1 for each domain.",1,related,1,positive
"In this work, we refer to a certain attribute of an element as a field and formulate the various design tasks as a unified masked field prediction, which is inspired by the recent masked autoencoders [9,15] and multi-task models [19,36].",1,related,1,positive
"Results for the Masked Autoencoder (He et al., 2022) (Appendix B.",1,related,0,negative
"Specifically, we apply a linear layer to project the latent features Fl to patch pixels [86], and compute the mean squared error (MSE) between the reconstructed and original images on the masked pixels [29].",1,related,1,positive
"We follow the conventions in [29, 86] and mask random patches with 16√ó 16 pixels, and adopt a high masking ratio i.",1,related,1,positive
"Following MAE [27], ·∫ë is then ‚Äúunmixed‚Äù to recover the input batch before mixing by inserting a special [MASK] token with M j .",1,related,1,positive
"In this section, we start by adopting mixing in MAE [27] with a simple baseline in Sec.",1,related,1,positive
"Implementation Details: We follow most of the practices of [1, 8].",1,related,0,negative
"To this end, we closely follow the approach of MAE [8], and employ the Masked Image Modeling strategy.",1,related,1,positive
We tailor the MAE approach for the endoscopic setting with three modifications: Layer Wise Learning Rate Decay: The MAE encoder and decoder consist of several layers.,1,related,1,positive
"This unsupervised learning style normally requires numerous data and computation resources [41], [42], so we put the training of it on the resourceful cloud which can collect a lot of data from multiple edges.",1,related,1,positive
"In this section, we will first revisit the original image-version Masked Autoencoders (MAE) (He et al., 2022) and the video extension of Masked Autoencoders (Feichtenhofer et al., 2022), then detail the tailored MAE for the specific tasks.",1,related,1,positive
"To implement our belief in a regressive manner where the learned loss function can be treated as image prior, the first choice is the masked autoencoders (MAE) proposed by He et al (He et al., 2022).",1,related,1,positive
"In this section, we will first revisit the original image-version Masked Autoencoders (MAE) (He et al., 2022) and the video extension of Masked Autoencoders (Feichtenhofer et al.",1,related,1,positive
"To solve this issue, we adopt an approach inspired by MAE [22], where we only feed the non-masked embeddings to the student .",1,related,1,positive
"For instance, if BERT4ETH follows the original masking ratio (15%) and uses 85% unmasked addresses to predict 15% masked addresses, the masked addresses have a high likelihood of being present in the unmasked addresses, leading to an overly easy prediction task [12].",1,related,1,positive
", 2022f) for example, supports a video masked encoder for MAE (He et al., 2022) losses in addition to a module similar to ALBEF.",1,related,1,positive
"We train the tokenizers of the columns C, the encoder, and the decoder by following the approach of the masked autoencoder (He et al., 2022); that is, we randomly select the columns with a masking ratio and replace the corresponding embeddings with a special embedding called a mask token.",1,related,1,positive
"While the original masked autoencoder in He et al. (2022) uses its decoder only in the pretext task and removes it in the downstream task, our model uses the decoder trained on the pretext task in the downstream tasks.",1,related,1,positive
"Furthermore, we note that our model does not use the [cls] token, unlike the approach by He et al. (2022).",1,related,1,positive
"Given the limited data scale, we leverage mask modeling [32] to make good use of the video data.",1,related,1,positive
"For plain architectures, we adopt the MAE [31] pre-training strategy to provide a good initialization for QFormerp, which has been proven effective in mitigating over-fitting in plain ViTs.",1,related,1,positive
"The official MAE pre-trained weights for the backbone are utilized, and the entire model is finetuned for 100 epochs on the MS COCO dataset.",1,related,0,negative
"For large models, we use plain ViT as the reference architecture and adopt the MAE pretrained [31] weights for initialization.",1,related,1,positive
"The hyperparameter for the proposed regularization term is set to 1, and all other parameters follow the same settings as in [31].",1,related,1,positive
"We use the official MAE pre-trained model to initialize the ViT-B backbone and the default training settings in MMPose, i.e., an input image size of 256√ó192 and a learning rate of 5e-4.",1,related,0,negative
"For the plain models, we adopt the fine-tuning practice proposed in [31], which involves pre-training the model on the ImageNet-1K dataset for 1,600 epochs using the ImageNet-1K training set, followed by supervised fine-tuning.",1,related,1,positive
We overcome the above-mentioned issues by leveraging generic Masked AutoEncoder (MAE) [19].,1,related,1,positive
We use the publicly available Masked Autoencoders [19] pretrained on ImageNet to assist blind defense.,1,related,0,negative
"Masked Autoencoders (He et al., 2022) are scalable self-supervised learners.",1,related,1,positive
"We use two pretrained Masked Autoencoders (He et al., 2022) that are available from their official repository.",1,related,0,negative
"In terms of supervision, in addition to the standard supervised pre-training on ImageNet-1k, we also consider self-supervised methods MoCo-V3 [17], SwAV [11], DINO [12], MAE [32] trained on ImageNet-1k, the weakly supervised SWAG [76] trained on 3.6B uncurated instagram images and CLIP [59] trained on 400M image-language pairs [71].",1,related,1,positive
"In terms of supervision, in addition to the standard supervised pre-training on ImageNet-1k, we also consider self-supervised methods MoCo-V3 [17], SwAV [11], DINO [12], MAE [32] trained on ImageNet-1k, the weakly supervised SWAG [76] trained on 3.",1,related,1,positive
"Masked auto-encoders assign mask tokens to an encoded latent space [16, 44], and adopt them to masked sequences at the decoder level.",1,related,1,positive
"the performance of our architecture against the BERTbased motion in-painting transformer [10], the encoderdecoder-based ‚àÜ-interpolator [31], the RNN-based approach TGcomplete [15], and the masked auto-encoder (MAE) architecture [16].",1,related,1,positive
"Moreover, inspired by MIM [2, 10], we propose a simple yet effective strategy named Masked Pose and Shape Estimation, which helps the global transformer further mine the inter-frame correlation on human motion.",1,related,1,positive
"During the global decoder phase, following [10], we pad the SMPL tokens onto the masked location and send the whole sequence into the global decoder to obtain a long-term representation.",1,related,1,positive
"Discriminative SSL methods [11, 22, 27, 79] learn the embeddings by enforcing closeness and/or distantness on the pairwise distance structure among the augmented training samples.",1,related,1,positive
We conjecture that an SSL pre-trained encoder is desirable to capture the demanding diverse semantics instead of a supervised one learned from pre-defined labels.,1,related,1,positive
"Given a frozen prediction model PŒ∏(y|x), and perturbed image xÃÉ with prompt corresponding to the label y, the training objective is formulated as:
argmin œï
‚àí logPŒ∏;œï(y|xÃÉ)
While VP and AR optimize the input space visual prompt directly, we reparameterize the visual prompt to the prompt generation network hœï(¬∑) parameterized by œï = {œïd, œït} ‚àà Rd. Specifically, we build a novel autoencoderstyle network named Coordinator composed of a frozen encoder f(¬∑) which is pre-trained on ImageNet [14] by selfsupervised learning (SSL) objective and followed by an extremely light-weight learnable decoder gœïd(¬∑).",1,related,1,positive
"Therefore, for Coordinator, BlackVIP adopts an SSL encoder (i.e., Masked Auto-Encoder [26]).",1,related,1,positive
"Though the encoder can also be a supervised counterpart or lightweight learnable network, we adopt the SSL pre-trained encoder for the following three reasons: 1) It has been widely substantiated that self-supervised representation contains
the multiple discriminative features and spatial information [8, 19, 26, 31, 42, 43, 50], so it is more helpful to use SSL pre-trained encoder than label-supervised encoder for robustly performing on diverse downstream tasks.",1,related,1,positive
"Consequently, the image equipped with a prompt (prompted image) is constructed as follows:
xÃÉ = clip(x+ œµhœï(x)) hœï(x) = gœïd(zx, œït)
where zx = f(x) is the feature vector of x from the frozen SSL encoder f(¬∑), and œµ ‚àà [0, 1] is a hyperparameter that controls the intensity of visual prompt.",1,related,1,positive
6 confirms that the SSL encoder outperforms the supervised pre-trained or randomly initialized encoder (scratch).,1,related,0,negative
We exploit an SSL pre-trained encoder while we plug the randomly initialized extremely lightweight decoder.,1,related,1,positive
"Concretely, the computation of (2) can directly benefit from just dropping all the masked patches before forwarding them, in a similar manner to masked autoencoder [12].",1,related,1,positive
"(4) Inspired by the principles of self-supervised training [41, 15, 14], we leverage this methodology to train our encoder-decoder architecture.",1,related,1,positive
"Inspired by the advantage of long-range receptive fields from transformer layers, we follow MinD-Vis [6] to adopt the architecture of masked autoencoder [14] as the encoder-decoder model for fMRI signals.",1,related,1,positive
The only difference from MAE is that we finetune on iNaturalist21 rather than iNaturalist17.,1,related,0,negative
"As a base model, we consider the ViT-Base model using the Masked AutoEncoder (MAE) pretraining setup [16], which leads to state-of-the-art results for this general task.",1,related,1,positive
"For iNaturalist, we consider a Masked AutoEncoder (MAE) [16] pre-trained model and finetune it following the same procedure and hyperparameters as the original paper.",1,related,1,positive
"Though the scRNA-seq data is distinct from images, our results show that the performance gains of xTrimoGene are comparable to those of MAE, with more efficient training and better downstream task performance.",1,related,0,negative
"To further tackle the mode collapse challenge in unsupervised learning and scale up the optimization iterations, inspired by recent masked autoencoders [23, 57], we construct a masked point modeling paradigm where both point color reconstruction objective and surfel normal reconstruction objective are proposed to recover the masked color and geometric information of the point cloud respectively.",1,related,1,positive
"To this end, we turn to use MAE [19], a SOTA image encoder pre-trained in an unsupervised manner, similar to MLM.",1,related,1,positive
"To this end, we turn to use MAE [18], a SOTA image encoder pre-trained in an unsupervised manner, similar to MLM.",1,related,1,positive
"We find MAEbase-MLM clearly improves the standard MAEbase on HM with the TS model, but obtains marginal gains with the E2E model.",1,related,1,positive
"We extract the ViT features from the same ViT architecture training in different ways: (b) supervised ViT [17], (c) self-supervised DINO [43], and (d) MAE [44].",1,related,1,positive
using transformer feature representations extracted from pretrained masked autoencoder (MAE) [44].,1,related,1,positive
"Note that other popular image feature extractors [3, 9] can also be applied in our framework.",1,related,1,positive
"Given the simplicity and efficiency of Masked AutoEncoding (MAE) [33], we leverage it as the self-supervised prepretraining approach.",1,related,1,positive
We follow [33] to train MAE models on IG-3B without using any labels.,1,related,0,negative
"In particular, we show that (i) MAE not only scales with model size as shown in [33], but also with the size of the training data (Figure 2).",1,related,1,positive
We first begin with the Masked Autoencoder (MAE) [33] self-supervised learning technique to pre-pretrain vision models without using any labels.,1,related,1,positive
We compare the performance of MAE pretraining on the large scale IG-3B with the original MAE [33] models trained on IN1k for 1600 epochs.,1,related,0,negative
We follow the same hyperparameters used in [33] for pretraining on IN1k.,1,related,1,positive
"We follow SimpleClick [26] to build the interactive segmentation model, which consists of two patch embedding modules for image and click map respectively, a ViT [10] backbone initialized with MAE [16], a simple feature pyramid [21], and an MLP segmentation head.",1,related,1,positive
"As shown in Table 3, the model initialized with MIS pre-training the MAE [16] counterpart especially on the relatively hard dataset (i.e., SBD and DAVIS).",1,related,0,negative
"As shown in Table 3, the model initialized with MIS pre-training the MAE [16] counterpart especially on the relatively hard dataset (i.",1,related,0,negative
"Following the principle of MAE [27], we randomly mask out some tokens, and utilize the EAT encoder to extract high-level latent features.",1,related,1,positive
"Beyond augment-and-compare or mask-and-predict pretext tasks in MV-SSL and MIM, in this paper, we endeavor to investigate another simple yet effective paradigm for selfsupervised visual representation learning.",1,related,1,positive
"Overall, our CIM is a simple yet effective approach that can perform on par or better than existing MV-SSL and MIM methods with both ViT and ResNet models.",1,related,1,positive
"Due to the architectural difference between ViT and CNN models, we observe performance degeneration of some MIM and MV-SSL pretraining methods, such as SimMIM [68], MoCo v2 [10], and SimSiam [11].",1,related,1,positive
2) We demonstrate the advantages of our CIM in learning transferable representations for both ViT and ResNet models that can perform on par or better than the current state-of-the-art MIM and MV-SSL learners while improving model robustness and training efficiency.,1,related,1,positive
"For all ablation experiments, we choose ViT-Base (ViT-B/16) as the default backbone and follow a common setting used in existing works [2, 26]: 300-epoch self-supervised pre-training without labels and 100-epoch supervised end-to-end fine-tuning, to evaluate the quality of learned representations.",1,related,1,positive
"We omit the results in other metrics (NDCG, MAE MAPE) and on other data as their trends are similar.",1,related,1,positive
"We also adopt the mean absolute error (MAE), rooted mean squared error (RMSE) as well as mean absolute percentage error (MAPE) to evaluate the model accuracies.",1,related,1,positive
"The concatenation of unmasked patches‚Äô embedding and masked tokens are processed by the decoder to predict the unmasked patches [17, 43].",1,related,1,positive
Masked autoencoder (MAE) [141] (see Figure 8) simplifies it to an end-to-end denoising framework by predicting the masked patches from the unmasked patches.,1,related,1,positive
Masked Autoencoders (MAE) [30].,1,related,1,positive
"One of the key observations of the MAE work [30] is that the decoder does not need to be very good for the encoder to achieve good performance: by using only a small decoder, MAE successfully trains a ViT in an auto-encoder fashion.",1,related,0,negative
"Compared to a model that uses masked image modeling, the original MAE [27] and to the MaskFeat model [55], our model underperforms by 0.",1,related,1,positive
"First, inspired by [28, 97], we randomly mask some joints and require the model to reconstruct them.",1,related,1,positive
"Starting with the baseline ViT configurations used in the original BEiT series pre-training [5, 92, 123] (‚àó in Table 2), we progressively refine the model design and make the following observations: (i) The performance of SwiGLU FFN is mediocre with the random weight initialization method used in BEiT, but works quite well with JAX weight initialization [14, 51] (+1.",1,related,1,positive
"Still, we note that this efficient decoder retains much of the modeling capacity of standard masked decoders [5, 8, 14] that employ full self-attention on context and mask {zL, zM}.",1,related,1,positive
"Since the standard masked modeling [5, 8, 14] of applying a bidirectional transformer on mask and context {zM , zL} leads to a complexity of O((NM + NL)(2)), we opt into an efficient decoder that reduces the cost to linear to mask sizeO(NM ) while preserving as much modeling capacity as possible.",1,related,1,positive
"First of all, we pre-train a Masked Autoencoder (MAE) [13, 36] on a large-scale facial dataset in a selfsupervised manner.",1,related,1,positive
Our framework starts with pre-training a masked autoencoder (MAE) [9] from a set of face databases.,1,related,1,positive
"To verify transfer learning ability on scalable Vision Transformers beyond UNets, we compare ImageNet 2562 pre-trained DiT [43] to MAE pre-trained vanilla ViTs [17] on CIFAR-10
and Tiny-ImageNet.",1,related,1,positive
"Drawing from the connections between diffusion networks and denoising autoencoders (Section 3.1), we infer that DDAEs can produce linear-separable representations at some implicit encoderdecoder interfaces, resembling DAEs and MAEs.",1,related,1,positive
"Following [10, 26], we randomly mask the features of joints after the embedding layer.",1,related,1,positive
"Different from MAE [10] and some existing graph data augmentation methods [12, 23], we feed the topology of the original joints into the encoder.",1,related,1,positive
The great success of MAE [10] makes us rethink data augmentation.,1,related,0,negative
"Surprisingly, when we directly reconstruct masked joints using a method similar to that in MAE [10], the performance degrades instead.",1,related,1,positive
We will discuss the relationship between MAE [10] and graph data augmentation in detail in Sec.,1,related,1,positive
"Inspired by MAE [10], we propose an augmentation framework named MGPose.",1,related,1,positive
"As in MAE [23], the encoded tokens zT (Eqn.",1,related,1,positive
"Thus, this work is based on Masked AutoEncoders (MAE) [23] for pre-training.",1,related,0,negative
"We implemented the 3D MAE using 3D-ResNetBlocks instead of Vision Transformer, different from the previous study [13], due to the constraint of GPU memory.",1,related,1,positive
"Following the previous study [13], we implement an asymmetric design with a lightweight decoder compared to the encoder.",1,related,1,positive
"To avoid information leakage, we use self-supervised MAE for model pre-training on split ImageNet-subset.",1,related,1,positive
One exception is fine-tuning MAE on ImageNet-subset where we found the transformations used for CPP generate inferior results and thus we follow the data transformations used in the original paper [20].,1,related,1,positive
"DualPrompt [59] is originally designed using Transformer structures, so we only change the pre-trained weights to MAE to avoid information leakage.",1,related,1,positive
"To this end, we implement CPP with four up-to-date pre-training methods including ViT [14], Deit [53], Dino [5], and MAE [20] that sweep supervised and self/un-supervised learning as well as discriminative and generative models.",1,related,1,positive
"2) MIM: To handle unpaired images, we leverage the Masked Image Modeling (MIM) paradigm [59].",1,related,1,positive
"Therefore, we use a multiway transformer to extract multi-modal features and two linear layers to solve PLM and MIM tasks, respectively [38], [59].",1,related,1,positive
"Our VLP model achieved the best performance with a higher masking ratio of 85%, which is in contrast to the optimal masking ratio of 75% reported by MAE [59].",1,related,0,negative
"PosterV2-Vit feature: To acquire advanced visual features, we trained a Vision Transformer (ViT) [4] model on the AffectNet7 dataset [21] using unsupervised learning techniques [5].",1,related,1,positive
"In this paper, we revisit deep supervision for masked image modeling (MIM) [15, 11, 48, 2], a self-supervised pretraining strategy for Vision Transformer [12] (ViT).",1,related,1,positive
"Then, similar to MAE [15], we randomly mask a high proportion of patches, yielding a masked image xÃÉ.",1,related,1,positive
"For concreteness, we use MAE [15] to illustrate our underlying approach.",1,related,1,positive
"Then ViT-B is finetuned on ImageNet-1K [9], following common practice [15].",1,related,1,positive
"We follow the same training recipe of MAE [15], more details can be found in supplementary materials.",1,related,0,negative
"FULL 100% 79.1 86.2 59.7 75.0
Addition-based methods
MLP-3 1.60% 73.6 75.2 35.7 61.5 PROMPT-SHALLOW 0.04% 79.9 82.5 37.8 66.7 PROMPT-DEEP 0.23% 76.8 84.5 53.4 71.6 ADAPTER-8 1.18% 81.7 87.3 61.2 76.7 SPT-ADAPTER (ours) 0.33% 83.0 87.3 62.1 77.5
Reparameterization-based methods
baseline method on VTAB-1k benchmark with only 0.26% and 0.08% trainable parameters for MAE and MoCo v3 pretrained backbones, respectively.",1,related,1,positive
"We show more parameter sensitivity patterns for ViT-B/16 with various pre-training strategies (i.e., MAE [20] and MoCo V3 [11]) and datasets sampled from FGVC benchmark [24].",1,related,1,positive
"We conduct experiments on the plain vision Transformer backbone ViT-B/16 [13] that is pre-trained on ImageNet [27] with different pre-training strategies following [24], including supervised pre-training and self-supervised pre-training with MAE [20] and MoCo v3 [11] following [24].",1,related,1,positive
"Visualizations of sampled VTAB-1k datasets with MAE and MoCo V3 pre-trained ViTB/16 are shown in Figures 5, 6, 7.",1,related,0,negative
"For our RFFR model, we adopt a base version of Masked Autoencoder (MAE) [19] and train it on real faces with a batch size of 128.",1,related,1,positive
We find that Data2Vec [3] (row 3) attains similar performance to the MAE [18] (row 4) initialization we use in Sec.,1,related,1,positive
"Specifically, we find that visual representation learning (using masked autoencoding (MAE) [18]) not only improves performance, but also enables model scaling with ViTs.",1,related,1,positive
"Specifically, we propose novel FaceMAE module, a masked autoencoder [25] specialized for FER-W, to achieve the goal of FFL.",1,related,1,positive
"However, we designed FaceMAE, a masked autoencoder [25] specialized for FER-W, by making two major modifications to the original masked autoencoding scheme.",1,related,1,positive
"However, we notice that since MAE‚Äôs mask tokens only pass through the decoder [20], a shareddecoder design is critical in our scheme for mask tokens to learn mutual information before performing reconstructions in separate modal-specific decoders.",1,related,1,positive
We follow MAE [20] and Point-M2AE [64] to generate input tokens from images and point clouds.,1,related,1,positive
"Consequently, we design a MAE [20]-structured multi-modal learning framework that incorporates projection alignment for more interactive multi-modal learning.",1,related,1,positive
"For the image branch, we follow [20] to divide images into regular patches with a size of 16 √ó 16, before the ViT backbone.",1,related,1,positive
"To examine the effectiveness of our method, we perform DPPMask on two representative MIM methods: MAE [26], iBOT [67], which represent two different MIM frameworks: pixel reconstruction and feature contrast.",1,related,1,positive
"For the self-supervised learning models, DINO [5] and MAE [16], we additionally measure the endto-end performance of benchmark models pre-trained on our UnlabelledNAIP.",1,related,1,positive
"DINO [5] based on knowledge distillation and the generative model MAE [16] based on autoencoder, on our FireRisk.",1,related,1,positive
"For the self-supervised architectures, MAE [16] and DINO [5], we use ViT-B/16 [10] as the backbone and fine-tune on FireRisk using latent representa-",1,related,1,positive
"Using transfer learning, we fine-tune ResNet-50 [17], ViT-B/16 [10], as well as DINO [5] and MAE [16] with ViT-B/16 as the backbone, all of which were pre-trained on ImageNet [8], using our",1,related,1,positive
"‚Ä¢ To investigate the performance of supervised and self-supervised learning on our FireRisk, we employ ResNet [17], ViT [10], DINO [5], and MAE [16] as benchmark models.",1,related,1,positive
"On FireRisk, we present benchmark performance for supervised and self-supervised representations, with Masked Autoencoders (MAE) [16] pre-trained on ImageNet1k [8] achieving the highest classification accuracy, 65.",1,related,1,positive
"learning, we select two representative self-supervised models for their performance, namely DINO [5] and MAE [16].",1,related,1,positive
"In generating the latent representations, we use ViT-B/16 as the backbone architecture for MAE [16], pre-",1,related,1,positive
"‚Ä¢ We gather an unlabelled dataset, UnlabelledNAIP, from the NAIP remote sensing project and utilize it to pre-train novel latent representations of DINO [5] and MAE [16].",1,related,1,positive
"We choose publicly available PTMs, i.e., ResNet18/50/152 [24], ViT-B/16-IN1K/21K, ViT-L/16-IN1K, ViT-B/16-DINO [7], ViT-B/16-SAM [11], ViT-B/16-MAE [23], ViT-B/16CLIP [50] (image encoder) for a holistic evaluation, and report the results in Figure 6.",1,related,1,positive
"Additionally, we report the backbones adopted in the ablation study, including ViT-L/16-IN1K, ViTB/16-DINO [7], ViT-B/16-SAM [11], ViT-B/16-MAE [23], ViT-B/16-CLIP [50] (image encoder), in the table.",1,related,1,positive
"(3) RefCOCO, RefCOCOg, RefCOCO+ [81] 60K MLM with PEVL text encoder [78] Phrase Grounding (1) Flickr30K [79] 32K MLM with PEVL text encoder [78]
Visual Relationship Detection (1) Visual Genome [41] 101K MLM with PEVL text encoder [78] Visual Commonsense Reasoning (1) VCR [84] 100K MLM with PEVL text encoder [78]
Self-Supervised Learning (2) ImageNet-1K [10] 1.3M MoCo-v2 [7], MAE [24] TOTAL (47) 32M
them for specific downstream task.",1,related,1,positive
"Similar to MAE [22] and PointMAE [37], we compute the loss only on masked parts.",1,related,1,positive
"Following MAE [22] and VideoMAE [51], we adopt the asymmetric encoder-decoder design to reduce computation.",1,related,1,positive
"We use 3 alternative models to initialize the feature extractor in the mask generator: (1) the ViT-B network pretrained with MAE [16] (termed as MAE-800), (2) the ViT-B pretrained with iBOT [48] (termed as iBOTB), and (3) the ViT-S pretrained with DINO [3] (termed as DINO-S).",1,related,1,positive
"Also, since the methods
in this section refer to MAE, a comparison test is done between the methods in this section and MAE using the same training method.",1,related,0,negative
"The self-supervised pretraining method in this paper takes reference from MAE, but differs from it in that MAE uses two identical structures of ViT as encoder and decoder, while our method uses a symmetric convolution-deconvolution structure for the autoencoder.",1,related,1,positive
"The resulting data are shown in Table III, from which it can be seen that in each of the four datasets, our method is higher than MAE by more than 3 points in each metric.",1,related,1,positive
"Meanwhile, to better prove the reliability of the method proposed in this chapter, we conducted peer-to-peer experiments using MAE, i.e., we first performed masked self-supervised
learning pre-training, and then selected the one with the lowest training loss model for the pedestrian re-identification task.",1,related,0,negative
"Considering ViT‚Äôs flexibility and great potential in masked image modeling [9,14], we explore acceleration algorithms based on the standard ViT.",1,related,1,positive
"We implement a baseline inspired by MIM [2,9].",1,related,1,positive
"Instead of a random formulation [9, 30], we sample a fixed ratio Œ≥ of the tokens [M ] (X) to be masked out according to a multinomial distribution related to the patch hand saliency m(X):",1,related,1,positive
"Then this prior is encoded as the token form [9], and a ViT [15] sketcher is trained to disentangle the corresponding structure tokens from partial image patches [30].",1,related,1,positive
"Inspired by this, we implement an MAE by masking the outputs of our encoder, Fo, and then passing the masked encoded features along with the masked tokens to our decoder.",1,related,1,positive
"As illustrated in Figure 1 (b), we adopt a simple Regression head composed of two deconvolution layers and one 1√ó1 convolution layer on the reshaped Fo following the common setting of the previous works [23].",1,related,1,positive
"We initialize the Temporal ViTPose weights from the pre-trained model of MAE [23], and perform masked region reconstruction with a masking rate of 75%.",1,related,1,positive
"Regarding selfsupervised models, the masked autoencoder (MAE [30]), DINO [7], MoCov3 [10], MSN [2] were selected, because they all include the base ViT (ViT-B/16) for comparison between pretraining schemes (Fig.",1,related,1,positive
"Here, we make a modification to the original MAE (He et al. 2022) by replacing the masked contents with x and processing them in the same way as non-masked ones, instead of discarding them.",1,related,1,positive
"Therefore, we empirically set an even higher combined masking rate of 87.5% (compared to 75% used by He et al. (2022)) in our M3AE to make the self-supervising task nontrivial.",1,related,1,positive
"Here, we make a modification to the original MAE (He et al. 2022) by replacing the masked contents with xsub and processing them in the same way as non-masked ones, instead of discarding them.",1,related,1,positive
"Above all, witnessing the recent success of masked autoencoders in learning rich visual representations (He et al. 2022), we propose multimodal masked autoencoder (M3AE), where a random subset of the modalities and random patches of the remaining ones are masked simultaneously.",1,related,1,positive
"Therefore, we sample a random subset of the modalities for masking to mimic the real situation, in addition to randomly masking 3D patches of the remaining modalities as in the original MAE for natural images.",1,related,1,positive
"Evaluation of Representation: So, why is the Slow Learner such effective, and what accounts for the remaining performance gap? We perform a linear probing experiment [12] to evaluate the performance of the representation layer.",1,related,1,positive
"In addition to supervised pre-training, we consider representative self-supervised paradigms that provide pre-trained checkpoints on ViT-B/16, i.e., MoCo v3 [4], BEiT [2] and
MAE [12].",1,related,1,positive
"Considering architectural consistency with previous works of CLPM [43, 42], we select representative self-supervised methods (i.e., MoCo v3 [4], MAE [12] and BEiT [2]) that release checkpoints on ViT-B/16 in our comparisons.",1,related,1,positive
We utilize the mask ratio 25% and 8 decoder blocks following the practices in MAE [4].,1,related,1,positive
"To address the above issues, we propose to Mimic before Reconstruct for Masked Autoencoders, termed as MR-MAE, which is a simple and effective strategy to enhance MAE [4] by regularizing the intermediate representations with pre-trained off-the-shelf feature encoders.",1,related,1,positive
"Inspired by the popular masked image modeling [12, 13], we build a universal masked autoencoder architecture for VAD by embedding random mask tokens to simulate anomalies, which is a simple yet efficient synthetic method that avoids extra data processing in the original normal data.",1,related,1,positive
"The image encoder is initialized with the first 12 layers of MAE-base (He et al. 2022) weight, which is pre-trained on the ImageNet-1k without any labels.",1,related,1,positive
"We also provide a new pipeline achieving data augmentation efficiently for imbalanced image datasets, using cGAN or diffusion models and ResNet or Masked Autoencoder (MAE) classifiers.",1,related,1,positive
Table 6 provides goodness-of-fit (R2) and Mean Absolute Error (MAE) measurements for the function f .,1,related,1,positive
"To justify our proposed metric and pipeline work regardless of the classifier selection, we also provide the results with Masked Autoencoder (MAE) ViT-H128 [14] which shows state-of-the-art results in dataset such as [12].",1,related,1,positive
a ResNet [15] or Masked Autoencoder (MAE) [14] classifier also trained on the original set with sub-class labels are applied on the synthesized images to select qualified ones.,1,related,1,positive
5% 1 State-of-the-art (SOTA) classification validation accuracy with Masked Autoencoder ViT-H448 [14],1,related,0,negative
The high R2 and low MAE values show that the formulation of f is highly effective on modeling the relationship between SSIM-supSubCls and accuracy improvement with our proposed data augmentation pipeline.,1,related,1,positive
"The ResNet18 classifiers are trained for 100 epochs and the MAE for 50 epochs when their validation accuracy converges, with their hyperparameters remaining the same throughout the whole procedure in each case.",1,related,0,negative
Table 7 compares the state-of-the-art (SOTA) classification validation accuracy from [14] and ours on the whole iNaturalist-2019 dataset.,1,related,0,negative
"To certify that this conclusion can be drawn regardless of the classifier selection, we also conduct the experiments with MAE classifier and the same results are obtained.",1,related,0,negative
"Then
a ResNet [15] or Masked Autoencoder (MAE) [14] classifier also trained on the original set with sub-class labels are applied on the synthesized images to select qualified ones.",1,related,1,positive
"From this table, our results with Masked Autoencoder (MAE) is comparable with the SOTA one, even though the SOTA is with 448√ó 448 while ours is with 128√ó128 input image resolutions which largely reduce the need of running time and computational resources.",1,related,1,positive
"In our experiments, the deep generative models, ResNet18 and MAE classifiers are first trained on the original imbalanced set with sub-class instead of super-class labels. cGAN models are trained until the Frechet Inception Distance (FID) scores converge.",1,related,1,positive
"Specifically, our efficient centroid-based MIM outperforms the prior tokenbased MIM [2] and pixel-based MIM [21] in equivalent ViT size and epochs.",1,related,1,positive
"Besides, we also consider a baseline strategy (nofill) that drops the masked patches without refilling as in MAE [13].",1,related,1,positive
"Inspired by the work [24], we propose to use a masked autoencoder based on vision transformer.",1,related,1,positive
"Moreover, we have picked up some parameters that have been proven successful in [24].",1,related,0,negative
"Different from conventional autoencoders, the used masked encoderdecoder operates in an asymmetric fashion [24] enabling the encoder to operate on only the partial observed signal (without the masked patches) and the decoder to rebuild the full image based on the representation given by the encoder and the masked patches.",1,related,1,positive
"We use the standard ViT-B [14] as the encoder network and initialize it with the MAE pretrained [18] weights following [43, 88].",1,related,1,positive
BEiT [1] RRC+40% mask ViT+Linear DALLE SimMIM [56] RRC+60% mask ViT+Linear RGB MaskFeat [53] RRC+40% mask ViT+Linear HOG ConvMAE [15] RRC+75% mask ConvViT+MSA RGB MAE [20] RRC+75% mask ViT+MSA RGB,1,related,1,positive
"In the following analysis, we investigate MAE [20] by diagnosing its reconstruction target and input image patches, identifying two important but overlooked bottlenecks that could have hurt the representation quality.",1,related,0,negative
"We then present a careful analysis with the milestone method, MAE [20], to disclose two important but overlooked bottlenecks of most pixel-based MIM approaches (subsection 3.",1,related,1,positive
"However, unlike the MAE, the occlusion positions of samples generated by OIA are randomly sampled, so we need to conduct completion on each instance.",1,related,1,positive
"As mentioned in III-C, our FCD adapts MAE‚Äôs notion [25] of restoring entire features using implicit unoccluded features.",1,related,1,positive
We customize the original masking strategy from MAE [22] and design a facial part masking strategy to ensure that the model can learn the consistencies of all facial parts.,1,related,1,positive
Our masked autoencoder is based on an asymmetric encoder-decoder architecture [22].,1,related,1,positive
"To realize this restoration concept, we pretrain a simple yet powerful masked autoencoder (MAE) model [22] solely on real images.",1,related,1,positive
"In this stage, we customize MAE[22] and its masking strategy with modifications to perform self-supervised learning of real faces, so that the model can learn generic facial part consistency features and can also prevent over-fitting to specific forgery patterns.",1,related,1,positive
"Inspired by [22], we propose a novel self-supervised pretraining method that is based on masking and recovering faces, named Mover.",1,related,1,positive
The emergence of the masked autoencoder (MAE) [22] has greatly influenced our community.,1,related,0,negative
"For 8K iterations, we find VPDA32 surpass all the baseline methods, including those pre-trained on mask image modelling [17, 38], contrastive learning [7] and supervised learning [27, 29].",1,related,1,positive
"‚ô† SpiderCNN (Xu et al., 2018) 69.8 73.7 ‚ô† DGCNN (Wang et al., 2019) 73.6 78.1 ‚ô† PointCNN (Li et al., 2018) 75.1 78.5 ‚ô† GBNet (Qiu et al., 2021) 77.8 80.5 q PointBert (Yu et al., 2022d) - 83.1 q Point-MAE (Pang et al., 2022) - 85.2 q Point-TnT (Berg et al., 2022) 81.0 83.5
‚ô£ PointNet (Qi et al., 2017a) 63.4 68.2 ‚ô£ PointNet++ (Qi et al., 2017b) 75.4 77.9 ‚ô£ BGA-PN++ (Uy et al., 2019) 77.5 80.2 ‚ô£ PointMLP (Ma et al., 2022) 83.9 85.4 ‚ô£ PointMLP-elite (Ma et al., 2022) 81.8 83.8 r PointMLP-CoC (ours) 84.4‚Üë0.5 86.2‚Üë0.8
Context Clusters are a natural fit for point clouds Qi et al. (2017b); Lu et al. (2022).",1,related,1,positive
"1 on ImageNet and do not use dropout when pre-training on the much larger JFT-300M dataset; recent languagesupervised or self-supervised vision models (Radford et al., 2021; He et al., 2021) do not use dropout.",1,related,0,negative
"Unlike MAE He et al. (2022), BEiT Bao et al. (2022), and DIT Li et al. (2022), we do not use patch-level masking strategy for pre-training.",1,related,0,negative
"‚Ä¶8, we compare several tuning methods with our proposed U-Tuning based on different architectures and pre-trained models, i.e., IN-21K (Deng et al., 2009), MAE (He et al., 2022b) and CLIP (Radford et al., 2021) pre-trained ViT/B16 (Dosovitskiy et al., 2020), as well as CLIP pre-trained ViT/L-14.",1,related,1,positive
"In Table 8, we compare several tuning methods with our proposed U-Tuning based on different architectures and pre-trained models, i.e., IN-21K (Deng et al., 2009), MAE (He et al., 2022b) and CLIP (Radford et al., 2021) pre-trained ViT/B16 (Dosovitskiy et al., 2020), as well as CLIP pre-trained ViT/L-14.",1,related,1,positive
", MAE [15]), we use this finding to further reduce the computational complexity of our model.",1,related,1,positive
"Similar to recent works in the computer vision domain (e.g., MAE [15]), we use this finding to further reduce the computational complexity of our model.",1,related,1,positive
"For model fine-tuning with MAE, we adopt the settings in (He et al., 2021).",1,related,1,positive
"For example, we obtain better results by integrating our method into the fine-tuning process of the Masked AutoEncoder (MAE) in (He et al., 2021), achieving improvements on ImageNet by 0.3%, ImageNet-A by 1.8%, ImageNet-R by 1.4% and ImageNet-C by 3.7% on Vision Transformer Large (Dosovitskiy et‚Ä¶",1,related,1,positive
"15 is that we take the results directly from (He et al., 2021), which have no detailed data of different corruptions.",1,related,0,negative
"For example, we obtain better results by integrating our method into the fine-tuning process of the Masked AutoEncoder (MAE) in (He et al., 2021), achieving improvements on ImageNet by 0.",1,related,1,positive
"C.15 is that we take the results directly from (He et al., 2021), which have no detailed data of different corruptions.",1,related,0,negative
"FT follows MAE (He et al., 2022), while Ours uses MoCo v2 contrastive loss and regularization coefficient Œª = 0.",1,related,1,positive
"FT follows MAE (He et al., 2022), while Ours uses MoCo v2 contrastive loss and regularization coefficient Œª = 0.1.",1,related,1,positive
"FT follows the training in MAE (He et al., 2022) mostly, using SGD for 200 epochs with a cosine learning-rate scheduler, the base learning rate of 0.",1,related,1,positive
"FT follows the training in MAE (He et al., 2022) mostly, using SGD for 200 epochs with a cosine learning-rate scheduler, the base learning rate of 0.06, weight decay 5e-4, momentum 0.9 and batch size 256.",1,related,1,positive
"We adopted the masked auto-encoder (MAE) (He et al., 2022) to process the point cloud data (denoted as MAE-dense) for pre-training, which is close with Point-MAE (Pang et al., 2022).",1,related,1,positive
"To pre-train the MAE-dense and MAE-sampled, we first follow the standard train-test split of ShapeNet (Chang et al., 2015) adopted by Pang et al. (2022); Yu et al. (2022).",1,related,1,positive
It is denoted as MAE-sampled.,1,related,1,positive
"Specifically, we build the classifier by keeping the encoder structure and weights of the pre-trained MAE-dense and MAE-sampled models, followed by max-pooling as well as a fully connected layer of dimension [256, 256, 40] to map the global token of a dimension of 256 to the 40 categories.",1,related,1,positive
"3, our proposed MAE-sampled outperformed all state-of-the-art methods on 3 out of 4 settings, while MAE-sampled consistently outperformed MAE-dense.",1,related,1,positive
"We adopted the masked auto-encoder (MAE) (He et al., 2022) to process the point cloud data (denoted as MAE-dense) for pre-training, which is close with Point-MAE (Pang et al.",1,related,1,positive
"To evaluate the effectiveness of our claimed contribution, we replace the dense-attention layer in MAE-dense with our sampled-attention layer (Fig.",1,related,1,positive
"To maintain a fully self-supervised pre-training paradigm, we initialize with weights obtained by self-supervised ImageNet pre-training [19].",1,related,1,positive
"Similar to MAE [19], we normalize the output patches as well as the target patches prior to computing the loss, which we found to empirically improve the performance.",1,related,1,positive
2) MAE-unsupIN‚ÜíSN [19] ViT ImageNet+ScanNet 54.,1,related,1,positive
6) MAE-unsupIN‚ÜíSN [19] ViT ImageNet+ScanNet 63.,1,related,1,positive
"Although similar to MAE, we choose SimMIM because it adopts the backbone of Swin Transformer, which performs better than ViT adopted in MAE, as shown in the experiment.",1,related,1,positive
"It is shown that the robust curves of the models with the same
Springer Nature 2021 LATEX template
ARES-Bench 3
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8 1.0 A cc ur ac y
(a)
VGGs VGG13_Normal VGG16_Normal VGG19_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (b)
ResNets ResNet50_Normal ResNet50_MOCO ResNet101_Normal ResNet152_Normal Wide-ResNet50_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(c)
DenseNets DenseNet121_Normal DenseNet161_Normal DenseNet201_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(d)
ConvNexts ConvNextS_Normal ConvNextS_21K ConvNextB_Normal ConvNextB_21K ConvNextL_Normal ConvNextL_21K
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(e)
ViTs ViTS_Normal ViTS_21K ViTB_Normal ViTB_21K ViTB_MAE ViTL_Normal ViTL_21K ViTL_MAE
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(f)
XciTs XciTS_Normal XciTM_Normal XciTL_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (g)
T2Ts T2T14_Normal T2T19_Normal T2T24_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(h)
Swins SwinS_Normal SwinS_21K SwinB_Normal SwinB_21K SwinL_21K
Fig.",1,related,1,positive
"Springer Nature 2021 LATEX template
ARES-Bench 13
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8 1.0 A cc ur ac y
(a)
Normal Models VGG19_Normal ResNet152_Normal DenseNet161_Normal ConvNextL_Normal ViTL_Normal XciTL_Normal T2T24_Normal SwinB_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (b)
Pre-trained Models ResNet50_Normal ResNet50_MOCO ViTB_Normal ViTB_21K ViTB_MAE ConvNextL_Normal ConvNextL_21K SwinB_Normal SwinB_21K
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(c)
AT Models ResNet152_Normal ResNet152_AT ConvNextL_Normal ConvNextL_AT ViTB_Normal ViTB_AT XciTL_Normal XciTL_RB SwinB_Normal SwinB_AT",1,related,1,positive
"IN-Val IN-V2 IN-Real ON IN-A IN-R IN-V IN-C SIN IN-Sketch
ViTS
Normal 74.4 61.6 80.0 13.1 8.8 30.4 11.2 32.0 9.1 19.9 34.0 Pre-train 81.4 70.3 86.8 22.7 27.3 45.7 16.6 47.1 15.8 32.5 44.6 AT 70.2 57.3 77.9 11.5 6.1 46.0 8.5 27.8 16.8 29.8 35.2
ViTB
Normal 75.8 61.6 80.9 13.2 11.4 32.8 13.3 34.3 10.9 23.7 35.8 Pre-train 84.6 73.9 88.8 27.4 44.5 56.8 19.4 57.5 22.6 43.0 51.9 MAE 83.6 73.1 88.1 24.9 37.7 49.8 18.2 49.4 20.2 36.4 48.1 AT 73.4 60.4 80.5 12.7 8.9 50.7 9.4 36.6 22.2 35.7 39.1
ViTL
Normal 75.2 60.7 79.8 11.2 11.3 33.3 13.4 35.4 9.3 25.0 35.4 Pre-train 85.8 76.0 89.2 30.5 56.1 64.2 25.5 65.3 30.1 51.8 57.4 MAE 85.1 75.6 89.0 27.3 50.6 60.0 21.5 56.2 24.1 46.4 53.6
XciTS Normal 82.4 71.5 86.8 23.7 31.3 45.0 17.0 50.1 19.5 32.9 46.0
RB 73.3 60.5 80.6 12.7 6.3 45.7 9.7 28.5 18.4 31.2 36.7
XciTM Normal 82.6 71.0 86.8 23.4 33.3 44.7 17.7 50.5 20.3 33.1 46.3
RB 74.1 61.7 81.3 13.6 7.0 47.1 9.5 30.2 19.7 32.6 37.7
XciTL Normal 83.0 72.0 86.9 23.7 36.2 46.2 17.9 50.2 20.4 34.4 47.1
RB 75.1 62.7 81.7 13.4 8.8 49.0 10.7 32.0 19.9 34.4 38.7
T2T14 Normal 81.6 70.9 86.8 22.3 24.1 44.7 16.7 46.8 17.7 32.2 44.4
T2T19 Normal 82.3 71.6 87.2 23.2 29.0 47.3 18.0 50.2 20.9 34.4 46.4
T2T24 Normal 82.4 71.7 87.2 22.9 29.7 47.9 18.0 52.0 20.8 35.1 46.8
SwinS
Normal 83.2 72.1 87.5 24.7 33.0 44.9 19.3 45.1 16.8 32.0 45.8 Pre-train 83.3 73.5 88.6 28.1 43.9 54.8 21.3 50.6 17.2 41.2 50.3 AT 75.8 63.3 82.6 15.3 10.6 52.5 10.8 37.1 21.1 37.1 40.6
SwinB
Normal 83.4 72.3 87.6 25.5 35.8 46.6 20.2 45.6 17.9 32.4 46.7 Pre-train 85.1 75.2 89.1 28.8 51.8 59.1 22.7 56.4 19.6 45.1 53.3 AT 76.8 64.5 83.4 15.5 13.1 53.5 11.8 39.3 22.7 39.3 42.0
SwinL Pre-train 86.3 77.0 89.6 31.6 61.0 63.6 26.4 61.3 23.4 48.8 56.9
AT 78.7 66.9 84.9 18.2 18.1 57.3 11.6 43.4 25.2 42.9 44.7
increases from 34.1% of ResNet50 to 36.8% of ResNet101, and finally to 38.0% of ResNet-152.",1,related,1,positive
"C V
] 2
8 Fe
b 20
23
2 ARES-Bench
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8 1.0 A cc ur ac y
(a)
VGGs VGG13_Normal VGG16_Normal VGG19_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (b)
ResNets ResNet50_Normal ResNet50_MOCO ResNet50_AT ResNet50_RB ResNet50_RL ResNet101_Normal ResNet101_AT ResNet152_Normal ResNet152_AT ResNet152_FD Wide-ResNet50_Normal Wide-ResNet50_AT Wide-ResNet50_RB
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(c)
DenseNets DenseNet121_Normal DenseNet161_Normal DenseNet201_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(d)
ConvNexts ConvNextS_Normal ConvNextS_21K ConvNextS_AT ConvNextB_Normal ConvNextB_21K ConvNextB_AT ConvNextL_Normal ConvNextL_21K ConvNextL_AT
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(e)
ViTs ViTS_Normal ViTS_21K ViTS_AT ViTB_Normal ViTB_21K ViTB_MAE ViTB_AT ViTL_Normal ViTL_21K ViTL_MAE
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(f)
XciTs XciTS_Normal XciTS_RB XciTM_Normal XciTM_RB XciTL_Normal XciTL_RB
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (g)
T2Ts T2T14_Normal T2T19_Normal T2T24_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(h)
Swins SwinS_Normal SwinS_21K SwinS_AT SwinB_Normal SwinB_21K SwinB_AT SwinL_21K SwinL_AT
Fig.",1,related,1,positive
"Specifically, according to the pioneering work [19], given a natural image from an unlabeled dataset X, we divide it into N regular image patches, denoted as x ‚àà RN√óS where S denotes the patch size (e.",1,related,1,positive
"The training settings are the same as MAE [19], we adopt the same encoder-decoder structure to perform the MIM task.",1,related,0,negative
"For a fair comparison, we fully follow the hyperparameters of MAE [19] in our image classification experiments.",1,related,1,positive
"DeiT[14] proposes an effective receipt to train ViT with limited data, and MAE[15] adopts a masked autoencoder to pre-train the ViT.",1,related,0,negative
"We adopt the recipe in vanilla ViT[13], DeiT III[14], and MAE[15] to train ViTs.",1,related,0,negative
"Follow MAE (He et al., 2021), we evaluate the performance of the proposed Layer Grafted Pre-training with different number of fixing blocks.",1,related,0,negative
"‚Ä¶first step of Layer Grafted Pre-training, since it identically follows the original MIM pipeline, we directly adopt the pre-trained model of MAE (He et al., 2021) and conduct the second step, where we initialize the network with the MIM pre-trained model and train with Moco V3 (Chen et al.,‚Ä¶",1,related,1,positive
"For other settings such as data augmentation, LR scheduling or weight decay, we identically follow He et al. (2021).",1,related,1,positive
"‚Ä¶65.3 -
C-MAE (Huang et al., 2022) 73.9 65.3 77.3 MimCo (Zhou et al., 2022) 70.2 62.7 - Layer Grafted Pre-training (Ours) 77.7 65.5 77.8
ViT-L/16 MAE (He et al., 2021) 75.8 55.2 78.7
Moco V3 (Chen et al., 2021) 77.6 - - Layer Grafted Pre-training (Ours) 81.0 69.3 80.1
Our method also demonstrates‚Ä¶",1,related,1,positive
"For fine-tuning, we train with AdamW (Loshchilov & Hutter, 2017) for 100 epochs following MAE (He et al., 2021).",1,related,1,positive
"We by default adopt MAE (He et al., 2021) and Moco V3 (Chen et al., 2021) as our MIM and CL frameworks, respectively.",1,related,1,positive
"Then, the image with minimal augmentation would be utilized for computing MIM loss following MAE He et al. (2021).",1,related,1,positive
"In contrast, we employ MAE (He et al., 2021), an image modeling framework that is free from strong augmentations.",1,related,1,positive
"In this paper, we propose to predict the masked edges in Emask in training, inspired by the success of masked autoencoding in computer vision [23].",1,related,1,positive
"As shown in Figure 4, we adopt two reconstruct heads of simple linear layers to respectively recover the masked 2D pixels and 3D coordinates following MAE [He et al., 2021] and Point-MAE [Pang et al.",1,related,1,positive
"Additionally, we compared our method with some recent self-supervised methods, including MoCoV2 [32], MAE [13], and ConvMAE [33].",1,related,1,positive
"In this paper, we present VoxFormer, a strong camera-based 3D semantic scene completion (SSC) framework composed of (1) class-agnostic query proposal based on depth estimation and (2) class-specific segmentation with a sparse-to-dense MAE-like design.",1,related,1,positive
"Our framework is a two-stage cascade composed of class-agnostic proposals and class-specific segmentation similar to [68]: stage-1 generates class-agnostic query proposals, and stage-2 uses an MAE-like architecture to propagate information to all voxels.",1,related,1,positive
"Motivated by reconstruction-before-hallucination and sparsity-in-3D-space , we build a two-stage framework: stage-1 based on CNN proposes a sparse set of voxel queries from image depth to attend to images since the image features correspond to visible and occupied voxels instead of non-visible and empty ones; stage-2 based on Transformer uses an MAE-like architecture to first strengthen the featurization of the proposed voxels by voxel-to-image cross-attention, and then process the full set of voxels with self-attention to enable the voxel interactions.",1,related,1,positive
We name such learnable parameter as mask token [3] for conciseness since unselected from Q is analogous to masked from Q.,1,related,1,positive
Auto Encoder (MAE) [108] pre-trained model to initialize the ViT backbone.,1,related,1,positive
"Algorithmically, the Figure 2: Illustration of representative SSL methods: SimCLR [9], MoCo V3 [9], BYOL [15], and the Masked Auto-Encoder [10].",1,related,1,positive
"We further evaluate our defense under other popular SSL training algorithms and different model structures and datasets, e.g., ResNet-18 and ViT-Small/16 trained using SimCLR, MoCO V3, BYOL, MAE over CIFAR-10 or the ImageNet (Appendix 6.3).",1,related,1,positive
"Here we use self-supervised learning methods consistent with those in Section 4.3, including the contrastive learning method SimCLR, MoCO V3, BYOL, and the masked-model training method MAE.",1,related,1,positive
"For Case-1, we incorporate four state-of-the-art SSL training methods, i.e., SimCLR [9], MoCo V3 [14], BYOL [15], and the MAE [10], for evaluation.",1,related,1,positive
"For example, the accuracies of our model pre-trained with MAE were 8.68% and 5.16% higher than those of ResNet101 pre-trained with SimSiam in the five-class and binary classification tasks.",1,related,0,negative
"By leveraging a simple SSL framework, MAE, we alleviated the problem of training classification models without sufficient high-quality labeled OCT images.",1,related,1,positive
[35] as the initial weights of our model.,1,related,1,positive
"In the model pre-training stage, we transfer the weights of the MAE encoder and decoder pre-trained on the ImageNet-1 K dataset [35] (see the upper part of Fig.",1,related,1,positive
"For the classification task of cervical OCT images, we are
the first to propose a ViT-based image classification model pre-trained with a non-contrastive SSL framework, MAE, which can help ease the burden of insufficient labeled image data on the model‚Äôs prediction performance.",1,related,1,positive
A few essential parameters were configured in the selfsupervised model pre-training with MAE.,1,related,0,negative
"1(a) presents the self-supervised pre-training of our model with MAE, which includes a ViT encoder and a lightweight Transformer decoder.",1,related,1,positive
"For example, compared with the supervised MViT-B (the SOTA model) with the weights transferred from the ImageNet-1 K dataset, the five-class and binary classification accuracies of ViT-B (224) pre-trained with MAE were increased by 2.65% and 1.52%, respectively.",1,related,1,positive
"However, note that our approach is general and easily extends to self-supervised settings. e.g. (Chen et al., 2020; He et al., 2021; Chen et al., 2021).",1,related,1,positive
We only take the unmasked patches as the input of the encoder similar to MAE [9].,1,related,1,positive
"(Yang et al., 2022)
Oriented RCNN ViTAE 81.24 Use MAE (He et al., 2022) to pretrain the plain ViTAE transformer.",1,related,1,positive
"With the pretraining method MAE andRVSA, the detector outperformed all previousmethods, achieving 81.24% and 71.05%mAP onDOTA-V1.",1,related,0,negative
"First, it trains a ViT-based encoder fŒ∏(¬∑) on all images in X via self-supervised methods such as MAE [28].",1,related,1,positive
"For feature extraction, we use MAE [7].",1,related,1,positive
"For pre-training Edge MAE, we tuned the learning rate as 1.5e-4, weight decay as 0.05, the mask ratio as 1/3, and batch size as 2048.",1,related,0,negative
"From the two tables, we can see that, Edge Transformer outperforms almost all baseline models, and the results can be further improved by pre-training Edge MAE on unlabeled edges.",1,related,1,positive
"Furthermore, we propose a novel Edge Transformer model and pre-train the model via Masked Auto-Encoders (MAE).",1,related,1,positive
"In this section, we evaluate our Edge Transformer and Edge MAE in two settings.",1,related,1,positive
"We can see the proposed Edge MAE achieves the best results, and link prediction models perform better as they model the whole return process entirely.",1,related,1,positive
"4.2 Parameter Settings For Edge Transformer and Edge MAE, we set the latent vector size ùê∑ as 256, the dropout rate as 0.0, and the number of attention heads as 3.",1,related,1,positive
"Inspired by MAE [7] in CV which pre-trains masked auto-encoders on unlabeled images, we propose to improve Edge Transformer with an Edge MAEmodel.",1,related,1,positive
"For the MAEViT-Base (He et al., 2022) baseline, we report additional results for Œª values 1, 10, and 50.",1,related,1,positive
"When Œ∑ = 1.0, it means the number of features in the baseline is the same as the number of features learned using PISCO and as a result, similar for CIFAR-10 results in ¬ßD.2, we observe the in-distribution performance of PISCO in this case being almost the same as that of baseline methods even for higher values of Œª.
MAE-ViT-Base results for Œª values 1, 10, and 50: Results for additional values of Œª when MAE-ViT-Base is the baseline are in Table 19.",1,related,1,positive
"We also report analogous results for another popular feature extractor, MAE-ViT-Base (He et al., 2022), in Table 5.",1,related,1,positive
"We train two types of auto-encoders (AEs) to reconstruct the images in the collection D, namely denoising AEs [73] and masked AEs [28].",1,related,1,positive
We employ Adam [36] and the mean squared error (MSE) to train both AEs.,1,related,1,positive
"First of all, we propose four novel pre-retrieval predictors, namely (i) the magnitude of the reconstruction error of denoising [73] or masked [28] auto-encoders trained on the database, (ii) the density of the k-means cluster to which the query image embedding is assigned, (iii) the confidence distribution of a classification head attached to the embedding layer of the retrieval model, and (iv) the score predicted by a fine-tuned ViT model [20].",1,related,1,positive
"We compare the soups to a nominal model, the l‚àû-robust classifier used in the soups, their ensemble, the Masked Autoencoders of [18], AdvProp [54], PyramidAT [24], and the ensemble obtained by averaging the output (after softmax) of the four models included in the soups.",1,related,1,positive
"Finally, while the capability of selecting a model specific to each image distribution is a main point of our model soups, we also show that it is possible to jointly select a soup for average performance across several IMAGENET variants to achieve better accuracy than adversarial and self-supervised baselines [18,24].",1,related,1,positive
Other alternatives carefully tweak R per dataset and architectures e.g. to only compute the reconstruction loss on parts of the data as with BERT Devlin et al. (2018) or MAEs He et al. (2022).,1,related,1,positive
"|D<t) where k ‚àà [1, . . . ,K] on ImageNet for 4 different architecture and pre-training methods: ResNet50 + SimCLR, ResNet50 + BYOL, ViT/B16 + DINO and ViT/B16 + MAE.",1,related,1,positive
"For ResNet-50, we pre-train on supervised SimCLR and BYOL losses; for ViT/B16, we pre-train on supervised (Touvron et al., 2020; He et al., 2021) DINO and MAE losses.",1,related,1,positive
"Natural # Special # Structured #
ViT/B16 DINO 1.57 1 1.00 1 2.63 3 ResNet50 BYOL 2.71 2 3.00 2 2.13 1 ViT/B16 MAE 5.71 6 3.25 3 2.38 2 ViT/B16 SUP 2.86 3 3.25 4 5.75 6 ResNet50 SimCLR 4.29 5 5.75 6 3.38 4 ResNet50 SUP 3.86 4 4.75 5 4.75 5
ranks in Table 1b.",1,related,1,positive
"Following the command in Masked AutoEncoder [9], we use a lightweight decoder, which has only 10% computation per token compared with the encoder.",1,related,1,positive
"After the pre-training & fine-tune method [18] was proposed, more andmore experiments demonstrated its superior performance, so here, we also used the Masked AutoEncoder pre-trained ViT-base model.",1,related,0,negative
"As for the structure of our decoder, inspired by Masked AutoEncoder [9], Our decoder is also based on the attention algorithm, which has the same structure as the encoder(ViT-base) but is only 10% of its size.",1,related,1,positive
Proposed Masked AutoEncoder.,1,related,0,negative
"(2022), Z is obtained by clustering the latent features produced by a Masked Autoencoder (He et al., 2022).",1,related,1,positive
"For example, in Liu et al. (2022), Z is obtained by clustering the latent features produced by a Masked Autoencoder (He et al., 2022).",1,related,1,positive
"To further improve the robustness of SGLDM to different sketches, we adopt the arbitrarily masking conditional training strategy in training inspired by Masked AutoeEncoder [7], which masks random patches of the input and let the model reconstruct it automatically.",1,related,1,positive
"2022), we aim to adapt the data-efficient masked image autoencoder (MAE) (He et al. 2022) to multi-modal and small-scale RGB-D datasets for better representation learning.",1,related,1,positive
"Driven by the good self-supervised learning performance of MAE (He et al. 2022) in images, we design a multi-modal masked autoencoder (MM-MAE) in RGBD data.",1,related,1,positive
"Unlike the original MAE (He et al. 2022), our CoMAE presents a shared encoder and decoder among RGB and depth modalities and acts as a kind of regularizer to guide pre-training.",1,related,0,negative
"Inspired by the results in VideoMAE (Tong et al. 2022), we aim to adapt the data-efficient masked image autoencoder (MAE) (He et al. 2022) to multi-modal and small-scale RGB-D datasets for better representation learning.",1,related,1,positive
"In addition, our CoMAE instantiated with ViT-B also achieves
competitive and even better results of the same ViT-B model pre-trained by ImageMAE (He et al. 2022), this reveals the data efficiency advantage of our CoMAE pre-training.",1,related,1,positive
"Vision models We include 14 computer vision models (VMs) in our experiments, representing three families of models: SegFormer (Xie et al., 2021), MAE (He et al., 2022), and ResNet (He et al., 2016).",1,related,1,positive
We also compare M2A2E with the symmetric multimodal MAE [3] when finetuning on all downstream modality settings.,1,related,1,positive
"Compared with the vanilla MAE [18], M(2)A(2)E adopts the same masked strategy in unimodal ViT encoder but targeting at multimodal reconstruction with multiple unshared ViT decoders.",1,related,1,positive
"We find that 1) leveraging local feature descriptors benefits the ViT on IR modality; 2) partially finetuning or using adapters can achieve reasonable performance for ViT-based multimodal FAS but still far from satisfaction; and 3) mask autoencoder [3, 18] pre-training cannot provide better finetuning performance compared with ImageNet pre-trained models.",1,related,1,positive
"Following MAE [21], we use the normalized pixel values as targets in the reconstruction task.",1,related,1,positive
"Following MAE [21], we only feed the visible, unmasked patches to the online encoder Es.",1,related,0,negative
"For MIM tasks, we apply augmentations and masking strategy following MAE [21] to the input of the online encoder.",1,related,1,positive
"5, which is lower than the best configuration reported in (He et al., 2022).",1,related,0,negative
"The masking ratio used during pre-training was set to 0.5, which is lower than the best configuration reported in (He et al., 2022).",1,related,0,negative
"For our experiments, we use the CNN feature map (Santoro et al., 2017; Zambaldi et al., 2018) for end-to-end learned fixedregion representations and the mini-patch representations from pre-trained Masked AutoEncoder (MAE) (He et al., 2022) for pre-trained fixed-region representations.",1,related,1,positive
"The pre-training of the MAE encoder was conducted using a batch size of 128, a learning rate of 0.001, and a weight decay of 0.05.",1,related,0,negative
"In this work, we develop our pretraining objective based on a masked image modeling approach like [41, 18].",1,related,1,positive
We perform OOD detection with MIM pretext task with each metric ‚Äì the results are shown in Tab.,1,related,1,positive
"For one-class OOD detection, we pre-train the MIM model and finely tune it on ImageNet-21k [49], as recommended by BEiT [2].",1,related,0,negative
"Specifically, we adopt the masked image modeling (MIM) [2, 11, 20] as our self-supervised pretext task, which has been demonstrated to have great potential in both natural language processing [11] and computer vision [2,20].",1,related,1,positive
"In our method, we pre-train the model with Masked Image Modeling (MIM) pretext [11] on a large dataset and fine-tune it on the ID dataset.",1,related,1,positive
"Specifically, we adopt the masked image modeling (MIM) [2,11,20] as our self-supervised pretext task, which has been demonstrated to have great potential in both natu-ral language processing [11] and computer vision [2,20].",1,related,1,positive
We compare the performance of MIM and contrastive learning pretext task MoCov3 [8] in Tab.,1,related,1,positive
"In the MIM task, we split images into patches and randomly mask a proportion of image patches before feeding the corrupted input to the vision transformer.",1,related,1,positive
"For multi-class OOD detection, we pre-train the MIM model, intermediately use fine-tuning on ImageNet-21k [49], and apply fine-tuning again on the ID dataset.",1,related,1,positive
The self-supervised pretext task in our framework is Masked Image Modeling (MIM).,1,related,1,positive
"Inspired by the tremendous success of the masked autoencoding paradigm in NLP [19] and Computer Vision [32], we propose to incorporate a new masked autoencoder model into our generalized framework.",1,related,1,positive
"paradigm in NLP [19] and Computer Vision [32], we propose to incorporate a new masked autoencoder model into our generalized framework.",1,related,1,positive
"Notably, whenŒ≥ = 1, our masked autoencoder is equivalent to MAE for vision [32].",1,related,1,positive
"We also hope to incorporate selfsupervised methods that learn representations by training on a union of unlabeled data from source and target via proxy tasks like reconstruction (Gidaris et al., 2018; He et al., 2022) and contrastive learning (Caron et al.",1,related,1,positive
"We also hope to incorporate self-supervised methods that learn representations by training on a union of unlabeled data from source and target via proxy tasks like reconstruction (Gidaris et al., 2018; He et al., 2022) and contrastive learning (Caron et al., 2020; Chen et al., 2020).",1,related,1,positive
"Pretrained MAE and CLIP with world model (MAE+WM, CLIP+WM)
Masked autoencoder (MAE; He et al. 2021) learns visual representation in a self-supervised manner by training a vision Transformer (ViT; Dosovitskiy et al. 2021) to reconstruct masked patches.",1,related,1,positive
"Convolutional feature embedding Unlike prior work that masks random pixel patches (He et al., 2021), we embed camera observations into convolutional feature maps and mask these features following the design of Seo et al. (2022a).",1,related,1,positive
"Specifically, we consider CLIP (Radford et al., 2021) and MAE (He et al., 2021) representations as they have recently shown to be effective for robotic manipulation (Shridhar et al., 2021; Radosavovic et al., 2022).",1,related,1,positive
"We hypothesize this is because spatial information redundancy (He et al., 2021) is more significant in visual observations from
manipulation tasks than natural images.",1,related,1,positive
"We hypothesize this is because spatial information redundancy (He et al., 2021) is more significant in visual observations from manipulation tasks than natural images.",1,related,1,positive
"We also incorporate the idea of a prior work (Seo et al., 2022a) that masks convolutional features instead of pixel patches (He et al., 2021) and predicts rewards to learn representations capturing fine-grained details required for visual control.",1,related,1,positive
"Convolutional feature embedding Unlike prior work that masks random pixel patches (He et al., 2021), we embed camera observations into convolutional feature maps and mask these features following the design of Seo et al.",1,related,1,positive
"R O
] 3
1 M
ay 2
02 3
experiments on RLBench (James et al., 2020) show that our method outperforms single-view representation learning baselines (Radford et al., 2021; He et al., 2021; Seo et al., 2022a) and a multi-view representation learning baseline (Sermanet et al., 2018).",1,related,1,positive
", 2020) show that our method outperforms single-view representation learning baselines (Radford et al., 2021; He et al., 2021; Seo et al., 2022a) and a multi-view representation learning baseline (Sermanet et al.",1,related,1,positive
"We then perform extensive experiments on CIFAR10 and CIFAR100 datasets with various representative pre-trained vision models (He et al., 2016; Dosovitskiy et al., 2021; Chen et al., 2020d;c; Radford et al., 2021; Xie et al., 2022; He et al., 2022), which are trained in supervised or self-supervised manners.",1,related,1,positive
"1, we empirically analyze the main assumptions of our theory in various deep vision models (Dosovitskiy et al., 2021; He et al., 2016; Radford et al., 2021; Chen et al., 2020d;c; He et al., 2022; Xie et al., 2022).",1,related,1,positive
"com/facebookresearch/mae (He et al., 2022) License https://github.",1,related,0,negative
"We adopt pre-trained checkpoint in (He et al., 2022).",1,related,0,negative
"(3) (4)Note that for methods like MAE (He et al., 2022; Pang et al., 2022), the teacher network is an identity mapping with no parameters, and therefore œÜ is an empty set.",1,related,1,positive
"(3) 4Note that for methods like MAE (He et al., 2022; Pang et al., 2022), the teacher network is an identity mapping with no parameters, and therefore œÜ is an empty set.",1,related,1,positive
"The masked ratio can be even higher than the original configuration described in MAE (He et al., 2022) (e.",1,related,1,positive
", 2021) and MAE (He et al., 2022), we adopted AdamW (Loshchilov & Hutter, 2017) for pre-training, fine-tuning.",1,related,0,negative
"According to the setup in MoCo v3 and MAE (He et al., 2022), both adopted a batch size of 4,096.",1,related,0,negative
"To address the representation deficiency issue in MLM, we propose a simple framework MAE-LM, which pretrains bidirectional Transformer encoders using the MLM objective, but based on the Masked Autoencoder (He et al., 2022) structure.",1,related,1,positive
"To address the representation deficiency issue, we propose a simple text encoder pretraining method, MAE-LM, which conducts MLM pretraining based on the Masked Autoencoder architecture (He et al., 2022).",1,related,1,positive
"AE + SR 16 denotes a baseline experiment with a auto-encoder architecture as in He
et al. (2022).",1,related,1,positive
"In Table 3, we compare our method against DINO Caron et al. (2021), MoCo-V3 Chen et al. (2021), MaskFeat Wei et al. (2021), BEiT Bao et al. (2021), iBOT Zhou et al. (2022), and MAE He et al. (2022).",1,related,1,positive
"Unlike recent methods Zhou et al. (2022); He et al. (2022), we do not perform exhaustive searches for the optimal hyperparameters such as learning rates.",1,related,1,positive
The result of MAE He et al. (2022) with 400 epochs is based on our reimplementation.,1,related,0,negative
"(2021a); Ho et al. (2020) with ‚Üì (x) = ‚àöŒ≥x + ‚àö 1‚àí Œ≥Œµ, with Œµ ‚àº N (0, I) and Œ≥ uniformly sampled as Œ≥ ‚àº U(0, 1).",1,related,1,positive
"We follow the details presented in MAE He et al. (2022) and implement an asymmetric
Methods GPUs √ó H Acc.",1,related,1,positive
"To show the effectiveness of noisy image modeling in visual feature learning and adversarial defense, we adopt two simple, representative MIM methods, SimMIM [24] and MAE [10] for comparison.",1,related,1,positive
"In our study, the random sampling masking ratio is 75% [37].",1,related,0,negative
"Referring to the research about the decoder in pretraining in MAE [37], combined with the task of our work, we design an extremely lightweight encoder.",1,related,0,negative
"Because the reconstruction part appears only in pretraining and is not used in the sequence-based inference, we follow He et al. (2022) and use a lightweight decoder with fewer channels in Revolution.",1,related,1,positive
"Inspired by the work of Masked Autoencoder [20], our transformer can be divided into an encoder part and a decoder
1 3
part.",1,related,1,positive
"Inspired by the work of Masked Autoencoder [20], our transformer can be divided into an encoder part and a decoder Fig.",1,related,1,positive
"Inspired by these, we inherit the recent success of spectrogram SSL in the frequency domain, which guarantees efficient compression and high-level semantic understanding.",1,related,1,positive
"Following He et al. (2022), we split low-resolution radiograph into nonoverlapping image patches, where 75% patches are randomly masked.",1,related,1,positive
"In Figure 2, we provide relation maps of three samples from ImageNet, using 10 checkpoints of MAE-Large [13].",1,related,1,positive
2 Results and analysis ImageNet We measure the label error detection performance on ImageNet with the synthetic label noise by training an MAE-Large model [13].,1,related,1,positive
"We measure the detection performance with MAE-Large [13], BEIT-Large [1], and ConvNeXt-Large [29] models.",1,related,1,positive
"For ADE20K, the input size is set to 512√ó512 following previous works (Bao et al., 2021; He et al., 2021; Chen et al., 2022a; Zhou et al., 2021).",1,related,1,positive
"‚Ä¶reconstruct the targets.
methods:
`MIM(Decoder(Regressor(Encoder(Rv))),Target(Rm)), (3) where Target(Rm) is a function to map the masked patches to the targets, e.g., d-VAE (Ramesh et al., 2021) token used in CAE and BeiT (Bao et al., 2021), or normalized RGB values used in MAE (He et al., 2021).",1,related,1,positive
"The main tracker only consists of a ViT backbone and a box estimation head, we test both ViTBase and ViT-Large, and the ViT parameters are initialized with MAE (He et al. 2022) pre-trained model.",1,related,1,positive
"Our approach is inspired by the previous MIM method (Xie et al. 2022; He et al. 2022), but we have to deal with two fundamental problems in the tracking framework: (1) Visual tracking is a downstream vision task that generally does not have the pre-train process to apply the MIM strategy.",1,related,0,negative
We only use the pretrained encoder part to extract the image features [32].,1,related,1,positive
"Note, we have not demonstrated it here, but Zorro can also be trained using unimodal self-supervised methods such as MAE [25] and DINO [12] separately on the audio and visual streams.",1,related,0,negative
"Motivated by the great success of other MAE-style approaches (He et al., 2021; Feichtenhofer et al., 2022; Hou et al., 2022), we also adopt an asymmetric design that the encoder only operates visible tokens after applying masking on input embedding, and a lighter decoder processes encoded tokens‚Ä¶",1,related,1,positive
"It is hypothesized and summarized in (He et al., 2021; Feichtenhofer et al., 2022) that the masking ratio is related to the information density and redundancy of the data, which has an immense impact on the performance of the autoencoders.",1,related,1,positive
Our encoder/predictor architecture is reminiscent of the generative masked autoencoders (MAE) [35] method.,1,related,1,positive
"Methods without view data augmentations data2vec [7] ViT-L/16 1600 73.3
MAE [35] ViT-L/16 1600 67.1 ViT-H/14 1600 71.5
trained with I-JEPA requires less computational effort than a ViT-Small/16 trained with iBOT.",1,related,0,negative
I-JEPA outperforms MAE while requiring less pretraining epochs when using a similar encoder architecture.,1,related,0,negative
"Methods without view data augmentations data2vec [7] ViT-L/16 1600 77.3
MAE [35] ViT-B/16 1600 68.0 ViT-L/16 1600 76.0 ViT-H/14 1600 77.2
CAE [21] ViT-B/16 1600 70.4 ViT-L/16 1600 78.1
Closest to our work is data2vec [7] and Context Autoencoders [24].",1,related,1,positive
We draw inspiration from MAE [19] for this design.,1,related,0,negative
We draw inspiration from MAE [20] for this design.,1,related,0,negative
"Our decoder is a lighter-weight 8-layer 512-dimensional transformer, following MAE [19].",1,related,1,positive
Our decoder follows the decoder design from MAE [20].,1,related,1,positive
"For the image reconstruction loss, we normalize the ground-truth per-patch, following [38].",1,related,1,positive
"During finetuning, we use a layer-wise learning rate decay, following [17, 38, 3].",1,related,1,positive
"We find that a combination of two state of the art approaches: masked auto-encoders, MAE [38] and contrastive language image pre-training, CLIP [69] provides a benefit over CLIP when trained on a corpus of 11.",1,related,1,positive
"For all models which rely on masking, we use a 75% masking ratio, consistent with [38, 31], as we did not find an alternative that improved downstream results.",1,related,0,negative
"As in [38] we experiment with predicting both normalized and un-normalized patch values, finding that predicting the normalized patch value slightly improves the performance of our MAE implementation.",1,related,1,positive
"For evaluation on CIFAR-10, CIFAR-100, Tiny Imagenet-100 and CBIS-DDSM datasets, we use the linear probing strategy (He et al., 2021).",1,related,1,positive
"For evaluation on CIFAR-10, CIFAR-100, Tiny Imagenet-100, and chest X-ray datasets, we use the linear probing strategy (He et al., 2021).",1,related,1,positive
"3 exhibits the performance of PARSeq with CLIPTER, when leveraging the vision-based image encoders of DiNO, ViT-MAE and OWL-ViT, and when using the visionlanguage models of CLIP, BLIP and GIT.",1,related,1,positive
"When transferring the representation to ImageNet-1K [18], we follow the widely used fine-tuning recipe introduced by [5], [24].",1,related,1,positive
"For single-modal SSL methods, we choose MoCoV3 [14] and SimCLR [11] as representative discriminative methods, and MAE [24] as a representative generative method.",1,related,1,positive
"During pre-training, input images are resized to 224√ó 224 and we set random mask ratio to 75% following [28].",1,related,1,positive
"Among numerous architecture designing spaces, without loss of generalization, we adopt an asymmetric encoderdecoder architecture following MAE [28] and a dualencoder architecture following CLIP [47] for their flexibility.",1,related,1,positive
We follow most of setups in [28] for fine-tuning.,1,related,1,positive
"Meanwhile, following MAE [28], we randomly mask a large portion of image patches and leave the remaining patches to be",1,related,1,positive
We choose MAE [28] and CLIP [47] as representative methods of masked image modeling and vision-language contrastive learning.,1,related,1,positive
"Inspired by MAE [12], we propose a naive background reconstruction method (NBR).",1,related,1,positive
"Therefore, we utilize a mature but large Vision Transformer (MAE-ViT-large [12]) for background reconstruction.",1,related,0,negative
"3) Comparison With Feature Extraction: We use pre-trained MAE10 [72] and SimCLR11 [73] models to extract features for the NUS-WIDE dataset and obtain 768- and 2,048dimensional feature representations respectively.",1,related,1,positive
"3) Comparison With Feature Extraction: We use pre-trained MAE10 [72] and SimCLR11 [73] models to extract features for the NUS-WIDE dataset and obtain 768- and 2,048- dimensional feature representations respectively.",1,related,1,positive
"We compute the loss only on masked parts, similar to BERT (Devlin et al., 2019) and MEA (He et al., 2021), so the model output on unmasked parts is qualitatively worse.",1,related,1,positive
"Our research is based on masked autoencoding, which is a form of more general denoising autoencoding (He et al., 2021).",1,related,1,positive
"We also observe calculating loss values only on masked patches gives higher accuracy (row 6), which is consistent with He et al. (2021).",1,related,1,positive
"We try to conclude that in essence, the difficulty is rooted in the gap between language and vision in terms of data processing (Bateman, 2014; He et al., 2021).",1,related,1,positive
"One may therefore ask: what exactly is preventing the application of BERT to convnets? We try to conclude that in essence, the difficulty is rooted in the gap between language and vision in terms of data processing (Bateman, 2014; He et al., 2021).",1,related,1,positive
"We then remove the hierarchical design (row 4), which results in a single-scale masked modeling that is commonly used
for transformers (Devlin et al., 2018; Bao et al., 2021; He et al., 2021; Xie et al., 2021).",1,related,1,positive
"As vision encoder, we consider (1) ViT-B/16 [7] (patch size of 16√ó16 pixels) with pre-trained weights from self-supervised MoCo-v3 [5], DINO [2] and MAE [10], all trained on IN-1K but without any labels.",1,related,1,positive
"For the downstream task of OAR segmentation, we employed the ViT backbone and UperNet [20] decoder as the encoder and decoder parts of the segmentation model, following the implementation described in a previous work [10].",1,related,1,positive
"Specifically, we replace the multi-crop strategy with a random masked sampling strategy, consistent with pioneering SSL approaches that use masked image modeling with ViT in a patch-wise manner [9], [10].",1,related,1,positive
"Then for enhancing the learning of visual representations, we introduce a masked autoencoder (MAE)[8] branch, which is parallel to the SSL framework and they share the same encoder.",1,related,1,positive
"Therefore, we investigate other MIM methods besides MAE[8] and observe that LoMaR[18] can further boost the model performance by 0.",1,related,1,positive
Therefore we introduce a masked autoencoder (MAE)[8] branch to enforce the visual representation learning and help ViT generate more accurate pseudo,1,related,1,positive
"As for the MAE branch, we follow the default settings of [8].",1,related,1,positive
"For data augmentation, we follow the settings in MAE [18].",1,related,1,positive
"In this work, we adopt MAE [18] as the MIM model due to its popularity and simplicity.",1,related,1,positive
"Therefore, instead of following the common transferring assumption, we revisit the old good idea of training with indomain egocentric data only, but this time in light of the development of recent data-efficient training methods, such as masked autoencoders [33, 63, 28] as well as the scale growth of egocentric data collections (e.",1,related,1,positive
Our method applies the original MAE [33] and video MAE [28] algorithms.,1,related,1,positive
"4, we visualize the MAE [33, 28] reconstruction results on a few Ego4D [31] examples with a ViT-B [25] trained for 200 epochs without per-patch normalization.",1,related,1,positive
"In image representation, MAE [24] and SimMIM [25] use the random masking strategy to discard or replace the selected patches and in this paper, we adopt the former approach for selected frames.",1,related,1,positive
"It is worth noting that, unlike MAE [24] or SimMIM [25], we do notmake predictions for themasked sequences at the pixel level, choosing to reconstruct the input at the representation level in an implicit way and ensuring that the pair of masked sequences can be as close as possible in the representation.",1,related,1,positive
"Inspired by the masked auto-encoders [24], we consider the masking mechanism as a way of data augmentation and combine it with contrastive learning framework; the frame-",1,related,1,positive
"Compared to the plain ViT pre-trained with MAE [31], our approach performs similarly up to the Large model regime, despite using much fewer parameters (198M vs 307M).",1,related,0,negative
"We compare our approach to previous masked auto-encoder methods [3, 31, 77], which were all designed for transformer-based models.",1,related,1,positive
"To perform this analysis, we randomly select 1,000 images from different classes in the ImageNet-1K validation set and extract the high-dimensional features from each layer of different models, including the FCMAE models, the ConvNeXt supervised model [52] and the MAE pretrained ViT model [31].",1,related,1,positive
We retrained the CSWin transformer without redesign and original transformer [9] separately and compared them with our redesigned,1,related,0,negative
"In this work, we tackle an active visual exploration problem using a vision transformer model [15] as the architectural backbone.",1,related,1,positive
"Table 3: MAE Attention as glimpse: We use CLS attention of pre-trained MAE[15] as the glimpse map and use locations corresponding to the max value, min value, median value, and random value as the next glimpse location.",1,related,1,positive
We study the use of CLS attention map as an alternative for the learned glimpse map through a set of heuristics to use the base MAE model for active visual exploration.,1,related,1,positive
"Since the pretext task for training MAE, i.e. random masking and pixel value prediction for masked regions, resembles the partial observability constraint that we are trying to solve, we find MAE encoder to be best suited for our context extractor module and the pre-trained weights to be transferable for our use case.",1,related,1,positive
"We build our context encoder on the mask-auto-encoder (MAE) [15] model, pre-trained as a masked-image-model for the image reconstruction pretext task.",1,related,1,positive
"In this work, we evaluate our method on the widely studied task of image reconstruction,
We first compare against a baseline where the base MAE model with random glimpse selection is finetuned on the SUN360 and ADE20K datasets, denoted by ‚ÄòRandom glimpse‚Äô in Table 1.",1,related,1,positive
"Next, in addition to finetuning the task module and the context extractor, initialized with MAE weights, we train the glimpse selection module to predict the loss of the task module (i.e reconstruction loss).",1,related,1,positive
"While the random selection outperforms other heuristics for base MAE, we see that random selection of glimpse performs inferior to a learned glimpse selection policy, Table 1.",1,related,1,positive
Analysis for the importance of self-supervised feature: Our encoder and decoder are initialized with pretrained self-supervised weights [15].,1,related,1,positive
"As we intend to evaluate the use of base MAE for active vision, we do not finetune it on SUN360 dataset.",1,related,0,negative
9% on Imagenet 1000 class classification [15].,1,related,0,negative
"We show that vision transformer mod-
els, and in particular MAE, trained on large unlabelled data can replace contemporary CNN-based counterparts.",1,related,1,positive
We therefore use MAE‚Äôs decoder as our task module.,1,related,1,positive
Our context extractor is a ViT [12] initialized with MAE‚Äôs encoder weights.,1,related,1,positive
"2) Generating Input Tokens: According to ViT [52], a trainable and lightweight projection should be applied to embedding patches, denoted as F .",1,related,1,positive
"Baselines We benchmark the performance of our two baselines - Average Pairwise Feature Distance (Distance) and Cluster Assignment Distribution (Cluster) using five different feature extractors for each: pixels (œÜ ), three intermediate ResNext-101 [32] convolutional feature layers (œÜ, œÜ, and œÜ), and features from the encoder output of a Masked Autoencoder [13] (œÜ ).",1,related,1,positive
"Finally, œÜ extracts the per-patch features produced by the encoder of a large Transformer model (we used Masked AutoEncoder [13]).",1,related,1,positive
"Finally, œïT extracts the per-patch features produced by the encoder of a large Transformer model (we used Masked AutoEncoder [13]).",1,related,1,positive
"Baselines We benchmark the performance of our two baselines - Average Pairwise Feature Distance (Distance) and Cluster Assignment Distribution (Cluster) using five different feature extractors for each: pixels (œïP ), three intermediate ResNext-101 [32] convolutional feature layers (œïR(1), œïR(2), and œïR(3)), and features from the encoder output of a Masked Autoencoder [13] (œïT ).",1,related,1,positive
"Specifically, we pre-train models based on MAE [14] for 100 epochs on ImageNet-1k [17] with uniform masking and plot the corresponding convergence curve on the validation set during fine-tuning.",1,related,1,positive
"We pre-train models on ImageNet1K [17] following the settings of MAE [14], where the decoder TABLE I ABLATION STUDY ON THE ADAPTIVE LEARNING RATE SCALE RULE.",1,related,1,positive
1) Disjoint Masking: We evaluate DM based on MAE [14] with normalized pixels as reconstruction targets and only pretrain 100 epochs for fast ablation study on ImageNet-1K.,1,related,1,positive
"Following a standard SSL evaluation recipe [14], [21], we conduct end-to-end fine-tuning or linear probing for classification on ImageNet-1K and transfer learning for object detection/instance segmentation on COCO [56] and semantic segmentation on ADE20k [57].",1,related,1,positive
"In Table III, we evaluate the training efficiency of DM based on several MIM methods with their original pre-training recipes, including MAE [14], BEiT [10], SimMIM [15], and MaskFeat [16].",1,related,1,positive
"Another line of work is completionbased [25, 31, 36, 55, 58, 60] methods, which get inspiration from Masked Autoencoders [14].",1,related,1,positive
"Particularly, in the self-supervised setting, e.g., as in MAE (He et al., 2022), we identify a condition number that is pivotal to solving downstream tasks.",1,related,1,positive
", as in MAE (He et al., 2022), we identify a condition number that is pivotal to solving downstream tasks.",1,related,1,positive
Scale-MAE is a selfsupervised pretraining framework based on the Masked Autoencoder (MAE) [25].,1,related,1,positive
"As a result, we develop an asymmetric encoder-decoder architecture similar to MAE [15].",1,related,1,positive
"Therefore, in this framework, we construct a semi-supervised cross-algorithm ensemble method for lesion identification in UW-OCTA DR images based on MAE, ConvNeXt, and SegFormer algorithms[10,24,13,5,6].",1,related,1,positive
"In this framework, first, we pre-train the MAE algorithm [10] on the DR grade assessment dataset in the diabetic retinopathy analysis challenge (DRAC) 2022.",1,related,0,negative
"After the pre-training, we would extract the backbone of the MAE encoder (also considered as pre-trained MAE-ViT) [9,10].",1,related,0,negative
"Then, the MAE encoder [10] performs feature extraction on the visible patches subset of the image.",1,related,1,positive
"We conduct experiments using MVTN with ResNet-50 [37] and ViT [21] as backbone networks, starting from scratch or using weights from ImageNet [73], Dino [9], and Masked Autoencoders (MAE) [36] as initial weights.",1,related,1,positive
"Figure 1: Architecture of the Masked Autoencoder (MAE) in [8], with Vision Transformer (ViT) backbone.",1,related,1,positive
We divide an image into regular non-overlapping patches as in [8] and then calculate the gradient sum of each patch and rank them by its gradient sum.,1,related,1,positive
"Inspired by He et al[7], we propose a full convolutional neural network[16] based cross-modal text feature extractor(TFE).",1,related,1,positive
"One is a decoder (He et al., 2022) which reconstructs the input image, the other is a linear classifier.",1,related,1,positive
"Secondly, we compared the classical MIM-based methods such as MAE (ViT-B) (He et al., 2022) and SimMIM (Swin-B) (Xie et al.",1,related,1,positive
"We denote the methods with * when they are adapted to Point Transformer backbone, e.g., ‚ÄúPointMAE [39]* + PointTrans‚Äù.",1,related,1,positive
"With the same consistency loss, PointMAE gets 71.0%, which is 0.9% lower than ours.",1,related,0,negative
"In our experiments, RGMIM and MAE used the ViT-Base model.",1,related,0,negative
"Compared with the vision transformer-based methods RGMIM [69] and MAE [70], although we use the traditional and straightforward ResNet model, our method outperformed them, especially when the amount of annotated data was significantly reduced.",1,related,1,positive
"We employ the vanilla ViT model (Dosovitskiy et al., 2021) as the backbone of our audio SSL models without heavy
structure engineering, and apply the speed-up technique proposed in He et al. (2022).",1,related,1,positive
"A learnable corruption embedding e[M] is used to replace the masked position, with which the corrupted representation ZM = 1(M) e[M] + 1(1‚àíM) T is input to encoder (Devlin et al., 2019) or decoder (He et al., 2022b)2.",1,related,1,positive
"We find that this leads to an inferior result, consistent with the observation in 2D that data of low semantics requires a non-trivial decoder for modeling purpose (He et al., 2022b).",1,related,1,positive
"The objective is to train the a student encoder fS to predict/reconstruct the output from a teacher encoder fT , where the teacher could be a discrete variational autoencoder (dVAE) (Bao et al., 2022) or simply identity mapping (He et al., 2022b).",1,related,1,positive
"D (7)
With this parameter-efficient prompt tuning strategy, we are able to tune the pretrained foundational Transformer while preserving as much pretrained knowledge as possible (He et al., 2022a).",1,related,1,positive
"To comprehensively compare our proposed model with other state-of-the-art methods, We use four widely used metrics to evaluate our method: structuremeasure (Sm) [6], E-measure (Em) [7], weighted F-measure (FœâŒ≤ ) [19], and mean absolute error (MAE).",1,related,1,positive
"We note that this budget constraint is similar to MAE [10] and other random masking schemes, which choose to randomly mask out a fixed 75% of the image.",1,related,1,positive
"To learn robust audio-video representations, we go beyond raw input reconstruction in uni-model MAEs [2, 4, 55], multimodal MAE [56], and CAV-MAE [41].",1,related,1,positive
"Following MAE [2], we only keep the pre-trained encoders and use the average-pooled top-layer outputs for classification.",1,related,0,negative
We notice a concurrent and independent study CAV-MAE [41] uses inter-modal contrastive objective and MAE [2] to reconstruct raw inputs.,1,related,1,positive
"For the visual branch, we either randomly initialize it or initialize it with the self-supervised MAE [2] pre-trained on ImageNet (compared in Table 6).",1,related,1,positive
"Furthermore, inspired by He et al. (2021), we explore enhancing the visual encoder via randomly masking the input image tokens and then reconstructing them, which can help reduce the computation cost during training and boost visual embedding by maintaining low-level visual information.",1,related,1,positive
MAE means MAE unsupervised pretraining [30] on the MillionAID [54].,1,related,0,negative
"We followed the data augmentation scheme of the MAE[21] fine-tuning phase, and the rest of the settings were kept consistent with the pre-training phase.",1,related,0,negative
"In Table 11, we compare our results with previous supervised pretraining methods[18, 29], self-supervised MIM methods [2, 10, 3, 7, 5] and CLIP-based MIM methods [25, 31, 13, 24] (i.",1,related,1,positive
We extend the masked-autoencoding framework [41] to learn audiovisual feature representations that can be leveraged for both multimodal and unimodal downstream tasks.,1,related,1,positive
"In the masked autoencoding framework [41], the input, x, is tokenised following previous supervised learning setups [6, 26, 33].",1,related,1,positive
"In the masked autoencoding framework [10, 29, 41], the decoder is another transformer that reconstructs the masked tokens given the encoded tokens as context.",1,related,1,positive
"Additional standardisation may also be applied to xÃÉ [41, 76].",1,related,0,negative
"Our approach is inspired by the masked autoencoding framework [10,41], which itself is based on similar masked data modelling approaches in NLP [24] and earlier works ar X iv :2 21 2.",1,related,1,positive
We begin with an overview of masked autoencoders [41] and transformers in vision in Sec.,1,related,1,positive
We follow the training strategy in MAE [31] and VideoMAE [21] for image teachers and video teachers respectively.,1,related,0,negative
"In this paper, we follow the decoupled encoder-decoder transformer architecture in MAE [31] due to its effectiveness and simplicity.",1,related,1,positive
"For obtaining spatial targets, we adopt the vanilla image ViT pretrained by masked image modeling [31] on the image dataset (e.",1,related,1,positive
"To show the feasibility of the concept, we explore the adoption of masked autoencoder (MAE) structure [21] for image reconstruction, which achieves state-of-the-art performance in the self-supervised learning regime.",1,related,1,positive
"1, we compare the pooling strategies based on the scratch training and fine-tuning on pre-trained models by Self-Supervised Learning (SSL), including MAE [27], BeiT [3], SimMIM [77] and Data2Vec [2].",1,related,1,positive
"Every SSL model are pre-trained with ImageNet-1K, except BeiT [3] with ‚Ä† used ImageNet-22K.",1,related,1,positive
We ablate the type of visual representation and prior use by trying an initialization using the VGG16 network [68] (VideoDex-VGG) and the MVP network [7] [69] (VideoDex-MVP) based representation trained for robot learning.,1,related,1,positive
"Inspired by the representation bottleneck and the mask-based method MAE [45], we propose a training pipeline based on the mask to boost data augmentation.",1,related,1,positive
"Inspired by the representation bottleneck and the mask-based method MAE [45], we propose a training",1,related,1,positive
"We also compare Deep Incubation with the recently proposed improved E2E baselines in [15], where a systematically hyper-parameter search is performed on training configurations.",1,related,1,positive
We follow the design choice of MAE [23] and VideoMAE [50] that skips the video mask token [M] in the encoder and then insert it in the decoder.,1,related,1,positive
"Following [23,50], we only apply the encoder on visible video tokens, a small subset (e.",1,related,1,positive
"Inspired by the success of MAE [18] in 2D images, we develop the masked autoencoder for self-supervised learning on LiDAR point clouds, as shown in Figure 4.",1,related,1,positive
"To set a baseline with the transformer decoder, we follow MAE [18] and some existing works [12,18,19,38,73] to design the pipeline, as shown in Figure 5(a).",1,related,1,positive
"Then, we preserve the encoder part of MAE as our shared face encoder and fix it all the time during training.",1,related,0,negative
"Thus, we opt to employ a pre-trained masked autoencoder (MAE) (He et al. 2022) to extract facial features that better capture facial appearances and identity information.",1,related,0,negative
"In contrast to the ViT decoder in MAE, we find the convolutional decoder achieves more realistic results.",1,related,1,positive
"Compared to the compact latent code of StyleGAN2 (Karras et al. 2020) and the identity embedding, the latent space of MAE can better capture facial appearances and identity information, because masked training requires reconstructing masked image patches from visible neighboring patches, thus ensuring each patch embedding contains rich topology and semantic information.",1,related,1,positive
The encoder is designed following MAE (He et al. 2022) and pre-trained on a large-scale face dataset using the masked training strategy.,1,related,0,negative
"As for F swa, we first pre-trained the face encoder following the training strategy of MAE on our face dataset.",1,related,0,negative
"To verify the superiority of using the latent representation of MAE, we train a new model which adopts the identity embedding as the identity representation and employs AdaIN as the injection method.",1,related,1,positive
"We first train our face masked autoencoder, following the same settings in MAE4 (He et al. 2022).",1,related,0,negative
"To demonstrate the robustness of VQT to different pretraining setups, we also evaluate VQT on self-supervised (MAE) [20] and image-language (CLIP) pre-trained [39]",1,related,1,positive
"Moreover, in order to demonstrate the robustness of the compatibility, we evaluate performance on three different pre-trained backbones: self-supervised pre-trained (MAE with ImageNet1K) [20], image-language pre-trained (CLIP) [39] and supervised pre-trained (ImageNet-21K).",1,related,1,positive
"To demonstrate the robustness of VQT to different pretraining setups, we also evaluate VQT on self-supervised (MAE) [20] and image-language (CLIP) pre-trained [39]
backbones.",1,related,1,positive
We make the following modifications to the MAE decoding process to customize it for document image generation and our task unification framework: (4.a) Cross-Attention with Character Embeddings.,1,related,1,positive
"Next, we describe the MAE decoding process.",1,related,1,positive
"For the vision decoder, we adopt the decoder of MAE [14] and directly generate the image pixels with text and layout information.",1,related,1,positive
We adopt the MAE objective [14] for vision selfsupervised learning.,1,related,0,negative
"Thus, here we directly apply a standard masked image modeling (MIM) pipeline [5, 19, 48] for training, illustrated in Figure 2.",1,related,1,positive
"We show that LOCA yields improved performance over state-of-the-art supervised [53,60,65] and unsupervised [13,17,34,84] representation learning methods for ViTs when transferred to 11 diverse and challenging semantic segmentation benchmarks.",1,related,1,positive
"In terms of training efficiency, based on our implementation, one LOCA epoch takes 17.4 minutes while one MAE epoch takes 5.7 minutes.",1,related,0,negative
"Indeed, we have observed that freezing the backbone and training a linear classifier on top of MAE features perform very poorly [34].",1,related,1,positive
"In this section, we compare LOCA to popular state-ofthe-art SSL models for ViTs: DINO [13], MoCo-v3 [17], MAE [34] and iBOT [84].",1,related,1,positive
"Under the longer training schedule (800 epochs), our model reaches 83.9% accuracy, 0.4% higher than MAE (He et al. 2021) and
0.9% higher than RandSAC (Hua et al. 2022) (a concurrent autoregressive work of ours).",1,related,0,negative
"For linear probing, we use the feature of the last block and adopt an extra BatchNorm (Ioffe and Szegedy 2015) layer before the linear classifier following MAE.",1,related,1,positive
"We followed the pre-training parameters in MAE (He et al. 2021) and MaskFeat (Wei et al. 2021) without using color jittering, drop path, and gradient clip.",1,related,1,positive
"Our model pretrained with 300 epochs achieves the same accuracy as MAE (He et al. 2021) pretrained with 1600 epochs, which indicates that our method converges faster in the pretraining stage.",1,related,1,positive
"We observe that our method achieves 47.8 mIoU, which is slightly lower than MAE by 0.3, but higher than all others.",1,related,0,negative
"We note that BEIT and MAE are pretrained with 1600 epochs and use grid-search to find the best hyperparameters, while we only pretrain 800 epochs and don‚Äôt tune any parameters in the fine-tune stage due to limited access to computation.",1,related,1,positive
"The patch-based iGPT achieves 82.70 Top1 accuracy, which is higher than DeiT-base (Touvron et al. 2021)(81.8) but still lags behind the MIM method (e.g. 83.6 for MAE).",1,related,0,negative
"While, our proposed stochastic autoregressive image modeling, SAIM, utilizes all the information of the image to generate clear images, and achieve better fine-tuning accuracy than MAE on ImageNet-1K.",1,related,1,positive
"Method Plane Bcycl Bus Car Horse Knife Mcyle Persn Plant Sktb Train Truck Mean
CDAN [48]
R es
N et 85.2 66.9 83.0 50.8 84.2 74.9 88.1 74.5 83.4 76.0 81.9 38.0 73.9 MCC [36] 88.1 80.3 80.5 71.5 90.1 93.2 85.0 71.6 89.4 73.8 85.0 36.9 78.8 SDAT [57] 95.8 85.5 76.9 69.0 93.5 97.4 88.5 78.2 93.1 91.6 86.3 55.3 84.3 MIC (SDAT) 96.7 88.5 84.2 74.3 96.0 96.3 90.2 81.2 94.3 95.4 88.9 56.6 86.9 TVT [87]
V iT 92.9 85.6 77.5 60.5 93.6 98.2 89.3 76.4 93.6 92.0 91.7 55.7 83.9 CDTrans [85] 97.1 90.5 82.4 77.5 96.6 96.1 93.6 88.6 97.9 86.9 90.3 62.8 88.4 SDAT [57] 98.4 90.9 85.4 82.1 98.5 97.6 96.3 86.1 96.2 96.7 92.9 56.8 89.8 SDAT w/ MAE [25] 97.1 88.4 80.9 75.3 95.4 97.9 94.3 85.5 95.8 91.0 93.0 65.4 88.4 MIC (SDAT) 99.0 93.3 86.5 87.6 98.9 99.0 97.2 89.8 98.9 98.9 96.5 68.0 92.8",1,related,0,negative
"Moreover, we compare the image imputation performance of our Edge-MAE with vanilla MAE [24].",1,related,1,positive
"We adopt the partial fine-tuning strategy inspired by [24], i.",1,related,1,positive
"Inspired by [16,23,46], we perform mask sampling on video and text to achieve end-to-end video-text alignment.",1,related,1,positive
"Instead of blindly applying the mask-thenprediction paradigm from MAE, we propose a maskedthen-alignment paradigm, namely Masked Contrastive Pretraining, for efficient video-text alignment.",1,related,1,positive
"Without blindly applying mask-then-prediction paradigm from MAE, we explore the masking contrastive mechanism based on
the video language domain, and propose a mask-thenalignment paradigm to efficiently learn a multimodal alignment.",1,related,1,positive
"Another common tactic is fulfilled with sinusoidal mapping Fsine [20,54], through which p is generated on-the-fly by a fixed function dependent on NH , NW and D. Due to space limitation, we provide explicit expression in Appendix B.1.",1,related,1,positive
We demonstrate the explicit mapping function Fsine for sine-cosine positional embedding p as follows.,1,related,1,positive
The Masked Autoencoder (MAE) method [29] further takes advantage of masking to reduce training time and memory.,1,related,0,negative
"Our fine-tuning implementation follows MAE [29], with the learning rate tuned for each entry.",1,related,0,negative
MAE sparsely applies the ViT encoder [20] to visible content.,1,related,1,positive
"Our work is related to MAE and its vision-language extensions [23, 41, 31, 19].",1,related,1,positive
"We do not use a reconstruction loss, unlike MAE [29].",1,related,0,negative
"Inspired by the sparse computation of Masked Autoencoders (MAE) [29], we randomly remove a large portion of image patches during training.",1,related,1,positive
"By default, we apply our models on intact images at inference-time, similar to [29].",1,related,1,positive
We use MAE pre-training for most experiments by default unless otherwise specified.,1,related,0,negative
"Thus, for the model initialized from MAE pre-training, we increase finetuning iterations to 180k and batch size to 64.",1,related,0,negative
"In experiments, we explore two pre-training schemes: 1) MAE pre-training: The ViT backbone is initialized from the self-supervised MAE [13] trained on ImageNet-1K [7], while the rest of the model parameters are randomly set; 2) GIT pre-training: The ViT backbone and text decoder are initialized from the pre-trained image VL model GIT [33] and the rest are randomly set.",1,related,1,positive
We find MAE pre-training can alleviate overfitting and benefit from more training epochs as discussed in [20].,1,related,0,negative
"Additionally, a special Classification (CLS) token is appended before adding the sinusoidal positional embedding as per convention [4], [5], [7] such that the final dimension of the input data after patchification is (S+1)√óD.",1,related,1,positive
"Inspired by the masked auto-encoder (MAE) [21], we modified and applied it to the adversarial environment.",1,related,1,positive
"Similar to MAE [25], we found that applying a batch normalization layer [30] without affine transformations is beneficial for VideoMAE models.",1,related,1,positive
"Unless stated otherwise, we train our models for 500 epochs (for example, training with VMAEB on SSV2 takes 137 minutes with one 3090 GPU) with a batch size of 512 and use all 16 clips.",1,related,0,negative
"Our SCALE linear with SVT backbone beats the previous state of the art (71.8% vs. 71.5% [18]) and SCALE ft can even improve the accuracy of VMAEBft , which is a strong supervised model, from 81.5% to 81.84%.",1,related,1,positive
"We choose œÅBYOL for their excellent linear performance, SVT for the usage of ViT [14], and VMAE for showing 1) the applicability of our proposed method to MAE models, 2) the scalability of our method to larger models, and 3) possibility of using supervisedly fine-tuned models as our backbone.",1,related,1,positive
We also used a backbone pretrained and fine-tuned on SSv2 (VMAEBSSv2) for the SSv2 experiment to show the universality of SCALE with respect to the pretraining dataset.,1,related,1,positive
"Since our clip representations are somewhat abstract representations of the video, we expect the optimal masking ratio to be close to NLP models rather than video MAEs.",1,related,1,positive
"Pretrained backbones: We use the pretrained checkpoints of œÅBYOL [18], SVT [44], and three variants of VideoMAE [54] (base(B), large(L), and fine-tuned base(FT)).",1,related,1,positive
"With SCALE k-NN, we see a consistent improvement over the baseline and find that pre-trained MAE-based models greatly benefit from our training.",1,related,0,negative
We even improve the supervised model trained on SSv2 (VMAEBSSv2).,1,related,1,positive
"Then, with this frozen visual encoder, we used the same feed forward architecture, Q-function parameterization, and training objective (CQL with C51) as scaled QL to
finetune the MAE network.",1,related,1,positive
"As before, we also evaluate fine-tuning performance using the representations obtained via masked auto-encoder pre-training (He et al., 2021; Xiao et al.).",1,related,1,positive
"In Figure 7, we present our results for offline fine-tuning on 5 games from Lee et al. (2022), ALIEN, MSPACMAN, SPACE INVADERS, STARGUNNER and PONG, alongside the prior approach based on decision transformers (‚ÄúDT (pre-trained)‚Äù), and fine-tuning using pre-trained representations learned from state-of-the-art self-supervised representation learning methods such as contrastive predictive coding (CPC) (Oord et al., 2018) and masked autoencoders (MAE) (He et al., 2021).",1,related,1,positive
"For MAE, we first pretrained a vision transformer (ViT-Base) (Dosovitskiy et al., 2020) encoder with 80M parameters trained via a reconstruction loss on observations from multi-game Atari dataset and freeze the encoder weights as done in prior work (Xiao et al.).",1,related,1,positive
MAE is a more recent self-supervised approach that we find generally outperformed CPC in this comparison.,1,related,1,positive
"For the MAE implementation, we used the Scenic library (Dehghani et al., 2022) with the typical configuration used for ImageNet pretraining, except using 84√ó 84√ó 4 sized Atari observations, instead of images of size 224√ó 224√ó 3.",1,related,1,positive
We train the MAE for 2 epochs on the entire multi-task offline Atari dataset and we observe that the reconstruction loss plateaus to a low value.,1,related,1,positive
"We train the completion network on the 11,426 complete skeletons using a masked auto-encoder strategy [32] where the missing keypoints are masked at the input and will be predicted using the unmasked keypoints.",1,related,1,positive
", 2022) and masked autoencoders (He et al., 2021) are all using patches as input.",1,related,1,positive
"Similar to [7], we utilized ADAMW [13] with learning of 1.",1,related,1,positive
"1, our framework consists of three asymmetric vision transformer inspired by the work of [7].",1,related,1,positive
"Inspired by these two papers, we add a mask-based random reconstruction module to the input images: x ‚Üê g(x), where g is a masked auto-encoder [16], defined by:",1,related,1,positive
"Here, we examine whether an additional reconstruction module can regularize the partial derivatives to make them semantic-aware to produce human-meaningful images [16, 41, 56].",1,related,1,positive
"The pre-training loss is computed on the masked tokens similar to MAE [29], and the final pre-training loss Lpre-train is defined as: Lpre-train = Lnode + Ledge, Lnode = ‚àë",1,related,1,positive
"We investigated works in NLP [17] and computer vision (CV) [29] where the pre-trained models have been dominantly used, and we find that the key to most successful pre-training models is to design simple but effective tasks that can scale well.",1,related,0,negative
"Inspired by MAE [29], we propose a bi-branch graph masking and reconstruction task for molecular pre-training.",1,related,1,positive
"Similar to MAE [29], we propose a transformer-style asymmetric encoder-decoder architecture for each branch.",1,related,1,positive
"To enable effective expansion, we explore three prior models for guided imagination: DALL-E2 [56] and Stable Diffusion [59] are advanced image generative methods, while MAE [24] is skilled at reconstructing images.",1,related,1,positive
"Thanks to strong image reconstruction abilities, our GIF-MAE applies the MAE-trained model [24] as its prior model.",1,related,0,negative
"As DALL-E2 [56] and SD are powerful in generating images, and MAE [24] excels at reconstructing images, we explore their use as prior models of GIF for data imagination.",1,related,1,positive
"We choose ViT-Base (ViT-B) [24] as the backbone of our framework for both audio and video modalities, due to its stability in performance across different data streams [34, 30, 8].",1,related,1,positive
"Inspired by the recent success of Transformers in different domains [23, 30, 34, 25], we use ViT [24] as the backbone of our framework for both audio and visual modalities.",1,related,1,positive
"Further, we drop the masked tokens x before feeding the input to Œ∏ae for computational efficiency [3, 34].",1,related,1,positive
"Inspired by the recent success and scalability of pretraining with masked reconstruction in different domains [23, 13, 65, 30, 12, 34, 50, 64], we adopt masked data mod-",1,related,1,positive
"Our suggested method, MAEDAY, addresses FSAD by using Masked AutoEncoder (MAE) [9], a model trained for general image completion based on partial observations.",1,related,1,positive
": To investigate the impact of neighbors on network performance, we set the number of neighbors [2, 8, 16, 32, 64] and compared them in the table II.",1,related,1,positive
"To learn visual representations of images in a self-supervised manner, we apply Masked Autoencoder (MAE) which is
trained to reconstruct the randomly masked image patches.",1,related,1,positive
"A comparison between the attention maps generated by iTPN, the variant without integral pre-training (w/o iPT), and the MIM baseline (MAE [25]).",1,related,1,positive
"We emphasize here that we directly adopted the pre-trained model from masked autoencoder (MAE) (He et al., 2022), instead of training the model ourselves.",1,related,1,positive
"Therefore, our dual transfer learning reduces training time via utilizing the public model from MAE (He et al., 2022).",1,related,0,negative
"Different from other masked prediction variants [12, 9], we found mask loss is not useful in our setting, as our goal is to obtain an scalable decision making model but not only for representation learning.",1,related,1,positive
"‚Ä¢ multi-goal reaching: For every trajectory in the validation set, we randomly sample a start state and 5 goal states at random future timesteps from [12, 60).",1,related,1,positive
Our first key observation is that masked token prediction with random masking similar to BERT [9] and MAE [12] provides a general and flexible way for learning from unsupervised data.,1,related,1,positive
"Architecture Our encoder is a Transformer [33] but applied only on visible, unmasked states and actions, similar to MAE [12].",1,related,1,positive
"As in recent unsupervised papers [1, 10, 11, 13], we compare the quality of our unsupervised keypoints to the features obtained by a fullysupervised ImageNet model.",1,related,1,positive
"An exponential moving average (EMA)[20] with Œ± = 0.998 is implemented, and the shadow weight is updated after each training step.",1,related,1,positive
The structural configuration of SIVT follows the design of the MAE-base but we reduce the embedding dimension to 240 for efficient computation.,1,related,1,positive
ST-MAE is proposed in [23] that is based on the MAE [19] utilizing the Siamese encoder,1,related,1,positive
"Co-DETR with Swin-L yields 56.9% and 62.3% AP on LVIS val and minival, surpassing ViTDet with MAE-pretrained [13] ViT-H and GLIPv2 [40] by
+3.5% and +2.5% AP, respectively.",1,related,0,negative
"3% AP on LVIS val and minival, surpassing ViTDet with MAE-pretrained [13] ViT-H and GLIPv2 [40] by Method Backbone enc.",1,related,0,negative
"However, our network‚Äôs encoder and decoder are asymmetric, which indicates a significantly smaller decoder [25, 57].",1,related,0,negative
"Method Better Worse √ó3, √ó4 training warm-start [40] scratch std in normalization from data [25] 1.",1,related,1,positive
Implementation Details We adopt the model architecture of Masked AutoEncoder (He et al. 2022) with BERT (Devlin et al. 2018) word embeddings and language modeling head.,1,related,1,positive
Implementation Details We adopt the model architecture of Masked AutoEncoder (He et al. 2022) with BERT (Devlin et al.,1,related,1,positive
"For test-time training we do not use any augmentation, instead we construct a batch from the single point cloud sample and for Masked Autoencoder reconstruction, we randomly mask 90% of the tokens.",1,related,1,positive
"We test this approach on a diverse set of chromosome aberrations (an intra-chromosomal unbalanced abnormality: del(5q); intra-chromosomal balanced rearrangements: inv(3) and inv(16), and inter-chromosomal translocations: t(9;22), t(9;11), and t(11:19)) commonly seen in",1,related,1,positive
"For example, the normal chromosome 9s from the held out pre-training folds were added to the t(9;11) aberration training set for normal vs aberrant chr9 identification and likewise for the remaining aberration datasets.",1,related,0,negative
"Similarly, (D) shows precision-recall for de novo aberration detection based on distance to N-nearest point (here 50th) for t(9;11), t(11;19), del(5q), and t(9;22), respectively.",1,related,1,positive
"In this section, we first briefly introduce Masked Image Modeling (MIM) for image representation learning and then extend it to our video representation learning scheme, Masked Video Modeling (MVM) (¬ß3.1 and ¬ß3.2).",1,related,1,positive
"To resolve this problem, we involve masked autoencoder [36], which applies 50% random masking to the input data.",1,related,1,positive
"Nevertheless, to compare to other pre-training strategies, we consider MAE [29] pre-trained on ImageNet [63], thus with a cosine positional embedding, a ViT-Base encoder, and with a Small decoder that is randomly initialized.",1,related,1,positive
"We observe that CroCo pre-training obtains the lowest errors, significantly outperforming the MAE pre-training and the random initialization.",1,related,0,negative
"Note that these methods, as MAE [29], regress pixel values that are normalized according to the mean and standard deviation inside each patch, we thus apply the inverse transform for
display: this means that the overall color of each patch will be correct, as it comes from the ground-truth values.",1,related,1,positive
The masking implementation follows [19]:,1,related,1,positive
"Different from [19], we reshape xmask into a masked images as input xinput ‚àà RH√óW√óC .",1,related,1,positive
"Specifically, apart from the original optimization target, we exploit the Masked Image Modeling (MIM) [1, 19] task to mask and recover random patches during the training.",1,related,1,positive
"Although we did not achieve the best performance on OSCC and temporal localization tasks in Ego4d Challenge 2022, we believe that, by paying much more
attention to downstream task formulation and optimization, models that are pretrained on egocentric datasets under the settings of VideoMAE will further improve state-of-the-art performance on various Ego4d tasks.",1,related,1,positive
"In this report, we demonstrate that merely pretraining the model on 3rd-person view datasets (e.g. Kinetics 400[7]) under the settings of VideoMAE can achieve state-of-the-art performance on egocentric video understanding tasks including Ego4d object state change classification and Ego4d PNR temporal localization.",1,related,1,positive
"We will show that even with weights obtained on 3rd-person view datasets, VideoMAE shows great generalization ability on egocentric downstream tasks and surpass most existing methods both on OSCC and temporal localization tasks.",1,related,1,positive
"As shown in Table 1 and Table 2, by simply pretraining on Kinetics 400 under the settings of VideoMAE, we ranked 2nd place in both tasks.",1,related,0,negative
"In our experiments, we use ViT as our backbone of which weights are initialized from VideoMAE[9] pretrained on Kinetics-400.",1,related,1,positive
"In order to make this mix strategy compatible with existing pretraining tasks like Masked Image Modeling (MIM) and Image Classification (IC), we split the mask m into patches with p√ó p size.",1,related,1,positive
"Restrictions apply.
auto-encoder [34, 76], global/dense distillation [33, 81] and masked image modeling (MIM) [4, 5, 14, 30, 87].",1,related,1,positive
"For example, p = 16 is by default used for MIM [5,30].",1,related,1,positive
"Given that the supervision on masked patches is a regularization for the learning of CAE v2, we suppose that it may not be appropriate to adopt a high mask ratio (75% in MAE [27], 40%-50% in BEiT [3], CAE [10], and MVP [49]) for all scales of ViTs.",1,related,1,positive
"The encoder F only receives the visible patches Xv following [10, 27].",1,related,0,negative
"Our findings are different from the common sense in the current MIM methods [3,10,27] that only compute the loss on the masked patches, which is inherited from BERT [15] in the NLP areal and has been verified by most current works.",1,related,1,positive
"Following previous MIM methods [3, 10, 27, 48], CAE v2 first embeds x into a total length of N patches, which are then randomly masked by a specific proportion Œ≥.",1,related,1,positive
Results with Masked Autoencoders (MAE).,1,related,0,negative
"As a representative example, we deploy EfficientTrain on top of MAE [22] in Table 10.",1,related,1,positive
"Additionally, to test transferability, we use the adversarially generated samples using PRIME-ResNet50 and FANVIT as classifiers, and test them on other architectures: (1) ResNet50 [17], (2) MAE [16], a generalizable and scalable asymmetric encoder-decoder architecture, (3) RegNet-
Y [14], a ResNet-type model with a regulatory model to extract complementary features, and (4) data2vec [6], a selfsupervised transformer that predicts contextualized latent representations in a self-distillation setup for any modality.",1,related,1,positive
"Additionally, to test transferability, we use the adversarially generated samples using PRIME-ResNet50 and FANVIT as classifiers, and test them on other architectures: (1) ResNet50 [17], (2) MAE [16], a generalizable and scalable asymmetric encoder-decoder architecture, (3) RegNet-",1,related,1,positive
"‚Ä¶we notice that there are various open-sourced image ViTs (Wightman, 2019; Touvron et al., 2021), which have been well-pretrained on huge web datasets under rich supervision such as image-text contrastive learning (Radford et al., 2021) and mask image modeling (He et al., 2022; Bao et al., 2021).",1,related,1,positive
"‚Ä¶of our UniFormerV2 design, we apply it on the ViTs with different pertaining methods, including supervised learning (Dosovitskiy et al., 2021; Touvron et al., 2022), contrastive learning(Caron et al., 2021; Radford et al., 2021) and mask image modeling (He et al., 2022; Bao et al., 2021).",1,related,1,positive
"We only use standard random cropping and horizontal flipping for data augmentation, following MAE [13], CAE [5], etc.",1,related,1,positive
"To restrain the feature magnitudes of teacher features, we generate the alignment target ·ªπ by normalizing each level of teacher features as MAE [13] does on pixel values:",1,related,1,positive
"Self-supervised Learning
We pre-trained MAE following the official implementation9 on eight NVIDIA A-100 GPUs and fine-tuned the last layer of its encoder on 16 NVIDIA V-100 GPUs.",1,related,0,negative
"We pretrain a Vision Transformer model, specifically ViT-B [13],
as MAE‚Äôs encoder for 200 epochs with a mask ratio of 0.75.",1,related,1,positive
"To verify this idea, we adopt a self-supervised learning method called masked autoencoder (MAE) [21].",1,related,1,positive
"Following MAE, we adopt the ViT as the backbone of the decoder g(¬∑).",1,related,1,positive
"In the objective segmentation, MR SimCLR significantly improves APmask over MAE by 0.4 points (46.9 vs. 46.5).",1,related,0,negative
In this manuscript we use a ViT-B8 with the modifications proposed by He et al. (2022) where the classifier is applied after global average pooling over the vision tokens.,1,related,1,positive
"Following [18], we divide the vectorized voxels into patches which will be subsequently transformed into embeddings using a 1D convolutional layer with a stride equal to the patch size.",1,related,1,positive
"We also adopt an asymmetric architecture as in [18]: the encoder is optimized to learn effective fMRI representations, while the decoder tries to predict the masked patches.",1,related,1,positive
"Commonly, the encoder‚Äôs output is averaged, or a cls token is appended to produce a pooled 1D feature vector for downstream tasks [8,18].",1,related,1,positive
"In this work, we benchmark four representative methods MoCo, DINO, MAE, and data2vec on the proposed dataset.",1,related,1,positive
"We find 70% to be the best masking ratio, which is similar to natural images as reported in MAE paper, where 75% is the best.",1,related,0,negative
"This way, we cover a reasonably diverse set of representative methods from each model category: MoCo utilizes contrastive representative, DINO represents a distillation method, MAE is based on masked reconstruction, and data2Vec combines the masking mechanism with a joint-embedding architecture.",1,related,1,positive
We pre-train the MAE models using its default settings following the publicly available repository (https: //github.com/facebookresearch/mae).,1,related,0,negative
"For EO applications we demonstrate SSL4EOS12 to succeed in self-supervised pre-training for a set of methods: MoCo-v2, DINO, MAE, and data2vec.",1,related,1,positive
"In the first stage, we pretrain the model by borrowing the idea from masked autoencoders (MAE) [11].",1,related,1,positive
Our encoder uses the same settings as ViT-B in MAE [11].,1,related,0,negative
"In the pretraining stage, we apply RandomResizedCrop to augment data, which is similar to MAE.",1,related,1,positive
Masked autoencoders (MAE).,1,related,0,negative
"To better use the pretrained knowledge, different from MAE or ViTSTR [2], which only fine-tune on the pretrained encoder, we fine-tune on both the encoder and the decoder.",1,related,1,positive
"To this end, we adopt a two-stage training strategy to train the model as follows:
In the first stage, we pretrain the model by borrowing the idea from masked autoencoders (MAE) [11].",1,related,1,positive
"However, recognizing texts is beyond the scope of MAE, and we propose a novel language-aware model to deal with it, as shown in Figure 1.",1,related,1,positive
"Different from MAE, our MVLT recognizes scene text in addition to reconstructing the masked patches.",1,related,0,negative
"The encoder of MAE is a ViT, which only operates on xu to learn the visual feature embeddings:
vu = encoder(xu), (1)
where vu ‚àà RNu√óD1 and D1 is the feature dimension in the encoder.",1,related,1,positive
"When finetuning the detector on COCO, we find that applying learning rate decay [1,4,5,9] for the components of the detector (encoder and decoder) gives a ‚àº0.",1,related,1,positive
"Then, REGCLR is instantiated by integrating masked autoencoders [12] as a representative example of a contrastive method and enhanced Barlow Twins as a representative example of a regularized method with configurable input image augmentations in both branches.",1,related,1,positive
MAE branch uses weak augmentation and masking to obtain X1 and then follows procedure introduced in MAE [12] to compute reconstructed loss for unmasked patches as LMAE to obtain X ‚Ä≤ 1.,1,related,1,positive
"Our implementation of pretraining is built on the repo provided in MAE [12], and the projector is implemented as a 2 layer multilayer perceptron with 1024 dimensional output.",1,related,1,positive
"For RGMIM and MAE, we employed the same settings in all experiments, except for the masking strategy.",1,related,0,negative
We also studied the masking ratio for RGMIM and MAE using hyperparameters.,1,related,1,positive
"As comparison methods, we used five SOTA selfsupervised learning methods as comparative methods, including masked autoencoder (MAE) [10], self-knowledge distillation based self-supervised learning (SKD) [47], cross-view self-supervised learning (Cross) [48], bootstrap your own latent (BYOL) [20], and simple siamese self-supervised learning (SimSiam) [21].",1,related,1,positive
"As comparison methods, we used five SOTA selfsupervised learning methods as comparative methods, including masked autoencoder (MAE) [10], self-knowledge distillation based self-supervised learning (SKD) [47], cross-view",1,related,1,positive
"In addition, we observe that RGMIM outperforms MAE in terms of robustness, especially when the masking ratio is relatively low, demonstrating the superiority of our proposed method in handling incomplete lung X-ray images.",1,related,0,negative
"Method Architecture 1% 5% 10% 50% 100% RGMIM ViT-Base 0.771 0.893 0.919 0.957 0.962 MAE [10] ViT-Base 0.754 0.875 0.903 0.948 0.956 SKD [47] ResNet-50 0.742 0.812 0.896 0.947 0.957 Cross [48] ResNet-50 0.747 0.795 0.817 0.934 0.953 BYOL [20] ResNet-50 0.683 0.754 0.790 0.933 0.954 SimSiam [21] ResNet-50 0.623 0.700 0.781 0.929 0.949 Transfer ViT-Base 0.689 0.861 0.893 0.940 0.953 Transfer ResNet-50 0.539 0.619 0.665 0.913 0.936 From Scratch ViT-Base 0.413 0.580 0.645 0.810 0.848 From Scratch ResNet-50 0.284 0.496 0.532 0.619 0.774
has layer L = 8, latent vector size D = 512, and the number of heads is 16.",1,related,1,positive
"Although existing Transformer models like ViT [32] and MAE [10] are usually trained on the large-scale dataset, We trained the ST-MAE model on the limited samples from scratch by an AdamW optimizer with a learning rate of 1e-4 and batch size of 8 for 400 training epochs, while the weights",1,related,1,positive
"Note that our ST-MAE is a feature vision Transformer that operates on the deep features of DCNN, which is slightly different from the base ViT [10], its consecutive computational process is roughly demonstrated.",1,related,1,positive
"like the MAE [10] and Intr [36], we adopted the feature-level measurement, the relaxed version of the pixel-level constraints, for better robustness.",1,related,1,positive
"1, in analogy to MAE [10], our ST-MAE has an asymmetric encoder-decoder design that reconstructs the input in feature space, yet our encoder applies a Siamese architecture.",1,related,1,positive
"In our proposed method, MobileNetV3([16]) is adopted as the backbone network.",1,related,1,positive
"In our proposed method, MobileNetV3[16] is adopted as the backbone network.",1,related,1,positive
"1) Black-Box Attack: For the attack model, we adopt a similar structure to the edge model: an MAE decoder [18] is used, and is pretrained on ImageNet.",1,related,1,positive
"We also report results for our MAE implementation, which approximately matches the original numbers reported by He et al. (2022), validating our MAE results on JFT-300M.",1,related,0,negative
For linear probing we list the hyperparameters in Table 10 for which we followed the settings in He et al. (2022).,1,related,1,positive
"For MAE pre-training, we use the same hyperparameters as listed in He et al. (2022), except for the use of Glorot uniform initialization instead of LeCun initialization as done in He et al. (2022).",1,related,1,positive
"Following He et al. (2022), only the T ‚Ä≤ unmasked patches are passed to the ViT encoder, which processes the two views in parallel.",1,related,1,positive
"Corroborating the findings of He et al. (2022), we find it best to only compute the reconstruction loss on masked patches:
Lrec = 1
2n ‚àë v=1,2 n‚àë i=1 ‚ÄñMvi ‚ó¶ (xvi ‚àí xÃÇvi )‚Äñ22
where ‚ó¶ multiplies all pixels in the tth patch of the residual image xvi ‚àí xÃÇvi by (Mvi )t ‚àà {0, 1}.",1,related,1,positive
Decoder architecture: Our decoder architecture is the same as He et al. (2022).,1,related,1,positive
"However, when we apply generative pretrained backbones, e.g. BERT (Kenton and Toutanova, 2019) and MAE (He et al., 2022), to classification tasks in NLP, the linear probing performs poorly because the extracted features are specialized for mask prediction and might not contain enough information for classification (Wallat et al., 2020; Yosinski et al., 2014).",1,related,1,positive
"However, when we apply generative pretrained backbones, e.g. BERT (Kenton and Toutanova, 2019) and MAE (He et al., 2022), to classification tasks in NLP, the linear probing performs poorly because the extracted features are specialized for mask prediction and might not contain enough information‚Ä¶",1,related,1,positive
Our model-based image augmentation method was implemented based on the official codes of the MAE [18] and ViTPose [21].,1,related,0,negative
"To solve the problem of loss and deformation of data acquired by clients, we used a masked image-encoding method that learns image representations corrupted by masking [7, 8].",1,related,1,positive
"Following [13], we apply an end-to-end fine-tuning mechanism instead of linear probing to obtain the highest performance.",1,related,1,positive
"Instead of random initialization, we initialize the ViT encoder using the ImageNet-1k pre-trained encoder released by [13].",1,related,1,positive
"(ii) MAE-IN1k refers to fine-tuned from the ImageNet-1k [14] pre-trained MAE [13], where we use the same fine-tuning settings as that of MAE-Face.",1,related,1,positive
"Among the various techniques proposed for self-supervised visual representation learning, we opt to adopt masked autoencoder (MAE) [13] as our baseline.",1,related,1,positive
"We also test the impact of patch-wise normalization (w/ norm), which has also been studied in [13].",1,related,1,positive
"CONCLUSIONS In this paper, based on MAE[16] and combined with convolution, we propose a transformer image inpainting model to solve the image inpainting problem of irregular missing areas.",1,related,1,positive
"In this paper, based on MAE, we propose a transformerbased inpainting model for irregular missing images.",1,related,1,positive
"By utilizing MAE pretraining, ProContEXT outperforms the recent SOTA method, OStrack [5], by 0.9%, 1.5%, and 2.1% for AO, SR0.",1,related,0,negative
We implement the proposed ProContEXT using PyTorch and utilize the ViT-base backbone that is pre-trained by MAE [28].,1,related,1,positive
Implementation Details We implement the proposed ProContEXT using PyTorch and utilize the ViT-base backbone that is pre-trained by MAE [28].,1,related,1,positive
"As mentioned in Section 1, we follow a similar approach to the work put forth in [7], where masked autoencoders reconstructed images with large amounts of masking.",1,related,1,positive
"Unlike [7], we don‚Äôt remove the masked patches from the input to the encoder and provide positional information to the input of the decoder.",1,related,1,positive
"We denote the former as ‚Äúspeech branch‚Äù and the latter as ‚Äútext branch‚Äù, where the former can be viewed as a masked autoencoder [28] and the latter is TTS.",1,related,1,positive
"Specifically, we propose an SSL model, MAEEG, inspired by both BENDR and the MAE model proposed in computer vision [He et al., 2022], to compare with BENDR and to broaden our understanding of reconstruction-based SSL for EEG.",1,related,1,positive
This study was inspired by MAE [1] for an MIM and Bootstrap Your Own Latent [12] (BYOL) as a framework for directly learning la-,1,related,1,positive
"While we use the same positional encoding as MAE [1], we tested various masking ratios as discussed in Section 4.",1,related,0,negative
"In this work, we capitalize in particular on the masked autoencoding of [2] and investigate using it for object-centric learning.",1,related,1,positive
"Then, exactly as in MAE [2], we add a learnable mask token at the positions of the masked tokens and the sine-cosine position embeddings.",1,related,1,positive
"We therefore choose the random masking strategy, exactly as in MAE [2].",1,related,1,positive
"Just as in MAE [2], we add fixed sine-cosine postional encodings from [23] to the embeddings of the patches.",1,related,1,positive
Our model has a narrower bottleneck in comparison to MAE [2].,1,related,0,negative
Our model is inspired by the transformer-based masked autoencoder [2] with modifications to enable end-to-end unsupervised multi-object segmentation and representation learning.,1,related,1,positive
"More specifically, we adapt the Masked Autoencoder (MAE) [2] design and modify it to for explicit object-centric representation learning and segmentation.",1,related,1,positive
"Similarly, while ViT has been shown to outperform CNNs on large datasets [17], we observe that ViT outputs contain uneven pixelation when decoding the 16 √ó 16 patch embeddings, resulting in its low SSIM score.",1,related,1,positive
"For MAE He et al. (2022), a method based on a reconstruction objective, we select an attention-based ViT encoder.",1,related,1,positive
"0 0.5 1
‚àí0.6
‚àí0.4
‚àí0.2
0
0 0.5 1 0 0.5 1
0 0.5 1
‚àí0.6
‚àí0.4
‚àí0.2
0 0.5 1
CLIPPretrained MAEPretrained MLPMixerPretrained1k MLPMixerPretrained21k ResNet50Pretrained1k ResNet50Pretrained21k SimCLRPretrained ViTPretrained1k ViTPretrained21k iBotPretrained1k iBotPretrained21k
Generalization when Lighting varies in training
percent of diverse Lighting training samples percent of diverse Lighting training samples percent of diverse Lighting training samples
percent of diverse Lighting training samples percent of diverse Lighting training samples
t o p - 1
a c c u r a c y d
r o p
t o p - 1
a c c u r a c y d
r o p",1,related,1,positive
"In addition to the finetning results, we include here linear evaluation results for class generalization gaps A11.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Translation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 92.24% 91.37% 92.25% 94.01% 96.33% 77.78% 75.93% 68.52% 68.52% 58.89% 39.82% 57.70% 64.51% 65.45% 66.98% MAEPretrained 52.54% 55.93% 60.44% 67.91% 83.79% 20.37% 27.78% 33.33% 38.89% 27.78% 9.67% 12.91% 14.26% 15.56% 15.18% MLPMixerPretrained1k 94.66% 93.98% 94.58% 95.75% 97.25% 77.78% 74.07% 72.22% 66.67% 48.15% 36.86% 53.70% 59.35% 63.05% 63.46% MLPMixerPretrained21k 94.95% 95.09% 95.19% 96.02% 97.13% 75.93% 75.93% 74.07% 77.78% 70.37% 43.89% 67.36% 71.25% 73.69% 76.18% ResNet50Pretrained1k 95.00% 94.58% 94.83% 95.79% 97.23% 88.89% 90.74% 87.04% 83.33% 70.37% 44.35% 63.26% 68.99% 70.04% 70.01% ResNet50Pretrained21k 95.51% 95.30% 95.90% 96.27% 97.39% 77.78% 72.22% 74.07% 74.07% 70.37% 46.61% 68.04% 73.02% 75.90% 77.13% SimCLRPretrained 96.13% 95.74% 96.22% 96.82% 97.50% 81.48% 79.63% 81.48% 72.22% 70.00% 43.73% 62.96% 69.10% 70.63% 72.02% ViTPretrained1k 95.79% 96.18% 96.37% 96.80% 97.75% 88.89% 83.33% 77.78% 77.41% 75.93% 49.34% 67.92% 72.74% 75.65% 77.22% ViTPretrained21k 95.43% 95.01% 95.62% 96.39% 97.50% 83.33% 83.33% 83.33% 77.78% 72.22% 46.59% 67.21% 71.71% 74.02% 75.17% iBotPretrained1k 96.67% 96.43% 96.49% 97.01% 97.66% 81.48% 81.48% 79.63% 79.63% 72.22% 40.27% 65.23% 73.10% 76.06% 77.33% iBotPretrained21k 96.84% 96.30% 96.44% 96.92% 97.61% 90.74% 85.19% 87.04% 81.48% 72.22% 47.69% 70.55% 76.57% 78.53% 79.04%
Table A1: Position varying linear eval top-1 accuracy across multiple percentages of varying training instances.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Rotation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 91.70% 91.43% 92.42% 93.98% 96.28% 81.48% 85.19% 85.19% 79.63% 55.56% 28.64% 41.16% 44.72% 46.17% 48.85% MAEPretrained 53.90% 57.24% 61.28% 68.44% 83.90% 25.93% 44.44% 38.89% 42.59% 29.63% 6.68% 7.93% 8.58% 8.63% 8.36% MLPMixerPretrained1k 94.24% 93.76% 94.74% 95.72% 97.28% 74.07% 74.07% 72.22% 70.37% 46.30% 26.84% 39.47% 43.04% 46.51% 48.73% MLPMixerPretrained21k 94.49% 94.81% 95.03% 95.88% 97.13% 79.63% 77.78% 77.78% 79.63% 72.22% 36.90% 55.46% 61.57% 63.67% 65.50% ResNet50Pretrained1k 94.72% 94.56% 94.89% 95.74% 97.18% 85.19% 87.04% 85.19% 81.48% 68.52% 34.44% 46.22% 49.90% 51.73% 53.24% ResNet50Pretrained21k 95.51% 94.96% 95.69% 96.12% 97.30% 77.78% 75.93% 75.93% 72.22% 66.67% 40.29% 57.68% 61.83% 63.91% 65.04% SimCLRPretrained 95.74% 95.63% 96.16% 96.80% 97.51% 81.48% 83.33% 83.33% 83.33% 62.96% 32.94% 47.35% 52.88% 55.97% 56.91% ViTPretrained1k 95.71% 95.76% 96.09% 96.79% 97.67% 87.04% 79.63% 81.48% 79.63% 59.26% 39.13% 55.09% 60.14% 64.14% 65.42% ViTPretrained21k 95.23% 94.89% 95.68% 96.42% 97.45% 85.19% 83.33% 83.33% 83.33% 66.67% 38.25% 54.50% 58.34% 60.94% 62.28% iBotPretrained1k 96.39% 96.22% 96.41% 96.96% 97.56% 83.33% 81.48% 81.48% 79.63% 72.22% 34.62% 51.88% 58.88% 62.19% 63.11% iBotPretrained21k 96.44% 96.09% 96.42% 96.92% 97.61% 90.74% 88.89% 90.74% 87.04% 72.22% 41.03% 57.48% 63.69% 65.49% 67.34%
Table A2: Pose linear eval top-1 accuracy across multiple percentages of varying training instances.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Spot hue_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 92.49% 91.60% 92.46% 94.06% 96.32% 77.78% 75.93% 70.37% 70.37% 59.26% 39.61% 60.78% 66.72% 68.27% 68.64% MAEPretrained 52.26% 55.43% 60.36% 67.78% 83.62% 22.22% 31.48% 37.04% 37.04% 20.37% 11.51% 15.09% 16.10% 18.91% 15.58% MLPMixerPretrained1k 95.17% 94.20% 94.98% 95.86% 97.30% 79.63% 77.78% 70.37% 66.67% 57.04% 40.09% 56.90% 64.43% 67.35% 68.93% MLPMixerPretrained21k 94.89% 95.29% 95.44% 96.13% 97.20% 81.48% 79.63% 72.22% 74.07% 76.30% 44.68% 69.30% 73.32% 76.12% 77.75% ResNet50Pretrained1k 95.26% 94.81% 95.03% 95.80% 97.23% 87.04% 88.89% 87.04% 83.33% 77.78% 44.08% 62.96% 68.67% 71.81% 72.54% ResNet50Pretrained21k 95.91% 95.45% 96.00% 96.28% 97.41% 77.78% 75.93% 75.93% 72.22% 71.11% 44.22% 67.22% 70.95% 72.38% 74.39% SimCLRPretrained 96.30% 95.91% 96.27% 96.84% 97.59% 79.63% 75.93% 74.07% 74.07% 66.67% 43.36% 63.22% 71.32% 73.72% 72.26% ViTPretrained1k 95.99% 96.36% 96.54% 96.95% 97.80% 90.74% 87.04% 83.33% 81.48% 74.07% 52.53% 69.53% 71.81% 76.01% 77.28% ViTPretrained21k 95.57% 95.39% 95.84% 96.51% 97.59% 85.19% 81.48% 83.33% 77.78% 75.93% 46.01% 67.50% 71.97% 75.75% 77.06% iBotPretrained1k 96.50% 96.55% 96.74% 97.12% 97.70% 79.63% 81.48% 81.48% 77.78% 74.07% 42.32% 68.61% 72.75% 76.28% 78.04% iBotPretrained21k 97.01% 96.42% 96.65% 97.04% 97.64% 88.89% 87.04% 83.33% 83.33% 75.93% 48.83% 73.37% 78.09% 80.39% 81.55%
Table A3: Spot hue linear eval top-1 accuracy across multiple percentages of varying training instances.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Scale_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 91.81% 91.51% 92.26% 93.90% 96.25% 79.63% 72.22% 74.07% 70.37% 61.11% 36.80% 51.29% 57.05% 56.99% 58.80% MAEPretrained 51.98% 56.15% 60.87% 68.07% 84.12% 25.93% 35.19% 33.33% 37.04% 35.19% 7.01% 10.74% 11.05% 11.29% 11.44% MLPMixerPretrained1k 94.27% 93.55% 94.47% 95.59% 97.17% 79.63% 77.78% 70.37% 68.52% 53.70% 32.50% 46.15% 51.66% 55.30% 56.62% MLPMixerPretrained21k 94.86% 94.98% 95.08% 95.93% 97.09% 79.63% 79.63% 75.93% 75.93% 74.07% 40.55% 60.87% 66.63% 67.52% 70.03% ResNet50Pretrained1k 94.89% 94.57% 94.84% 95.66% 97.16% 87.04% 90.74% 90.74% 83.33% 72.22% 39.22% 54.23% 59.10% 61.27% 60.89% ResNet50Pretrained21k 95.51% 95.28% 95.77% 96.08% 97.35% 81.48% 77.78% 77.78% 74.07% 74.07% 41.11% 60.06% 66.41% 69.69% 70.33% SimCLRPretrained 95.91% 95.53% 96.09% 96.66% 97.48% 83.33% 77.41% 77.78% 72.22% 62.96% 39.79% 54.93% 61.81% 62.93% 62.96% ViTPretrained1k 95.65% 96.01% 96.18% 96.69% 97.69% 87.04% 83.33% 79.63% 75.93% 72.22% 44.10% 58.13% 63.91% 67.85% 68.82% ViTPretrained21k 95.06% 94.87% 95.47% 96.30% 97.38% 83.33% 83.33% 83.33% 81.48% 72.22% 41.40% 58.53% 63.25% 65.43% 65.14% iBotPretrained1k 96.58% 96.37% 96.48% 96.98% 97.60% 84.07% 77.78% 79.63% 77.78% 66.67% 41.34% 58.93% 67.24% 68.66% 69.08% iBotPretrained21k 96.92% 96.18% 96.39% 96.86% 97.53% 85.19% 77.78% 81.48% 81.48% 75.93% 44.59% 63.11% 68.90% 71.45% 70.82%
Table A4: Scale linear eval top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Background path_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 90.37% 91.88% 92.73% 94.36% 96.70% 85.19% 77.78% 77.78% 77.78% 66.67% 41.30% 54.70% 59.96% 61.75% 63.41% MAEPretrained 51.51% 56.16% 62.31% 67.08% 84.32% 27.78% 31.48% 37.04% 37.04% 29.63% 9.47% 12.21% 11.84% 13.30% 12.67% MLPMixerPretrained1k 92.84% 93.82% 94.81% 95.41% 97.38% 75.93% 77.78% 72.22% 74.07% 61.11% 37.39% 49.19% 52.41% 55.75% 56.74% MLPMixerPretrained21k 95.42% 95.33% 95.62% 96.52% 97.55% 83.33% 81.48% 75.93% 75.93% 77.78% 48.30% 64.37% 66.79% 70.15% 70.84% ResNet50Pretrained1k 94.35% 95.12% 94.83% 95.93% 97.46% 90.74% 92.59% 88.89% 88.89% 83.33% 44.89% 59.87% 64.15% 66.70% 67.29% ResNet50Pretrained21k 95.20% 96.40% 95.89% 96.65% 97.67% 81.48% 79.63% 77.78% 77.78% 75.93% 51.21% 65.75% 69.47% 72.01% 73.90% SimCLRPretrained 95.50% 95.98% 96.14% 96.45% 97.52% 83.33% 83.33% 81.48% 77.78% 72.22% 44.33% 59.82% 65.39% 67.93% 68.93% ViTPretrained1k 95.72% 96.42% 96.27% 96.57% 97.95% 94.44% 88.89% 88.89% 87.04% 77.78% 50.62% 64.09% 67.14% 72.33% 73.19% ViTPretrained21k 94.91% 95.22% 96.09% 96.45% 97.71% 83.33% 83.33% 83.33% 83.33% 77.78% 49.36% 64.21% 67.42% 70.77% 72.13% iBotPretrained1k 96.42% 96.58% 96.60% 97.40% 97.28% 88.89% 83.33% 87.04% 81.48% 79.63% 44.58% 62.59% 68.10% 71.20% 73.36% iBotPretrained21k 96.16% 96.14% 96.38% 97.37% 97.27% 88.89% 90.74% 90.74% 83.33% 81.48% 51.20% 68.58% 71.57% 74.62% 75.94%
Table A5: Background path linear eval top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Translation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.49% 97.00% 97.25% 97.55% 96.80% 87.04% 90.74% 77.78% 81.48% 75.93% 45.33% 69.74% 74.53% 77.72% 78.19% MAEPretrained 96.75% 96.63% 97.20% 97.46% 97.91% 83.33% 74.07% 66.67% 64.81% 72.22% 29.17% 51.35% 61.05% 65.16% 70.07% MLPMixerPretrained1k 96.95% 96.73% 97.17% 97.24% 97.88% 88.89% 77.78% 77.78% 74.07% 64.81% 44.65% 64.56% 70.67% 74.34% 75.95% MLPMixerPretrained21k 97.71% 97.69% 97.90% 97.98% 98.35% 85.19% 88.89% 85.19% 81.48% 77.78% 46.53% 70.54% 77.27% 79.78% 81.68% ResNet50Pretrained1k 97.97% 97.92% 97.91% 97.86% 98.27% 87.04% 87.04% 72.22% 81.48% 81.48% 45.05% 64.83% 71.87% 76.56% 79.63% ResNet50Pretrained21k 97.54% 97.62% 97.74% 97.69% 98.20% 88.89% 83.33% 83.33% 83.33% 77.78% 52.22% 70.96% 75.78% 81.10% 82.45% SimCLRPretrained 97.40% 97.57% 97.68% 97.81% 98.12% 90.74% 77.78% 87.04% 83.33% 77.78% 45.21% 68.73% 74.59% 76.55% 79.86% ViTPretrained1k 97.80% 97.88% 98.00% 97.92% 98.28% 90.74% 90.74% 85.19% 81.48% 81.48% 49.30% 71.82% 76.95% 80.39% 82.58% ViTPretrained21k 97.80% 97.59% 97.89% 97.91% 98.25% 87.04% 88.89% 81.48% 81.48% 87.04% 45.83% 71.80% 75.51% 79.96% 81.86% iBotPretrained1k 97.77% 97.55% 97.64% 97.77% 98.06% 88.89% 85.19% 79.63% 75.93% 79.63% 45.69% 68.94% 74.76% 76.63% 79.83% iBotPretrained21k 97.97% 97.83% 97.88% 97.92% 98.13% 88.89% 87.04% 88.89% 81.48% 79.63% 49.84% 70.97% 78.13% 82.53% 84.07%
Table A6: Position finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Rotation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.60% 97.01% 97.30% 97.58% 97.97% 88.89% 88.89% 85.19% 83.33% 72.22% 34.29% 57.01% 62.67% 65.58% 68.45% MAEPretrained 96.87% 96.75% 97.25% 97.52% 97.95% 75.93% 68.52% 62.96% 68.52% 62.96% 22.64% 45.24% 50.87% 54.18% 53.82% MLPMixerPretrained1k 96.75% 96.58% 97.08% 97.25% 97.86% 83.33% 85.19% 83.33% 77.78% 64.81% 33.29% 51.43% 55.55% 58.78% 59.35% MLPMixerPretrained21k 97.68% 97.65% 97.90% 97.90% 98.35% 87.04% 87.04% 77.78% 77.78% 75.93% 38.93% 62.76% 67.97% 71.90% 73.47% ResNet50Pretrained1k 98.05% 97.81% 97.89% 97.93% 98.24% 84.81% 81.48% 81.48% 81.48% 75.93% 33.29% 53.51% 62.56% 64.45% 67.47% ResNet50Pretrained21k 97.52% 97.45% 97.65% 97.61% 98.18% 85.19% 79.63% 83.33% 87.04% 85.19% 37.45% 59.85% 65.39% 70.66% 73.13% SimCLRPretrained 97.37% 97.43% 97.53% 97.74% 98.07% 87.04% 87.04% 83.33% 87.04% 75.93% 35.50% 55.87% 64.34% 66.79% 69.34% ViTPretrained1k 97.94% 97.87% 97.98% 97.95% 98.31% 88.89% 87.04% 85.19% 77.78% 75.93% 40.13% 62.66% 68.53% 71.32% 73.36% ViTPretrained21k 97.68% 97.61% 97.90% 97.97% 98.27% 88.89% 79.63% 83.33% 74.07% 70.74% 39.75% 61.42% 68.39% 71.51% 73.76% iBotPretrained1k 97.49% 97.55% 97.56% 97.75% 98.02% 88.89% 77.78% 83.33% 75.93% 68.52% 36.30% 60.69% 65.98% 68.22% 70.00% iBotPretrained21k 98.00% 97.79% 97.96% 98.01% 98.19% 88.89% 87.04% 83.33% 85.19% 72.22% 38.86% 62.35% 69.18% 71.96% 73.84%
Table A7: Pose finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Spot hue_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.66% 97.13% 97.40% 97.62% 97.98% 90.74% 88.89% 87.04% 87.04% 81.48% 49.99% 70.06% 75.12% 82.36% 82.48% MAEPretrained 97.04% 96.67% 97.20% 97.41% 97.95% 79.63% 77.78% 72.22% 74.07% 68.52% 30.10% 54.63% 60.74% 66.78% 69.34% MLPMixerPretrained1k 97.32% 96.92% 97.20% 97.36% 97.89% 87.04% 83.33% 77.78% 79.63% 72.22% 48.80% 65.60% 70.82% 76.88% 78.59% MLPMixerPretrained21k 97.80% 97.83% 97.99% 97.99% 98.42% 88.89% 87.04% 81.48% 75.93% 79.63% 49.81% 73.15% 75.23% 79.01% 82.35% ResNet50Pretrained1k 98.02% 97.99% 98.03% 97.98% 98.28% 88.89% 87.04% 83.33% 81.48% 77.78% 48.48% 67.72% 74.65% 76.90% 75.83% ResNet50Pretrained21k 97.68% 97.60% 97.87% 97.79% 98.24% 90.74% 87.04% 83.33% 77.78% 75.93% 54.35% 71.69% 76.70% 81.03% 81.86% SimCLRPretrained 97.60% 97.61% 97.72% 97.87% 98.17% 90.00% 76.30% 85.19% 77.78% 74.07% 45.58% 70.36% 79.09% 78.06% 81.75% ViTPretrained1k 97.68% 97.93% NaN 98.00% 98.37% 94.44% 88.89% NaN 81.48% 81.48% 54.38% 70.94% NaN 82.53% 83.94% ViTPretrained21k 97.77% 97.68% 97.94% 98.00% 98.24% 92.59% 85.19% 77.78% 78.15% 81.48% 52.49% 72.55% 76.58% 79.75% 82.39% iBotPretrained1k 97.91% 97.58% 97.76% 97.88% 98.08% 88.89% 81.48% 79.63% 75.93% 74.07% 48.67% 69.24% 75.01% 79.44% 80.81% iBotPretrained21k 98.25% 97.87% 97.88% 97.96% 98.13% 90.74% 83.33% 92.59% 79.63% 83.33% 49.39% 74.42% 80.67% 83.05% 85.02%
Table A8: Spot hue finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Scale_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.68% 97.08% 97.39% 97.63% 97.95% 90.74% 90.74% 87.04% 83.33% 79.63% 45.47% 66.39% 72.46% 75.63% 78.05% MAEPretrained 96.81% 96.64% 97.19% 97.40% 97.91% 81.48% 70.37% 70.37% 70.37% 53.70% 28.70% 51.73% 60.66% 62.49% 64.47% MLPMixerPretrained1k 97.23% 96.60% 97.07% 97.24% 97.82% 87.04% 85.19% 83.33% 74.07% 70.37% 41.62% 60.01% 65.78% 70.24% 71.64% MLPMixerPretrained21k 97.80% 97.80% 97.96% 97.95% 98.37% 85.19% 87.04% 81.48% 72.22% 77.78% 46.02% 68.37% 73.08% 76.51% 77.38% ResNet50Pretrained1k 97.88% 97.94% 97.95% 97.89% 98.24% 87.04% 85.19% 81.48% 75.93% 79.63% 44.47% 62.86% 71.55% 73.81% 75.80% ResNet50Pretrained21k 97.40% 97.58% 97.84% 97.72% 98.18% 90.74% 79.63% 81.48% 79.63% 79.63% 49.37% 68.72% 70.89% 76.85% 79.18% SimCLRPretrained 97.57% 97.54% 97.65% 97.83% 98.10% 88.89% 83.33% 83.33% 83.33% 74.44% 42.25% 65.04% 71.88% 74.89% 76.25% ViTPretrained1k 97.80% 97.92% 98.05% 97.92% 98.34% 88.89% 83.33% 85.19% 79.63% 79.63% 44.17% 65.86% 71.66% 77.46% 78.46% ViTPretrained21k 97.77% 97.71% 97.85% 97.99% 98.24% 85.19% 85.19% 85.19% 79.63% 79.63% 42.63% 67.71% 70.61% 74.96% 76.14% iBotPretrained1k 97.85% 97.61% 97.74% 97.79% 98.04% 90.74% 87.04% 83.33% 83.33% 79.63% 45.14% 64.09% 71.34% 73.90% 76.99% iBotPretrained21k 97.88% 97.75% 97.86% 97.97% 98.12% 88.89% 87.04% 87.04% 81.48% 75.93% 48.70% 65.52% 72.39% 79.16% 80.04%
Table A9: Scale finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Background path_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 96.49% 97.05% 97.52% 97.81% 98.01% 94.44% 88.89% 92.59% 87.04% 81.48% 50.79% 67.24% 76.25% 79.79% 80.92% MAEPretrained 96.20% 96.61% 97.31% 97.63% 97.99% 81.85% 72.22% 77.78% 72.22% 62.96% 30.49% 52.04% 64.77% 66.67% 68.56% MLPMixerPretrained1k 96.16% 97.03% 97.13% 97.66% 97.76% 90.74% 87.04% 81.48% 75.93% 72.22% 47.41% 63.84% 71.85% 73.76% 75.96% MLPMixerPretrained21k 98.08% 98.15% 98.09% 98.33% 98.71% 88.89% 92.59% 90.74% 85.19% 79.63% 51.84% 72.00% 77.05% 80.46% 81.10% ResNet50Pretrained1k 97.71% 97.76% 97.95% 98.04% 98.38% 92.59% 92.59% 90.74% 92.59% 83.33% 46.50% 63.48% 70.90% 73.73% 77.72% ResNet50Pretrained21k 97.38% 98.09% 97.72% 98.33% 98.34% 88.89% 83.33% 87.04% 81.48% 85.19% 50.87% 71.59% 76.21% 79.13% 83.24% SimCLRPretrained 97.38% 97.31% 97.77% 97.55% 98.05% 77.78% 87.04% 88.89% 90.74% 83.33% 43.32% 61.47% 69.94% 76.99% 79.07% ViTPretrained1k 98.12% 97.76% 97.89% 97.73% 98.44% 88.89% 90.74% 87.04% 81.48% 83.33% 54.25% 71.56% 75.73% 80.58% 84.92% ViTPretrained21k 97.77% 97.49% 97.95% 98.04% 98.37% 91.67% 88.89% 90.74% 87.04% 81.48% 55.17% 73.53% 76.50% 82.28% 81.94% iBotPretrained1k 97.47% 97.44% 97.61% 98.15% 97.86% 88.89% 92.59% 90.74% 81.48% 79.63% 51.19% 71.17% 75.32% 78.37% 79.93% iBotPretrained21k 97.69% 97.91% 97.77% 98.17% 97.93% 93.52% 92.59% 90.74% 85.19% 83.33% 55.00% 73.11% 76.62% 83.56% 82.90%
Table A10: Background path finetuning top-1 accuracy across multiple percentages of varying training instances",1,related,1,positive
"0 0.5 1
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0 0.5 1 0 0.5 1
0 0.5 1
‚àí0.6
‚àí0.4
‚àí0.2
0 0.5 1
CLIPPretrained MAEPretrained MLPMixerPretrained1k MLPMixerPretrained21k ResNet50Pretrained1k ResNet50Pretrained21k SimCLRPretrained ViTPretrained1k ViTPretrained21k iBotPretrained1k iBotPretrained21k
Generalization when Lighting varies in training
percent of diverse Lighting training samples percent of diverse Lighting training samples percent of diverse Lighting training samples
percent of diverse Lighting training samples percent of diverse Lighting training samples
t o p - 1
a c c u r a c y d
r o p
t o p - 1
a c c u r a c y d
r o p",1,related,1,positive
"0 50 100 ‚àí40
‚àí20
0 50 100 0 50 100 0 50 100 0 50 100
CLIP MAE MLPMixer1k MLPMixer21k ResNet50-1k ResNet50-21k SimCLR ViT-1k ViT-21k iBot-1k iBot-21k best fit
Percent of varying images across all instances
0 50 100 ‚àí40
‚àí20
0
0 50 100 0 50 100 0 50 100 0 50 100",1,related,1,positive
"Using the typical evaluation procedures of self-supervised models (Caron et al., 2021; Dosovitskiy et al., 2021; Chen et al., 2020b) (finetuning and linear evaluation), we examine robustness across a catalog of state-of-the-art vision architectures, such as CLIP (Radford et al., 2021) that have significantly outperformed earlier models on robustness benchmarks such as ObjectNet (Barbu et al., 2019), Masked AutoEncoders (MAE, (He et al., 2022)), or ViTs Dosovitskiy et al. (2021)) among others.",1,related,1,positive
We use pre-trained weights from the official repo of He et al. (2022).,1,related,0,negative
"In our experiment we deploy our Lie operator on top of the Variance-Invariance-Covariance Regularization model (VICReg, (Bardes et al., 2021)), and of the very recent masked autoencoders model (MAE, (He et al., 2021)).",1,related,1,positive
"We incorporate our proposed Lie operator in two recent SSL approaches: masked autoencoders (MAE, (He et al., 2021)), and Variance-Invariance-Covariance",1,related,1,positive
"We incorporate our proposed Lie operator in two recent SSL approaches: masked autoencoders (MAE, (He et al., 2021)), and Variance-Invariance-Covariance
Evaluate Robustness
Self-Supervised Lie Operator
Training Data
Regularization (VICReg (Bardes et al., 2021)) to directly model transformations in‚Ä¶",1,related,1,positive
"In addition to the differentiation of the network structure pre-trained on ImageNet, we include models with a variety of pre-trained strategies, including SimCLR [6], MoCov2 [8] and
1https://pytorch.org/vision/stable/index.html 2https://github.com/rwightman/pytorch-image-models 3https://github.com/open-mmlab
BYOL [21] for ResNet50, MoCov3 [9] and MAE [26] for ViT-B.",1,related,1,positive
The most popular pretraining scheme for ViTs is called Masked Autoencoders (MAE) [45].,1,related,0,negative
"Following this spirit, we take masked autoencoders (MAE) [45] as pre-training task for its simplicity, efficiency, scalability, and compelling performance.",1,related,0,negative
"where LMIM is the MIM loss such as the prediction error used in the MAE [35] when reconstructing missing matches, p[u,v ] is the relative positional embedding with a 2D index of [u, v], g(xi ‚àí x j + Œ¥x ) and g(yi ‚àí y j + Œ¥y) are the continuous indexing function for x‚àí and y‚àícoordinate respectively, andM is the index of masked patches.",1,related,1,positive
"Publication date: October 2022.
where LùëÄùêºùëÄ is the MIM loss such as the prediction error used in the MAE [35] when reconstructing missing matches, p[ùë¢,ùë£ ] is the relative positional embedding with a 2D index of [ùë¢, ùë£], ùëî(ùë•ùëñ ‚àí ùë• ùëó + ùõøùë• ) and ùëî(ùë¶ùëñ ‚àí ùë¶ ùëó + ùõøùë¶) are the continuous indexing function for ùë•‚àí and ùë¶‚àícoordinate respectively, andM is the index of masked patches.",1,related,1,positive
"Then one adopts a Masked Auto-Encoder (MAE) [35] reconstruction error
Lùëöùëéùëí (xm, x;ùúÉ ) = D(IùúÉ (xm), x) with a distance measure D between reconstructed and original images to pre-train the network ùúÉ .",1,related,1,positive
Then one adopts a Masked Auto-Encoder (MAE) [35] reconstruction error,1,related,1,positive
"‚Ä¶al. (2022) requires access to an unlabeled public dataset for all models to measure the distance, and requires the communication of some datasets; He et al. (2021a) experiments with multiple methods to do personalization in decentralized learning, and our clustering-based approach can be viewed‚Ä¶",1,related,1,positive
"For masked autoencoder (He et al., 2021b) experiment on CIFAR-10, we use Vit-Tiny (Dosovitskiy et al., 2020) with AdamW optimizer for 1000 epochs with batch size 256.",1,related,1,positive
"Hence, a high masking ratio (75% (He et al., 2022)) may not be necessary to suppress the amount of information the encoder sees, and we consequently attempt lower ratios of 50%, 60% to introduce more information about the subordinate target.",1,related,0,negative
"In this work, we focus on MAE (He et al., 2022), which proposes to use a high masking ratio and a non-arbitrary ViT decoder.",1,related,1,positive
"We notice that recent work in the literature (He et al., 2022; Bao et al., 2022) performs many experiments in masking strategies, but to the best of our knowledge, we are the first to introduce image mixtures in the pre-training of MIM.",1,related,1,positive
"For better classification performance, we use normalized pixels (He et al., 2022) and a high masking ratio (0.75); for better visual reconstructions, we use a lower masking ratio (0.5) without normalizing target pixels.",1,related,1,positive
"Linear Probing: For linear evaluation, we follow MAE (He et al., 2022) to train with no extra augmentations and use zero weight decay.",1,related,1,positive
"Instead, our patch-dim normalization stresses the relations among patches, which is compatible to the patch-prediction in the MIM scheme.",1,related,1,positive
"We therefore choose a mask-reconstruction [19] SSL scheme for the pretraining of the new model, in which the base model generates reconstruction targets from the full input images and the new model tries to predict base model targets from random masked image input.",1,related,1,positive
We then propose to utilize the self-attention maps as a type of reconstruction targets for MIM to further enhance the semantic relation modeling capability of the new model.,1,related,1,positive
"With the plain backbone pretrained as a MAE [21], our method achieves 4.",1,related,0,negative
"Instead, we simply use the readily available pretrained MAE weights from [21].",1,related,0,negative
"‚Ä¢ We show that block-wise masking provides superior performance on Masked Siamese ConvNets to the discrete random masking, commonly used in selfsupervised representation learning frameworks [20, 24, 1].",1,related,1,positive
"Furthermore, instead of using discrete random masking that is applied in [20], we found using block-wise masking will give better performance since it can better preserve global information important for contrastive loss.",1,related,1,positive
"1) Evalutation protocols: Following previous works [14], [28], [31], we adopt three common evaluation protocols, namely fine-tuning evaluation, linear evaluation and k-NN classification [43], to assess the performance of each pretrained model.",1,related,1,positive
"We also collect 7 ViT-S models with different training regimes, including the original ViT training setup [26]6, a stronger data augmentation setup in the Deit paper [89]-36, the training setup with distillation [89]-36, an improved DeiT training setup [90]-36 , and selfsupervised training fashions by MoCo v3 [12]7, MAE [38]8 and BYOL [33]9.",1,related,0,negative
"Following MAE (He et al., 2022) and BEiT v2 (Peng et al., 2022), we test the robustness of MaskDistill on three ImageNet validation sets, i.e., ImageNet-Adversarial (Hendrycks et al., 2021b), ImageNet-Rendition (Hendrycks et al., 2021a) and ImageNet-Sketch (Wang et al., 2019).",1,related,1,positive
We compare between DINO [5] and MAE [11] as a representation for a kNN based anomaly detection algorithm.,1,related,1,positive
"For MAE, we experimented both with kNN and reconstruction error for anomaly scoring and found that the latter works badly, therefore we report just the kNN results.",1,related,0,negative
"A.1 Anomaly detection comparison of MAE and DINO
We compare between DINO [5] and MAE [11] as a representation for a kNN based anomaly detection algorithm.",1,related,1,positive
"In particular, we compare to DINO [14], a state-of-the-art self-supervised method based on instance discrimination, and to MIM methods with MAE [38] and MultiMAE [4].",1,related,1,positive
"We follow the exact same protocol as MAE [38] for that, with global average pooling for CroCo as we did not include a [CLS] token in our model.",1,related,1,positive
"Our model is trained using a simple pixel reconstruction loss over all masked patches, similar to MAE [38].",1,related,1,positive
"Therefore, we used ArcFace [9] to train a projection head composed of two layers, BatchNormalization and Linear. this architecture is based on MAE [13].",1,related,1,positive
"In order to increase robustness to such varying resolution, we utilize up to 2‚á• higher resolution images during training but randomly drop 80% of visual tokens to minimize additional compute overhead (similar to [38, 52]).",1,related,1,positive
"Table 1: Token Merging ablation experiments using ViT-L/16 from MAE (He et al., 2022) on ImageNet-1k evaluated off-the-shelf without training, using r = 8.",1,related,0,negative
"We perform several experiments on ImageNet-1k (Deng et al., 2009) using ViT models trained in four different ways: AugReg (Steiner et al., 2022), MAE (He et al., 2022), SWAG (Singh et al., 2022), and DeiT (Touvron et al., 2021).",1,related,1,positive
"Different from MAE [16], our encoder operates on the full set.",1,related,0,negative
"The experiments reveal that MAE and U-Net are the best shape denoising methods we evaluated for all six
types of noise.",1,related,1,positive
"We evaluate seven shape denoising methods (ASM [6], DBM [8], CDBM [9], EBM [10], U-Net [11], Deeplabv3+ [12], MAE [13]) for denoising shapes corrupted by the six types of noise introduced in Section 2.",1,related,1,positive
"We evaluate seven shape denoising methods (ASM [6], DBM [8], CDBM [9], EBM [10], U-Net [11], Deeplabv3+ [12], MAE [13]) for denoising shapes corrupted by the six types of noise introduced in Section 2.2.",1,related,1,positive
"For the decoder, we use a flexible one following [15].",1,related,1,positive
"We mainly follow the basic setup of MAE [15]: for the encoder, we adopt different variants of ViT [10], i.",1,related,1,positive
"Specifically, MAE adopts a simple mean square error (MSE) loss: LMAE(h) = ExÃÑEx1,x2|xÃÑ ‚Äñg(f(x1))‚àí x2‚Äñ 2 , (2) where the decoder output xÃÇ2 = h(x1) = g(f(x1)) is assumed to be l2-normalized following the original paper of MAE saying that normalized target x2 yields better performance [15].",1,related,1,positive
We begin by introducing a mathematical formulation of MAE [15].,1,related,1,positive
As inpainter we use a publicly available Masked AutoEncoder (MAE) [21] trained with an adversarial loss.,1,related,0,negative
The MAE [21] we use is based on a ViT architecture and has been pre-trained in an adversarial fashion (as opposed to the standard training with an MSE loss) to output more realistic-looking details,1,related,0,negative
"Following the recent trend of methods for unsupervised object segmentation [7‚Äì12, 22], we build our method on top of SSL features, and, in particular, DINO [4] or MAE [21] features.",1,related,1,positive
"This may be somewhat related to the network (transformer) used in MAE, so although we use a masked representation learning approach similar to MAE, we improve the network structure to make it more adaptable to the learning of ECG representations.",1,related,1,positive
"We also compare the performance of those models to instance-based methods such as MAE (He et al., 2021) and data2vec (Baevski et al., 2022), which do not employ volume maximization regularizers.",1,related,1,positive
"We also compare the performance of those models to instance-based methods such as MAE (He et al., 2021) and data2vec (Baevski et al.",1,related,1,positive
Our pretraining follow the standard hyperparameters defined in He et al. (2021).,1,related,1,positive
"Tables 5, 6,7, 8 and 9, and show the results of SimCLR, MSN, VICReg, data2vec and MAE on the CIFAR100, CIFAR100 1%, Place205, Clevr/Count, Clevr/Dist and KITTI downstream tasks.",1,related,1,positive
"We pretrain a ViT-B/16 with data2vec (He et al., 2021) using the AdamW optimizer with a batch size of 2048 for 800 epochs using the official code base, which is publicly available: http: //github.com/facebookresearch/data2vec_vision/tree/main/beit.",1,related,1,positive
"We pretrain a ViT-L/16 with MAE (He et al., 2021) using the AdamW optimizer with a batch size of 1024 for 800 epochs using the official code base, which is publicly available: https:// github.com/facebookresearch/mae.",1,related,0,negative
"Our long-sequence MAE is a simple and minimallychanged extension of MAE [19], which we summarize first.",1,related,1,positive
"To help the model better model the trajectory, we adopt the patch norm method to normalize the trajectory values in each patch into a standard normal distribution, as has also been done in MAE [37].",1,related,1,positive
"For computation efficiency, we follow MAE [37] to only feed the unmasked patches (and their positions) to the encoder.",1,related,1,positive
"We consider two supervised feature backbones: ResNet50 [25] and ViT-B/16 [18], and four selfsupervised backbones: SimCLR [13], MAE [23], MSN [4] and DINO [11].",1,related,1,positive
"We consider two supervised feature backbones: ResNet50 [16] and ViT-B/16 [13], and four self-supervised backbones: SimCLR [9], MAE [17], MSN [2] and DINO [7].",1,related,1,positive
The random mixing and block-wise mixing strategies are inspired by MAE [18] and BEiT [3] and we replace the masking operation with image mixing on patch-level and block-level (both of size 16√ó16) respectively.,1,related,1,positive
"In particular, we compare to two approaches: R3M [41], which utilizes the Ego4D dataset of human videos to obtain a representation, and MVP [46, 56], which trains a masked auto-encoder [16] on the Bridge Dataset and utilizes the learned latent space as the representation of the new image.",1,related,1,positive
"For a fair comparison, we compare our DMAE ViT-B model with the MAE ViT-B checkpoint released by He et al. (2022) in the linear probing setting on ImageNet.",1,related,1,positive
"Following He et al. (2022); Xie et al. (2022), we use ImageNet-1k as the pre-training corpus which contains 1.28 million images.",1,related,0,negative
"Like MAE (He et al., 2022), we adopt the asymmetric encoder-decoder design for DMAE.",1,related,1,positive
"To enforce the encoder (but not the decoder) to learn robust semantic features, we control the capacity of the decoder by setting a smaller value of the hidden dimension and depth following He et al. (2022).",1,related,1,positive
"For the pre-training of the two DMAE models, we set the masking ratio to 0.75 following He et al. (2022).",1,related,1,positive
"Motivated by the Masked AutoEncoder (MAE) (He et al., 2022), which learns latent representations by reconstructing missing pixels from masked images, we design a new self-supervised task called Denoising Masked AutoEncoder (DMAE).",1,related,1,positive
"In the case of only few labeled data for calibration, the proposed MV-SSTMA is evaluated with the self-supervised method MAE and the supervised methodMD-AGCN.",1,related,1,positive
"In addition, our model outperforms MAE and MD-ADCN in every scenario.",1,related,0,negative
"2 Comparisons of different types of augmentations In this section, we compare the generalization of three canonical augmentations that we analyzed in this work: 1) Gaussian noise injection [21], 2) random mask [16], and 3) random rotation (which we introduced in Section 3.",1,related,1,positive
"In our framework, we allow for cases in which augmentation leads to significant changes in distribution and provide a path to analysis for such OOD augmentations that encompass empirically popular DA [16, 17].",1,related,1,positive
"3, we simulate the biased and unbiased random mask augmentation [16] and test their performance in regression and classification tasks.",1,related,1,positive
"Note that a natural constraint r ‚â§ m holds, and the original MAE setting [14] can be regarded as the special case when r = m.",1,related,1,positive
"Inspired by MAE [14], we also use a shallow decoder with much fewer Transformer blocks than the encoder (Œ¶dec(.",1,related,1,positive
"We only calculate the loss on the mask tokens, which is consistent with MAE and SimMIM.",1,related,1,positive
"Different from the MAE [8] which utilizes a multilayer converter structure as decoder, we only employ a single-layer MLP is used as decoder.",1,related,1,positive
"Given an image-text pair (ùêº ,ùëá ), we random sample 25% of the image patches, the other 75% image patches will not be fed into the models following MAE[11].",1,related,1,positive
"Given an image-text pair (I ,T ), we random sample 25% of the image patches, the other 75% image patches will not be fed into the models following MAE[11].",1,related,1,positive
"The encoder and decoder of PA-former adopt the same structure following [11], in which the encoder transforms the input sub-sampled signal into latent representation and the encoder maps the latent representation back to the full-sampled data.",1,related,1,positive
"For computational efficiency, we adopt an asymmetric MAE architecture (He et al., 2022) based on vanilla Vision Transformers (ViTs) (Dosovitskiy et al.",1,related,1,positive
"For computational efficiency, we adopt an asymmetric MAE architecture (He et al., 2022) based on vanilla Vision Transformers (ViTs) (Dosovitskiy et al., 2020): The encoder operates only on visible patches and the decoder on all the patches.",1,related,1,positive
"For each decoder head, specific positional embeddings are applied following MAE (He et al., 2022).",1,related,1,positive
"Our decoder is another vanilla ViT deployed on the union of the encoded patch set and a set of mask tokens (He et al., 2022).",1,related,1,positive
"Following (He et al., 2022; Feichtenhofer et al., 2022), the encoder is applied only on unmasked patches.",1,related,0,negative
"Our proposed MotionMAE adopts the asymmetric Transformer based masked autoencoder architecture (He et al., 2022), as depicted in Fig.",1,related,1,positive
"MAE uses masked autoencoders (He et al., 2022) and we use its ViT-Large variant.",1,related,1,positive
"B we evaluate the transformer-based models MAE (He et al., 2022) and MOCOv3 (Chen et al.",1,related,1,positive
"B we evaluate the transformer-based models MAE (He et al., 2022) and MOCOv3 (Chen et al., 2021) as well as adversarially fine-tuned versions of MOCOv3 using the same three attacks as for MOCOv2.",1,related,1,positive
"As for segmentation, we use the ViT-base model provided by MMSegmentatation, which is pre-trained by MAE on ImageNet and then finetuned on the ADE20k dataset.",1,related,1,positive
"Note that SimR101, SimR101 and MAEViT stand for Resnet101 pretrained by SimCLRv2, Resnet50 pretrained by SimCLRv2 and ViT-base-16 pretrained by MAE, respectively.",1,related,0,negative
"We craft pre-trained adversarial perturbations (PAPs) for three pre-trained models (i.e., Resnet50 by SimCLRv2, Resnet101 by SimCLRv2, ViT16 by MAE) and evaluate the attack success rates on ten downstream tasks.",1,related,1,positive
We report the results of SimCLR and MAE in Section 4.2.,1,related,0,negative
"Note that Resnet50 and Resnet101 [18] are pre-trained by SimCLRv2 [4], and ViT16 [56] is pre-trained by MAE [21].",1,related,1,positive
"We fine-tune a stateof-the-art vision transformer model pretrained with masked autoencoding (He et al., 2021) for 100 epochs on rotated MNIST (Weiler and Cesa, 2019) (details in Appendix C.5).",1,related,1,positive
"We fine-tune a stateof-the-art vision transformer model pretrained with masked autoencoding (He et al., 2021) for 100 epochs on rotated MNIST (Weiler and Cesa, 2019) (details in Appendix C.",1,related,1,positive
"We use the base model and default finetuning procedure from (He et al., 2021), finetuning for 100 epochs, halving the learning rate on loss plateaus.",1,related,1,positive
"We opted to use a more flexible transformerbased visual architecture, which has recently achieved stateof-the-art results in computer vision tasks [11, 14], and language tasks [10, 31].",1,related,1,positive
125 We pre-train the models via the MAE framework [15].,1,related,1,positive
We leverage the masked 29 autoencoders (MAE) [15] that learn representations by masked prediction.,1,related,1,positive
"An important property of our visual pre-training approach is that uses a self-supervised objec194 tive [15] that makes minimal assumptions about the data distribution and does not rely on human195 designed proxy tasks, like data augmentations.",1,related,1,positive
We pre-train the models via the MAE framework [16].,1,related,1,positive
We train the MAE models for 400 epochs for the combined Ego dataset; 1600 epochs for the HOI dataset; and 1600 epochs for ImageNet dataset.,1,related,1,positive
"Our experiments follow the same architecture, settings, and pre-training recipe as MAE (He et al., 2022), and we find that the simple addition of a teacher (RC-MAE) consistently outperforms MAE in all model sizes (e.",1,related,1,positive
"Method Pre-Train Data APbox APmask Superivsed (He et al., 2022) IN1K w/ labels 47.",1,related,0,negative
"To validate the effectiveness of the EMA Teacher in RC-MAE, we compare RC-MAE to an otherwise identical MAE (He et al., 2022) baseline.",1,related,0,negative
"For the reconstruction target, we use standard normalized patch pixels as done with MAE (He et al., 2022).",1,related,1,positive
", 2021) and implementation details as MAE (He et al., 2022), we fine-tune Mask R-CNN (He et al.",1,related,1,positive
We observe the ASR ranged from 66.34% to 99.18% on both MAE and CAE.,1,related,0,negative
We then take MAE as the target model‚Äôs architecture and conduct comprehensive ablation studies to understand the impacts of important backdoor attack components in each supply chain‚Äôs phase.,1,related,0,negative
"Concretely, for Type I and Type II attacks, as the adversary does not involve in the pre-training phase, we utilize the public MAE 3 and CAE 4 as our target model.",1,related,0,negative
"We compare the MAE performance of using AdamW, SGD, and LARS as the optimizer and find AdamW reaches the best clean accuracy (see Table 9).",1,related,1,positive
"We consider two MIM architectures as the target models, i.e., Masked Autoencoder (MAE) [15] for end-to-end MIM and Contextual Autoencoder (CAE) [8] for tokenizer-based MIM.",1,related,1,positive
VideoMAE: Masked Autoencoders are DataEfficient Learners for Self-Supervised Video PreTraining.,1,related,0,negative
"To promise the results are comparable, we adopt the same linear probing configurations in all three scenarios for both MAE and CAE.",1,related,1,positive
MAE-AST: Masked Autoencoding Audio Spectrogram Transformer.,1,related,0,negative
", Masked Autoencoder (MAE) [15] for end-to-end MIM and Contextual Autoencoder (CAE) [8] for tokenizer-based MIM.",1,related,1,positive
We find that threshold [3-7] produces the best results which shows that it is important to keep a balance between similar and dissimilar view-pairs.,1,related,1,positive
Our decoder must be initialized randomly because the MAE authors do not share their decoder model parameters.,1,related,1,positive
"For our third specialized model, we first pretrain on in-domain simulated data for 150 epochs, initializing our ViT encoder from MAE‚Äôs pretrained model parameters and our ViT decoder randomly.",1,related,1,positive
"We introduce steps 2 and 3 that leverage self-supervised MAE training and finetuning, respectively, using additional adult pose datasets.",1,related,1,positive
"We leverage hierarchical pretraining by continuing to pretrain our ViT encoder on in-domain (i.e., fused depth and pressure) data using the MAE SSL algorithm.",1,related,1,positive
"fied by MAE [34], which we leverage in Section V.",1,related,0,negative
"For our first two specialized models, we pretrain on in-domain‚Äîeither simulated or real‚Äîdata for 150 epochs, initializing our ViT encoder from MAE‚Äôs pretrained model parameters and our ViT decoder randomly.",1,related,1,positive
We also demonstrate a masked autoencoding (MAE) hierarchical pretraining strategy for ViT that significantly improves accuracy on the SMaL dataset.,1,related,1,positive
"Given that the ViT backbone is amenable to hierarchical MAE pretraining strategy, we can further improve the performance of the best-performing architecture, ViTPose.",1,related,1,positive
"In ViTPose, they initialize with MAE‚Äôs encoder; we use this as a baseline to compare with ViTPose models initialized with our three specialized encoders.",1,related,1,positive
"5(left), our method is significantly superior to MAEL, which has the best accuracy of all single deep networks.",1,related,1,positive
"Following the procedure described in previous section, we first constructed object embeddings based on ViT (MAE), CNN (DenseNet) and used kNN classifier (our first approach).",1,related,1,positive
"A. Classifier comparison
In the first round of experiments, we encode RGB and depth modalities of the object separately using ViT (MAE) and CNN (DenseNet) and assessed the performance of seven classifiers, including k-Nearest Neighbors (kNN) [50], Multi-layer Perceptron (MLP) [51], Support Vector Machine (SVM) [52], Decision Tree (DT) [53], Gaussian Process (GP) [54], Random Forest (RF) [55], and Gaussian Naive Bayes (GNB) [56], on the restaurant fine-grained object dataset.",1,related,1,positive
We used MAE (RGB) + DenseNet (RGB-D) to represent each of the objects.,1,related,1,positive
"The accuracy of our multimodal appraoch-II with DeseNet (RGB-D) and MAE (RGB) was 93.51%, which outperformed all single models, CNNs (Dense.+Mnas.)",1,related,1,positive
"For future work, we are interested to explore AttnDistill for knowledge distillation between ConvNets and ViT.",1,related,0,negative
"Observing that the previous SSKD methods focussed on ConvNet do not work well on ViT, we proposed AttnDistill to distill the knowledge from a pretrained teacher model to its student model.",1,related,0,negative
"We draw the following conclusions:
‚Ä¢ Based on ViT-T/16 distilled from Mugs(ViT-S/16), our method AttnDistill gets state-of-theart k-NN and Linear probing performance compared with previous knowledge distillation methods based on ConvNet.",1,related,1,positive
"He et al., 2022), we avoid such learnable prefix design with random initialization and propose a parallel attention (PATT) to the original attention module (see Figure 3).",1,related,1,positive
"He et al., 2022) in the NLP domain, we aim to propose a unified model for the vision domain, especially for video-based downstream tasks.",1,related,1,positive
"He et al., 2022; Tong et al., 2022), adding a prefix for a sentence input in NLP can be structurally different from the visual domain.",1,related,1,positive
"Specifically, we use the weight from the original vision MAE model (He et al., 2022) (Weights from https://github.com/ facebookresearch/mae) with only self-supervised learning (SSL) pretraining for all audio, visual, and joint encoder and the decoder.",1,related,1,positive
"Specifically, we use the weights of the original vision MAE He et al. (2022).",1,related,1,positive
"Due to the space limitation, please refer to He et al. (2022); Huang et al. (2022a) for single-modal MAEs.",1,related,1,positive
"Implementation Detail For the image classification problem, we use Vision Transformer pretrained on unlabeled ImageNet dataset with masked auto-encoding (He et al., 2022) and fine-tune it on the downstream task with AdamW optimizer (Loshchilov & Hutter, 2019) for 10,000 steps with batch size 32.",1,related,1,positive
"Firstly, we remove masked auto-encoding objective LMAE and train the model with only distillation loss LDistill before fine-tuning.",1,related,1,positive
"Then we take the copy of the pre-trained model (fŒ∏init , gœïinit) as a student, and match the representations of the student encoder and those of the teacher encoder while optimizing the student with the MAE on the target unlabeled data.",1,related,1,positive
"Masked Auto-Encoding We briefly describe the masked auto-encoding objective (Liu et al., 2019; He et al., 2022) for a language model such as RoBERTA (Liu et al.",1,related,1,positive
"In this work, we focus on the masked auto-encoding (Devlin et al., 2019; He et al., 2022) as a pre-training objective function since its generality compared to other self-supervised methods (Chen et al.",1,related,1,positive
"Masked Auto-Encoding We briefly describe the masked auto-encoding objective (Liu et al., 2019; He et al., 2022) for a language model such as RoBERTA (Liu et al., 2019) and Vision Transformer (ViT) (Dosovitskiy et al., 2021).",1,related,1,positive
"Specifically, we take the pre-trained model with an encoder fŒ∏init and a decoder gœïinit which are pretrained on a massive amount of unlabeled data from general domain, and continue to pre-train it with masked auto-encoding (MAE) (Devlin et al., 2019; He et al., 2022) objective on the target unlabeled data to obtain fŒ∏0 and gœï0 .",1,related,1,positive
"Then we take the copy of the pre-trained initial network gœïinit ‚ó¶fŒ∏init as a student and further pre-train the student with masked auto-encoding objective but enforce hidden representation of the encoder of the student fŒ∏init to be close to that of the teacher fŒ∏0 as follows:
(Œ∏1, œï1) ‚àà argmin Œ∏,œï (LMAE(Œ∏, œï;Du) + LDistill(Œ∏; Œ∏0,Du))
LDistill (Œ∏; Œ∏0,Du) = 1
n n‚àë i=1 ‚à•‚à•‚à•fŒ∏(x(i))‚àí StopGrad(fŒ∏0(x(i)))‚à•‚à•‚à•2 2
(3)
where Œ∏ and œï are initialized with the pre-trained parameters Œ∏init and œïinit, respectively and StopGrad denotes the stop-gradient operation which does not back-propagate through the input.",1,related,1,positive
"Then the final objective for masked auto-encoding is defined as follows:
LMAE(Œ∏, œï;Du) = 1
n n‚àë i=1 Ez(i)‚àºpŒ≥,T (z)
[ ‚àí
K‚àë k=1 z (i) k Z(i) ¬∑ log pŒ∏,œï(x(i)k |xÃÇ(i))
] , Z(i) =
K‚àë k=1 z (i) k , (2)
where pŒ≥,K(z) denotes a Binomial distribution with its parameters Œ≥ for probability that zk = 1 and K for the number of trials.",1,related,1,positive
"In this experiment, we continue to pretrain the encoder of the pre-trained transformer (He et al., 2022) with supervised contrastive loss (Khosla et al.",1,related,1,positive
"Specifically, we take the pre-trained model with an encoder fŒ∏init and a decoder gœÜinit which are pretrained on a massive amount of unlabeled data from general domain, and continue to pre-train it with masked auto-encoding (MAE) (Devlin et al., 2019; He et al., 2022) objective on the target unlabeled data to obtain fŒ∏0 and gœÜ0 .",1,related,1,positive
"‚Ä¶model with an encoder fŒ∏init and a decoder gœïinit which are pretrained on a massive amount of unlabeled data from general domain, and continue to pre-train it with masked auto-encoding (MAE) (Devlin et al., 2019; He et al., 2022) objective on the target unlabeled data to obtain fŒ∏0 and gœï0 .",1,related,1,positive
"In this work, we focus on the masked auto-encoding (Devlin et al., 2019; He et al., 2022) as a pre-training objective function since its generality compared to other self-supervised methods (Chen et al., 2020b; He et al., 2020; Grill et al., 2020; He et al., 2020; Chen & He, 2021; Caron et al.,‚Ä¶",1,related,1,positive
"In this experiment, we continue to pretrain the encoder of the pre-trained transformer (He et al., 2022) with supervised contrastive loss (Khosla et al.,
2020) while fine-tuning a randomly initialized linear classifier with cross-entropy loss.",1,related,1,positive
"Xiao et al. (2022) demonstrates that masked-autoencoder (He et al., 2022) trained on diverse video data (Goyal et al., 2017; Shan et al., 2020) can be an effective visual embedding for online RL.",1,related,1,positive
"We address this gap by profiling four popular self-supervised methods (BYOL [12], SimCLR [7], DINO [6], and MAE [13]) and two supervised methods (CLIP [27] and standard classification).",1,related,1,positive
"We use the MAE ViT [13] as the backbone in SimCLR, BYOL, and CLIP (using e.",1,related,1,positive
"1 Pre-training Methods and Models We run four common self-supervised methods (BYOL [12], SimCLR [7], DINO [6], and MAE [13]) and two supervised methods (CLIP [27] and standard softmax classification).",1,related,1,positive
"Similar to [13], we ran an exploratory learning rate (LR) search across {1e-5, 5e-5, 1e-4, 5e-4} and weight decay (WD) values {0.",1,related,1,positive
"4.3), we investigate several recent ones (DINO (Caron et al., 2021), MoCo-v3 (Chen et al., 2021), MSN (Assran et al., 2022), MAE (He et al., 2022)).",1,related,1,positive
"For the pre-trained weights, we use the timm library for DINO and third-party releases for MoCo-v35, MSN6, and MAE7.",1,related,0,negative
"To this end, we train DINOSAUR with a ResNet34 encoder (from scratch) on COCO, but reconstruct targets obtained from ViTs pre-trained with different methods: DINO, MoCo-v3, MSN, and MAE.",1,related,0,negative
"In the following, we use block 9 for supervised training, DINO and MSN, and block 12 for MoCo-v3 and MAE.",1,related,0,negative
"Method Ours SKD BYOL SimSiam MAE Transfer From Scratch Accuracy 82.7% 74.2% 68.3% 66.8% 62.3% 53.9% 28.4%
Figure 3: Examples of real and distilled images.",1,related,0,negative
"For each training batch, we compute each objective through a separate forward pass and use the weighted sum of them for the final loss, where ŒªVAM = 1.0 and ŒªMAE = 0.3.
loss = ŒªVAMlossVAM + ŒªMAElossMAE (1)",1,related,1,positive
"(1), we use ŒªVAM = 1.0 and ŒªMAE = 0.3.",1,related,1,positive
"Following MAE-AST [7], we randomly mask 75% of the spectrogram patches.",1,related,0,negative
"We calculate the mean squared error between the reconstructed and original video frames and spectrograms:
lossMAE = 1
NVM ‚àë i‚ààmasked ||xVi ‚àí xÃÇVi ||22 + 1 NAM ‚àë j‚ààmasked ||xAj ‚àí xÃÇAj ||22 (3)
whereNVM andN A M are the number of masked patches for vision and audio, respectively.",1,related,1,positive
"4.2, we use separate decoders (with shared weights) for the vision and audio MAE pretraining objectives.",1,related,0,negative
"Following MAE [26], we randomly mask 75% of the visual patches, and the masking is applied for each video frame independently.",1,related,1,positive
"In addition to the VAM objective to learn cross-modal representation, we also use the masked autoencoding (MAE) objective to improve unimodal representations in the vision-and-language settings, by masking random patches of visual frames and the audio spectrogram, and reconstruct missing inputs as shown in Fig.",1,related,1,positive
"In Figure 3 and Figure 4, we show the reconstruction results with the MAE head, described in the main paper Sec.",1,related,1,positive
"The combination of VAM and MAE further improves the finetuning performance, and we use this configuration as default for TVLT pretraining.",1,related,0,negative
"(a) Using the state-ofthe art reconstruction-based SSL strategy, MAE [8] architecture for pre-training an representation extractor (encoder).",1,related,1,positive
"As indicated in [1, 7, 8] that SSL training requires a large amount of data, we begin with training a MAE on SSL Pre-Training Set.",1,related,0,negative
"1), we utilize a state-of-the-art reconstructed-based strategy, Masked Auto-Encoder (MAE) [8] to learn lowerdimensional semantic feature embedding.",1,related,1,positive
"Motivated by theoretical implication of SSL embedding space, we leverage a Masked Autoencoder [8] for feature extraction.",1,related,1,positive
"In this section, we verify that GPaCo can enhance model generalization ability and robustness on full ImageNet and its variants when compared with MAE [14] models.",1,related,1,positive
"To demonstrate the advantages of GPaCo on out-ofdistribution data, we load MAE [14] pre-trained weights and then fine-tune on full ImageNet with same training strategy as in [14].",1,related,0,negative
"We also experiment with the recently proposed masked auto-encoder framework [60], which is based on the ViT",1,related,1,positive
"Another exception is the masked auto-enconder [60] based on the ViT backbone, where we place SSPCAB and SSMCTB before the first transformer block.",1,related,1,positive
"Mixed Feature Prediction Task Motivated by MAE [12] and Point-BERT [11], we wanted to extend this self-supervised method to point cloud segmentation.",1,related,1,positive
"Mixed Feature Prediction Task
Motivated by MAE [12] and Point-BERT [11], we wanted to extend this self-supervised method to point cloud segmentation.",1,related,1,positive
"We pretrained five encoders, one using our proposed method TOV-VICReg and four using state-of-the-art self-supervised methods: MoCo v3 (Chen et al., 2021), DINO (Caron et al., 2021), VICReg (Bardes et al., 2022), and MAE (He et al., 2021).",1,related,1,positive
"For this study we selected DINO (Caron et al., 2021), MoCo (Chen et al., 2021), MAE (He et al., 2021), and VICReg (Bardes et al., 2022) since they are currently considered state-of-the-art, their official implementations are available in
PyTorch, and each represents a different type of approach.",1,related,0,negative
"Lastly, we also consider MAE (He et al., 2021), a masked reconstruction method, which consists in training an auto-encoder based on ViT to reconstruct an image with a set of patches masked.",1,related,1,positive
"The procedures here can be summarized as follow:
y = ClassHead(FF (CLSL))
ViT and other models based on it have found much success in producing state-of-the-art results in image processing domains [23, 24] and have been the building block for many pre-trained models such as, to mention a few, DINO [25], MAE [26], and MOCOv3 [27].",1,related,1,positive
"We take Vit [38], MAE [185], and MoCo [186] in our experiments.",1,related,1,positive
"To further reduce the encoder training time and improve robustness, a pre-trained encoder [27], [28] could be used and fine-tuned on our data.",1,related,0,negative
"Thus, we treat STMM as images and borrow the training approach of MAE [14].",1,related,1,positive
"To this end, we treat STMM as images and borrow the key principle of MAE [14] to process",1,related,1,positive
‚Äòw/ SSL‚Äô denotes training the segmentation network with the model weights pre-trained by a 3D Masked Autoencoder (MAE) SSL method [8].,1,related,1,positive
"Implementation Details To pre-train the ASA model, we use center-cropping augmentation, Xavier uniform initializer [5] for SW-ViT blocks and set the hyper-parameters following [8] (see Table 1(a)).",1,related,1,positive
"To address this problem, the Masked Autoencoder (MAE) self-supervised training method [41] is used to pre-train the proposed model on unlabeled data.",1,related,1,positive
"Further, the Masked Autoencoder pre-training method [41] is introduced.",1,related,0,negative
"By combining MAE [19] pre-training and DAT fine-tuning, our ViT-Huge [20] achieves 31.40 mCE on ImageNet-C [2] and 32.77% top-1 accuracy on Stylized-ImageNet [3].",1,related,1,positive
"Besides, we use DAT to conduct supervised finetuning on downstream ImageNet classification task based on a self-supervised ViTHuge pretrained by MAE [19].",1,related,1,positive
"We perform the pre-training on three largescale medical image-text datasets, i.e., ROCO [40], MedICaT [44], and MIMIC-CXR [21].",1,related,1,positive
"For ROCO and MedICaT, we filter nonradiology samples, and for MIMIC-CXR, we only keep images in the frontal view.",1,related,0,negative
"As for the dataset split, we adopt the official splits of ROCO and MIMIC-CXR.",1,related,1,positive
"For the pretext tasks, we adopt (knowledge-enhanced) MLM, MIM [17], and ITM, where the masking ratios of MLM and MIM are set to 15% and 75%, respectively.",1,related,1,positive
"We first fine-tune a ViT-Large model [9, 14] on Places365 [54], which is dubbed PlacesViT.",1,related,0,negative
The architectural settings strictly follow [19].,1,related,0,negative
"As a special case of MIM, we formulate MKD upon which an empirical investigation is conducted about the influence of different target representations on self-supervised masked autoencoders.",1,related,1,positive
"To answer these questions, we employ the standard masked autoencoder framework [19] to give a system-level study, introduced next.",1,related,1,positive
"To be specific, MIM randomly masks a portion of the input and then reconstructs the masked portion according to the transformed target, formulated as
min Œ∏ E x‚àºD M(T (x (1‚àíM)), fŒ∏(x M)), (1)
where ‚Äú ‚Äù means element-wise product; M is the patch mask; ‚Äúx M‚Äù represents ‚Äúunmasked patches‚Äù and vice ‚àóEqual contribution.",1,related,1,positive
"In this work, we paraphrase a term Masked Knowledge Distillation (MKD) to focus our discussion on a special case of MIM where the target is generated by a parameterized network (teacher network), i.e., T (¬∑) = hœÜ(¬∑).",1,related,1,positive
method data2vec [2] BEiT [3] MAE [19] dBOT,1,related,1,positive
"Similar to other MIM-based SSL work [19, 45], we reconstruct knowledge for those masked patches of input sample x .",1,related,1,positive
"MIM [45], MAE [19], and our MimCo on ImageNet-1K dataset.",1,related,1,positive
"What Semantic PatternsDoesMimCoLearn? To further help reveal what patterns does MIM learn, we follow the visualization of iBOT [47] to explore the learned patterns of the pre-trained models of SimMIM [45], MAE [19], and our MimCo via visualization, respectively.",1,related,1,positive
"We randomly choose 10 classes of ImageNet-1K dataset to visualize for simplicity, the visualization of learned representation shows that our MimCo significantly improves the linear separability of representations compared to SimMIM [45] and MAE [19].",1,related,1,positive
"Furthermore, we choose different generative blocks, including cross-attention block (Chen et al., 2022), self-attention block (He et al., 2022), and convolutional projector (Yang et al., 2022e).",1,related,1,positive
"Different from existing works that focus on MVM for pure vision problems [4, 22, 86], we study MVM as a VidL pre-training task.",1,related,0,negative
"Inspired by the MAE [15], we propose a spatial-temporal masked autoencoder framework for self-supervised 3D skeleton-based action recognition (SkeletonMAE).",1,related,1,positive
The generative SSL trains a generator consisting of an encoder and decoder to reconstruct the input data.,1,related,1,positive
"Like what ImageMAE does in [9], we directly discard a subset (e.g., 50",1,related,1,positive
"Like what ImageMAE does in [9], we directly discard a subset (e.",1,related,1,positive
"We propose the MAE-VQGAN model, which combines ideas from MAE [20] and VQGAN [15].",1,related,1,positive
"‚Ä¶our lexiconbottlenecked masked autoencoder (LexMAE) contains one encoder and one decoder with masked inputs in line with the masked autoencoder (MAE) family (He et al., 2021a; Liu & Shao, 2022), while is equipped with a lexicon-bottlenecked module for document-specific lexicon-importance learning.",1,related,1,positive
"We also compared our model with the few-shot counting sota method Fam-
Net [24], and our model outperforms it with a large advance (15.16 MAE and 24.29 RMSE on Val-COCO and 11.87 MAE and 14.81 RMSE on Test-COCO), which demonstrates the superiority of our model.",1,related,1,positive
"We use two standard metrics to measure the performance of our model, namely, Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).",1,related,1,positive
"MAE = 1 NI
NI ‚àë i=1 |Ci‚àíCGTi |, RMSE = ‚àö‚àö‚àö‚àö 1 NI NI ‚àë i=1 (Ci‚àíCGTi )2 (6)
Here, NI is the total number of testing images, and Ci and CGTi are the predicted number and ground truth of the ith image.",1,related,1,positive
"In this section, we aim to give the detail of our proposed two-stage training procedure, that is, first pre-train the ViT encoder with MAE [11], and then fine-tune the whole model on supervised object counting.",1,related,1,positive
"We propose a two-stage training scheme, with the transformer-based image encoder being firstly pre-trained with self-supervision via masked image modeling [11], followed by supervised fine-tuning for the task at hand.",1,related,1,positive
"# Shots Val Test
MAE RMSE MAE RMSE
A0 % % % % 0 24.84 86.",1,related,0,negative
"As input for pre-training ViT with MAE, we randomly drop 50% of the visual tokens, and task the model to reconstruct the masked patches with pixel-wise mean square error.",1,related,1,positive
"1 Training Details In this section, we aim to give the detail of our proposed two-stage training procedure, that is, first pre-train the ViT encoder with MAE [11], and then fine-tune the whole model on supervised object counting.",1,related,1,positive
"Specifically, we adopt the recent idea from Masked Autoencoders (MAE), to train the model by image reconstruction with only partial observations.",1,related,1,positive
"In Section 3.2, we further introduce a two-stage training regime, in which the model is firstly pre-trained with self-supervision via masked image reconstruction (MAE), followed by fine-tuning on the downstream counting task.",1,related,1,positive
"The two methods are based on the recent Masked Auto Encoder (MAE) approach [26], where we randomly mask the image ( e.g. , 75",1,related,1,positive
"The two methods are based on the recent Masked Auto Encoder (MAE) approach [29], where we randomly mask the image (e.",1,related,1,positive
"We also exploit more efficiently the input data with Masked Image Modeling, based on the recently proposed Masked Auto Encoder (MAE) [26].",1,related,1,positive
"We also exploit more efficiently the input data with Masked Image Modeling, based on the recently proposed Masked Auto Encoder (MAE) [29].",1,related,1,positive
"We propose a new efficient VLP approach centered on 3 main components; stronger Vision-Language pre-alignment through hierarchical contrastive objective, self supervision via masked image modeling based on MAE, and a new Visual Concepts injection and extraction technique.",1,related,1,positive
"We favor the MAE based, unimodal MIM loss which improves the results by 2.4% RSUM.",1,related,0,negative
"With the most recent representative work in this direction, masked autoencoder (MAE) [11], we are now able to efficiently and effectively pre-train high-capacity Vision Transformers (ViTs) with strong feature representations, leading to state-of-the-art solutions for a wide range of downstream visual tasks.",1,related,1,positive
"Following MAE [11], we first perform self-supervised pre-training on ImageNet-1k [7].",1,related,0,negative
We choose a decoder depth of 8 as the default setting as in [11].,1,related,1,positive
"Following the setting in MAE [7], we add a batch-norm layer without learnable affine parameters before the classifier to avoid adjusting the learning rate for each model.",1,related,1,positive
"CLIP
+
SimCLR
CLIP
+ MAE
Sandwich bread
MaskCLIP
(Ours)
CLIP
Bird
MaskCLIP
(Ours)
CLIP
CLIP
+
SimCLR
CLIP
+ MAE
SnowMountain goats Santa hatBearded Man
BandanaDog",1,related,1,positive
We start from CLIP+MAE and add three components of the distillation loss one by one.,1,related,1,positive
We adopted the MAE structure proposed in [14].,1,related,0,negative
"Following the previous work [9], we take ViTB [57] as the backbone network, which consists of 12 transformer layers and was pre-trained on ImageNet-21K with the self-supervised method MAE [21].",1,related,1,positive
"Inspired by [8], we design a decoder using a lightweight transformer structure [8] and is only used during the pre-training phase.",1,related,0,negative
"For image encoder, we employ a standard ViT-B/16 (Dosovitskiy et al. 2020) with random masking strategy (He et al. 2022).",1,related,1,positive
"Following MAE (He et al. 2022), we randomly divide the patches",1,related,1,positive
"Following (He et al. 2022), we take a 4-layer transformer with 512 dim as our image decoder.",1,related,1,positive
"Unlike MAE (He et al. 2022), we aim to reconstruct the invisible patches with both image and text features to facilitate multi-modal information fusion.",1,related,1,positive
"Motivated by recent generative methods in visual self-supervised learning (He et al. 2022; Chen et al. 2022), we propose to introduce pixel-level reconstruction task to VLP.",1,related,1,positive
"Following MAE (He et al. 2022), we adopt a lightweight vision transformer as our image decoder.",1,related,1,positive
"Following MAE (He et al. 2022), we randomly divide the patches into visible patches { xvisi }N‚àíM i=1
and invisible patches{ xmski }M i=1
according to mask ratio Œ±, where M = Œ±N .",1,related,1,positive
"We compared the performance with other baselines involving Vision Transformers such as ConViT [16], Masked Auto Encoders (MAE) [53], Convolution-enhanced image Transformer (CeiT) [33], LeViT [34].",1,related,1,positive
"Inspired by (He et al. 2022; Wettig et al. 2022) that a large mask rate may be beneficial for pre-training, we employ a mask rate greater than the BERT‚Äôs 15% setting.",1,related,0,negative
"Inspired by (He et al. 2022; Wettig et al. 2022) that a large mask rate may benefit pre-training, we adopt up to 30% and 45% mask rate in CoT-MAE‚Äôs encoder and decoder, respectively.",1,related,0,negative
"Thus, borrowing insights from masked autoencoders [33], we believe most of the points can be redundant and we randomly masked the points with a probability of 75% per visit.",1,related,1,positive
"In the long series prediction, we employ Mean Squared Error (MSE) and Mean Absolute Error (MAE) as indicators and follow the setting (Zhou et al. 2021).",1,related,1,positive
"The formula is as follows: ‚Ä¢ Mean Squared Error (MSE):
MSE = 1
n n‚àë i=1 (Yi ‚àí YÃÇi)2
‚Ä¢ Mean Absolute Error (MAE):
MAE = 1
n n‚àë i=1 |Yi ‚àí YÃÇi|
In the short series prediction, we employ Root Relative Squared Error (RSE) and Empirical Correlation Coefficient (CORR) as indicators and follow the setting (Lai et al. 2018).",1,related,1,positive
"In the absence of prior work applying masked autoencoding to abstract/synthetic imagery, we explore a large masked pixel percentage (75% of the image), thus forcing the model to attempt to recover the image based only on the unmasked 25% of the image, such high percentages have been shown to work well for natural imagery [30].",1,related,1,positive
%) of ViT-B and ViT-L trained by selfsupervised MAE on ImageNet.,1,related,0,negative
"2) self-supervised settings: we follow the MAE training framework to pretrain and fine-tune ViT-B and ViT-L, and report results in Table 3.",1,related,0,negative
"To obtain the benefits of self-supervised learning in video classification, we utilize VideoMAE pre-trained in Something-Something [8] to extract video features inherent in make-up scenes.",1,related,0,negative
"When self-pretrained, MAE [24] is mainly used, and we directly use their pretrained models.",1,related,0,negative
"When the model is self-pretrained by MAE [24], we first evaluate the fine-tuning performances of MAE on the labeled data only, as the common practice in self/un-supervised learning literature [25, 14, 22], with results shown Table 1.",1,related,1,positive
"Following the training recipe provided by MAE [25], the ViT models pretrained on ImageNet1K dataset serve as the backbone of UperNet [59], and are finetuned together with the segmentation layers.",1,related,0,negative
We use a masked autoencoder architecture similar to MAE [25].,1,related,1,positive
"Using the default 75% masking ratio, our prompting decoder reduces the decoding computation cost by 20% compared to MAE [25].",1,related,1,positive
"Our work takes some inspiration from MAE, however, PatchDropout can be applied to target tasks directly using standard ViTs (unlike MAE).",1,related,0,negative
"To ensure the effectiveness of training, we compute the loss on the common parts of the masked patches of T1 and unmasked patches of T2 (following [29]).",1,related,1,positive
"For simplicity, we take MAE [29] as an representative example of MIM, in which the similarity measurement is simply l2‚àídistance on the masked patches.",1,related,1,positive
"3In original MAE [29], the encoder network only generates tokens of unmasked patches and the decoder only predict the masked patches during training.",1,related,0,negative
"Furthermore, taking MAE [29] as a representative example of MIM, we empirically find the success of MIM models relates a little to the choice of similarity functions, but the learned occlusion invariant feature introduced by masked image ‚Äì it turns out to be a favored initialization for vision transformers, even though the learned feature could be less semantic.",1,related,1,positive
"Inspired by the above pilot studies, we employ the same MIM pretraining and finetuning routine to investigate the influences of plain ViTs on RS tasks.",1,related,0,negative
"For more details about MAE, please refer to [12].",1,related,0,negative
"Different from these works, we use the representative MAE method [12] for MIM pretraining and specifically focus on advancing plain ViTs for remote sensing tasks.",1,related,0,negative
"Different from these works, we use the representative MAE method [12] for",1,related,1,positive
"To explore the effectiveness of MIM pretraining in remote sensing tasks, we pretrain our model using MAE [12] with the large-scale RSI dataset, i.e., MillionAID [11].",1,related,1,positive
"To explore the effectiveness of MIM pretraining in remote sensing tasks, we pretrain our model using MAE [12] with the large-scale RSI dataset, i.",1,related,1,positive
"Specifically, by considering two typical ViT backbones ‚Äì plain-ViT Dosovitskiy et al. (2020); Caron et al. (2021); He et al. (2021) and hierarchical-ViT Wang et al. (2021c); Liu et al. (2021); Chu et al. (2021a), we implement MVSFormer-P and MVSFormer-H as in Fig.",1,related,1,positive
"alternative MVSFormer to plain-ViT with vanilla attention (Dosovitskiy et al., 2020; Caron et al., 2021; He et al., 2021), called MVSFormer-P.",1,related,1,positive
"In particular, 1) we utilize the recent ViTs pre-trained with self-supervised (MAE He et al. (2021), DINO Caron et al. (2021)) and supervised (Twins Chu et al. (2021a)) tasks, while it is an interesting future work of exploring the influence of different pre-training tasks on MVS.",1,related,0,negative
"For image masking, we follow [17, 52] and use random masking of raw image patches with a masking patch size of 32 √ó 32.",1,related,1,positive
"From left to right are masked images, results of pre-trained Masked AutoEncoder (MAE) [20], results of LaMa [40], and results from our method.",1,related,1,positive
"To address these dilemmas, we propose to guide the image inpainting with an efficient Masked AutoEncoder (MAE) pre-training model [20], which is called as prior Feature and Attention enhanced Restoration (FAR).",1,related,1,positive
"Benefited by the efficient design from [20] with 25% unmasked input tokens to the encoder, we should claim that the MAE pre-training is not heavier than CNNs.",1,related,0,negative
"Our method incorporates effective prior features from the transformer based representation learning [20] to enhance the inpainting, which make our method achieve superior results without overfitting the transformer results.",1,related,1,positive
Our MAE loss is the mean squared error (MSE) between MAE predictions and ground truth pixel values for masked patches as in [20].,1,related,1,positive
"Inspired by MAE [24], we propose a value normalization function upon teacher outputs.",1,related,1,positive
"For a fair comparison, we directly follow most of the hyperparameters of MAE [24] in our fine-tuning experiments.",1,related,1,positive
We follow UNITER [9] to set 15% masking ratio for masked language modeling and we follow MAE [29] to set 75% masking ratio for masked image modeling.,1,related,1,positive
"Specifically, we use Vision Transformer (ViT) pre-trained by AutoEncoder (MAE) [61] as the pre-trained model , and follow the fine-tuning settings obtained from the Github repository of MAE which is implemented on PyTorch.",1,related,1,positive
TABLE 1 Comparison of denosing autoencoder [12] and masked autoencoder [1],1,related,1,positive
"In this work, we use MAE exclusively to refer to the method in [1] not as shorthand for masked autoencoder to avoid confusion.",1,related,1,positive
"In other words, the success of masked autoencoder in vision paves a path that SSL in vision‚Äúmay now be embarking on a similar trajectory as in NLP‚Äù [1] by generative pretext task with masked prediction.",1,related,1,positive
"Note that for the SSL-based label augmentation, there are 400 classes in total, which 2D t-SNE is unable to clearly illustrate the distributions of all the classes (including the rotated classes).",1,related,1,positive
"In this section, we demonstrate that the Entropy-based geometric variance (HV) is a good indicator of predictive uncertainty induced by SSL-based label augmentation, and also exhibits high correlation with predictive accuracy.",1,related,1,positive
"(B) In PASS, SSL-based label augmentation is applied on all three phases and expands the classes to be 16, 9, and 9 classes.",1,related,1,positive
"To enhance the discriminative power of CIL method, SSLbased label augmentation has been used to expand the original Kt classes to 4Kt by rotating the
10 0 12 0 14 0 16 0 18 0 20 0
20
original image x. Specifically, for image x, the rotated image x(Œ±) = rotate(x, œÄ2Œ±), Œ± ‚àà {0, 1, 2, 3} will be assigned to a new class y = k(Œ±), k ‚àà {1, ...,K},K ‚àà ‚àët œÑ=1KœÑ .",1,related,1,positive
"In the 5-shot part segmentation task, our approach outperforms state-of-the-art methods in this dataset with mIoU increases of 3.9%, 0.3%, and 2.7% respectively for DatasetGAN, DDPM, and MAE.",1,related,1,positive
"We follow previous work [4] to first train a ViT-Large [12] by MAE, and then extract ViT features for part segmentation.",1,related,1,positive
Our model can better avoid noises (compared to MAE [19] and,1,related,0,negative
"In the 1-shot part segmentation task, our model achieves significant improvement of 4.5%, 8.6%, and 10.5% compared to DatasetGAN, DDPM, and MAE, respectively.",1,related,1,positive
"For MIM tasks, color enhancements degrade the results [24], so we do not apply them to the input of the online branch.",1,related,1,positive
", MAE [24] and SimMIM [42]), our method further processes the input image via a spatially shifted cropping operation.",1,related,1,positive
"So, can we extract features using a lightweight transformer model and CNN together? The MAE [21] model is a lightweight encoder-decoder model that uses a CNN to encode the image patch, mask most of the patch embedding to extract the global features, and then use a lightweight decoder for fine-grained reduction, which is perfect for global feature extraction is more than appropriate.",1,related,1,positive
"We also report the result on COCO test-dev with a large foundation model (ViT-Huge [62, 20, 7]).",1,related,1,positive
"When finetuning the detector on COCO, we find that applying learning rate decay [10, 1, 20, 7] for the components of the detector gives a ‚àº0.",1,related,1,positive
"For MAE with ViT-Base/16, we follow the default end-to-end fine-tuning schedule: AdamW as the optimizer with base learning rate 5e-4 using the cosine learning rate decay; the layer-wise learning rate decay is set to 0.65 and weight decay is set to 0.05; the drop path is set to 0.1 and the warmup epochs are 5.",1,related,1,positive
Table 5 shows the recognition accuracies on the VIPriors-10 and NICO datasets with ViT-Base/16 as the feature backbone and MAE [32] as the SSL method.,1,related,1,positive
‚ó¶ Section C shows the results with MAE pretrained feature (Section 4); the attention map visualizations (Section 5.4) and the algorithm complexities (Section 5.4).,1,related,1,positive
We leave the results based on the most recent MAE [32] in Appendix.,1,related,0,negative
"For the NICO dataset, we train the MAE for 2000 epochs and adopt the mixup version of IP-IRM [86] to achieve the reasonable performance.",1,related,1,positive
"We use ViT-base as the backbone for both two methods, which is pretrained by MAE [5] on face image benchmark CelebA [21].",1,related,1,positive
", BeiT [1], Masked Auto-Encoder (MAE) [5].",1,related,1,positive
"Note that OBJFEAT, MAE and EPC all benefit from pre-training on the same walkthrough videos as our approach.",1,related,0,negative
We use all frames from our generated walkthroughs to train an MAE VIT-large model with 16 √ó 16 patch size.,1,related,1,positive
"The encoder, generator and discriminator here are just the copies of the corresponding vanilla ViTs pre-trained by the method MAE [10] using AffectNet images.",1,related,1,positive
"We show the fine-tuning results of the pretrained ViTs, whose representations are obtained by conventional supervised learning (SL) or self-supervised learning as MAEs (MAE) [10] in Table 2.",1,related,1,positive
"Motivated by the success of self-supervised Masked Autoencoders [10] (MAEs), which employs an asymmetric encoder-decoder architecture to implement strong representation learning, and the deficiency analysis of the prior identity-invariant FER work, we propose a novel FER model referred to as Poker Face Vision Transformer (PFViT) .",1,related,1,positive
"We train ViTs as Masked Autoencoders [10] on AffectNet [22] dataset without noisy and biased emotion labels, so as to obtain stronger representations of expressive faces and narrow the gap between the selfsupervised pre-training task and downstream FER task.",1,related,1,positive
"Our PF-ViT benefits much from the strong ViT representations learned by the selfsupervision learning method MAE [10], described in Sec.",1,related,1,positive
"We firstly build and report several strong and transparent baselines based on vanilla Vision Transformers (ViTs) [6, 10] for the task of facial expression recognition (FER).",1,related,0,negative
"4, where IN1KViT and MAE-ViT represent the plain ViTs pre-trained using ImageNet1K data and AffectNet data as MAEs [10], respectively.",1,related,1,positive
"Inspired by MAE [26], our method directly recovers the input patches.",1,related,1,positive
"And following [26], we set a lightweight decoder, which has 6 layers.",1,related,1,positive
"Here, we propose several positional embedding strategies: a) utilizing learnable parameters as MAE [26] does; b) first embedding the center coordinates of each face, then applying max-pooling; c) first reshaping the center coordinates of 64 faces (64, 3) into a one-dimension vector (64√ó3), then embedding this vector directly; d) first calculating the center coordinates of the whole patches, then embedding the center of the patch directly (ours).",1,related,1,positive
"Inspired by MAE [26], we wonder whether performing masked autoencoding on the unlabeled 3D mesh data could also promote the ability of the network.",1,related,1,positive
"Unlike MAE [26], which reconstructs the raw pixels naturally, we choose to recover the geometric information of the masked patches.",1,related,1,positive
"Similar to [20], we use the mean squared error as loss function as follow:",1,related,1,positive
"Inspired by [20], we train our image patch generation network with random mask method on ImageNet [21] and our own dataset.",1,related,1,positive
"As for the second model, we first use the EmotionNet dataset to pre-train the MAE model with the reconstruction task, and then use the AffectNet [26] dataset to fine-tune the model further.",1,related,1,positive
‚Ä† https://github.com/pengzhiliang/MAE-pytorch,1,related,0,negative
"1) We explore several unsupervised (MAE-based) and supervised (IResNet/DenseNet-based) visual feature representation learning methods for learning effective and robust visual representations; 2) We utilize three types of temporal encoders, including GRU [4], LSTM [29] and Transformer [31], to capture the sequential information in videos; 3) We employ multi-task frameworks to predict the valence, arousal, expression and AU values.",1,related,1,positive
"The masked-prediction pretraining is now also applied on the spectrogram [10, 26], after being applied as Masked Language Model (MLM) , BERT [3] in NLP and Masked Auto-Encoder (MAE) [8] in CV.",1,related,1,positive
"Thanks to triplet loss‚Äôs ability to enlarge the relative distance of embedding, it improves Pearson corr in both the multitask method(pretrain-MAE+triplet loss) and our method.",1,related,1,positive
Only the last row of table 3 includes additional data augmentations used during finetuning as in [1].,1,related,1,positive
"To address this issue, in this paper, we propose SatMAE, a self-supervised learning framework based on masked autoencoders (MAEs) [1] which naturally handles temporal and multi-spectral input data.",1,related,1,positive
"The decoder outputs a reconstructed image √é ‚àà RC√óH√óW , which is compared to the original image using the mean-squared error (MSE) loss, computed per-pixel only on the masked patches [1].",1,related,1,positive
"For problems in which there are no annotated data available, the solution is to use pretrained models [6], unsupervised segmentation methods, e.",1,related,1,positive
"We also consider two Transformer based selfsupervised pretraining methods, MOCO V3 (Chen et al., 2021) and MAE (He et al., 2021).",1,related,1,positive
"Following the asymmetrical design in [15], we also design a small decoder which is 50% narrower and shallower than the encoder.",1,related,1,positive
"Encoder Following ViT-MAE (He et al., 2022), the PIXEL encoder only operates on unmasked patches (i.e.,‚âà 396 patches at 25% masking) and a special CLS embedding with its positional encoding c = x[cls] +E0pos ‚àà R1√óDenc is prepended to the sequence: h0 = [c, xÃÉvis] ‚àà R(1+bR¬∑Nc)√óDenc .32 Let {hi}Li=1‚Ä¶",1,related,1,positive
"Following He et al. (2022), we use a masked autoencoder with a ViT architecture and a lightweight decoder for pretraining (left).",1,related,1,positive
"Encoder Following ViT-MAE (He et al., 2022), the PIXEL encoder only processes unmasked patches (i.",1,related,1,positive
"Encoder Following ViT-MAE (He et al., 2022), the PIXEL encoder only operates on unmasked patches (i.e.,‚âà 396 patches at 25% masking) and a special CLS embedding with its positional encoding c = x[cls] +E0pos ‚àà R1√óDenc is prepended to the sequence: h0 = [c, xÃÉvis] ‚àà R(1+bR¬∑Nc)√óDenc .32 Let {hi}Li=1 be the encoder hidden states after each of the L = 12 encoder transformer layers, and h0 denotes the input sequence.",1,related,1,positive
"Encoder Following ViT-MAE (He et al., 2022), the PIXEL encoder only processes unmasked patches (i.e., ‚âà 396 ‚Äúvisible‚Äù patches at 25% masking) rather than on a sequence including mask tokens, which not only reduces memory requirements and increases training speed, but also has the advantage of not‚Ä¶",1,related,1,positive
"Our Pixelbased Encoder of Language (PIXEL) is built on the Masked Autoencoding Visual Transformer (ViTMAE; He et al., 2022).",1,related,1,positive
"This mismatch would occur when training the encoder with inserted mask tokens because they are not inserted during finetuning (He et al., 2022).",1,related,0,negative
"Encoder Following ViT-MAE (He et al., 2022), the PIXEL encoder only operates on unmasked patches (i.",1,related,1,positive
"Encoder Following ViT-MAE (He et al., 2022), the PIXEL encoder only processes unmasked patches (i.e., ‚âà 396 ‚Äúvisible‚Äù patches at 25% masking) rather than on a sequence including mask tokens, which not only reduces memory requirements and increases training speed, but also has the advantage of not creating a mismatch between pretraining and finetuning.",1,related,1,positive
The regressor aims to recover the missing pixels as in [28].,1,related,1,positive
We use normalized pixels as the reconstruction target for groundtruth as MAE [28].,1,related,1,positive
"We benchmark four types of self-supervised models, including contrastive (MoCov2 [11]), clustering-based (SwAV [7], DINO [8]), feature de-correlation (BarlowTwins [67]), masked autoencoder (MAE [22], BeiT [1]) models.",1,related,1,positive
"We observe, similar as in MAE for images [1], that a high pre-training masking ratio (80% in our case) is optimal for audio spectrograms.",1,related,0,negative
"As in MAE [1], we add fixed sinusoidal positional embeddings to the embedded patches.",1,related,1,positive
"S2(a), utilizing a pretrained ViT encoder contributes to the improvements of FID and IS by 2.418 and 0.204, respectively.",1,related,0,negative
Transformer Encoder Our encoder is a standard ViT [9].,1,related,1,positive
"The pretrained ViT encoder is capable of capturing the long-term dependencies, which may benefit the patch prediction.",1,related,0,negative
Transformer Encoder and Decoder We compare the impact of the pretrained ViT-based encoder and the number of transformer decoder layers M .,1,related,1,positive
"We develop a novel hybrid query-based encoder-decoder transformer framework, named Query Outpainting TRansformer (QueryOTR), to extrapolate visual context all-side around a given image taking advantages of both ViT [9] and pure transformer [41] in the image outpainting task, as shown in Fig.",1,related,1,positive
"M FID‚Üì IS‚Üë - 4 22.784 3.751 X 2 20.731 3.931 X 4 20.366 3.955 X 8 20.373 3.852
(a) Ablation of the pretrained ViTbase encoder and the number of transformer decoder layers M .",1,related,1,positive
We initialise the weights of generator encoder by utilizing the pre-trained ViT [17].,1,related,1,positive
"Different from the augmentation techniques in the previous methods [51,5,50,31], we propose a self-supervised strategy by recovering original 2D inputs from the occluded motion map, which can learn an expressive motion representation for the occluded problem via the MIM task [4,18].",1,related,1,positive
"Additionally, we will combine the symmetric pattern in Swin-Unet [48] with the coding pattern in MAE [57], to retain sufficient vessel details and make the performance of our model more outstanding on each metric.",1,related,1,positive
"Moreover, we will further focus on feature learning using some state-of-the-art methods, such as MAE [57] and ViT [38].",1,related,1,positive
"The table shows the FTTA performance on DomainNet with representations from MAE (He et al., 2021).",1,related,1,positive
", 2021)-based encoder (trained using masked auto-encoders (He et al., 2021)) finetuned on the ImageNet dataset.",1,related,1,positive
"(iii) We conduct, for the first time, a FTTA experiment on the large-scale DomainNet (Peng et al., 2019) dataset, based on self-supervised representations from the recent ViT-based masked autoencoders (He et al., 2021).",1,related,1,positive
"Furthermore, we also experiment with a vision transformer(ViT) (Dosovitskiy et al., 2021)-based encoder (trained using masked auto-encoders (He et al., 2021)) finetuned on the ImageNet dataset.",1,related,1,positive
"To this end, we consider the encoder from MAE (He et al., 2021) fine-tuned on ImageNet as our feature extractor 3.",1,related,1,positive
"Here, following the MAE [63], we adopt the MIM-based pretext task for self-supervised pretraining both in steps (1) and (2).",1,related,1,positive
Mask-RCNN(MAE) [63] SSP(IN1K) ViT-B 66.,1,related,1,positive
"Subsequently, to ensure effectiveness, we keep the same setting of MAE [63] by utilizing 75% tokens randomly masked and then reconstructed",1,related,1,positive
"In our study, the process of task-agnostic representation of MIM follows [63], and it can be expressed by the following equations:",1,related,1,positive
", via pŒ∏(xÃÇT|xÃÇS) with xÃÇ = g(x) or pŒ∏(h(xT)|k(xS)) [21, 53], where g(¬∑), h(¬∑), and k(¬∑) are domain-knowledge-inspired transformations.",1,related,1,positive
"Following the MAE [21], we design the GAN generator pŒ∏(xT|xS) with an autoencoder-like architecture, which employs an encoding G-Encoder and a decoding G-Decoder, as shown in Fig.",1,related,1,positive
"MAE [21] MaskFeat [53] Eq(S,T)KL[q(h(xT)|xS)||pŒ∏(h(xT)|xS)] q(S,T) = U{(S,T) : S is a 25% random subset of L, and T = L\S pŒ∏(h(xT)|xS) = N (h(xT)|ŒºŒ∏(xS), I) h(¬∑) is a normalization/HOG transformation for MAE/MaskFeat",1,related,1,positive
"For each downstream task, we adopt various fine-tuning strategies [28, 27], including transferring features protocol, linear classification protocol and non-linear classification protocol.",1,related,1,positive
"MAE [He et al., 2021] is created based on method adopted by autoencoder to train a vision transformer.",1,related,1,positive
"Unlike MAE [12] that calculates loss only on masked patches, we will compute the loss on the entire point cloud.",1,related,1,positive
"Similar to MAE [12], we discard the masked patches directly and do not use any mask tokens in the encoder part, which will save the computational time and memory effectively.",1,related,1,positive
"Noting this challenge and the potential for improved effectiveness and applicability, in the work here, we thus propose (and also fully develop) a method based on the idea of masked autoencoders [16] which will utilize unlabeled data to do selfsupervision.",1,related,1,positive
"For the pre-training, we follow the paradigm of MAE [19] and equip the model with a lightweight decoder that is structurally similar to the encoder.",1,related,1,positive
"This can partially be motivated by the reduced time needed for pre-training, but we also find the encoder to achieve higher downstream task performance when trained in conjunction with a smaller decoder, similar to the results in [19].",1,related,0,negative
"After using the MAE method to pre-train a VIT-Base model on the pathological dataset, we obtain an accuracy of 83.02% and an F1-score of 0.8299 in patch classification.",1,related,0,negative
"1) unsupervised pre-training based on MAE: To pretrain the ViT model, we employ an encoder-decoder architecture with a pretext task of image reconstruction.",1,related,1,positive
Figure 8: Frames reconstructed with the masked autoencoders (MAE) [13] trained on Meta-world (Top) Coffee Pull and (Bottom) Peg Insert Side.,1,related,0,negative
"Specifically, we provide reconstructions from MAE [13] trained on Coffee-Pull and Peg-Insert-Side tasks from Meta-world [16] in Figure 8.",1,related,1,positive
com/view/mwm-rl which contains videos for (i) reconstructions from masked autoencoders (MAE) [13] and (ii) predictions from latent dynamics models.,1,related,1,positive
"To assist the training of mixed-supervised network, we adopted an effective sampling strategy (random masking) to optimize the training process, inspired by MAE [6].",1,related,1,positive
"Next, we deviate from the common autoregressive training objective and pre-train a bidirectional transformer with window-restricted attention via masked visual modeling (MVM).",1,related,1,positive
"Unlike prior work on MVM [42, 40] that uses a fixed masking ratio, we propose to use a variable masking ratio that reduces the gap between pre-training task and inference leading to better evaluation results (see ¬ß 3.",1,related,1,positive
"The pre-training objective is to minimize the negative log-likelihood of the visual tokens given the masked video as input: LMVM = ‚àí E
x‚ààD
[‚àë ‚àÄi‚ààNM log p(zi|ZMp , Zc) ] , where D
is the training dataset, NM represents randomly masked positions, and ZMp denotes the output of applying the mask to Zp, and Zc are latent tokens corresponding to context frames.",1,related,1,positive
"Let Zp = [zi]Ni=1 denote the latent tokens corresponding to future video frames, where N = Tp√óh√ów. Unlike prior work on MVM [42, 40] that uses a fixed masking ratio, we propose to use a variable masking ratio that reduces the gap between pre-training task and inference leading to better evaluation results (see ¬ß 3.4).",1,related,1,positive
"Inspired by the success of masked language [8] and image [42, 40] modeling, and in the spirit of unifying methodologies across domains, we pre-train MaskViT via MVM for video prediction.",1,related,1,positive
The MVM training objective is different from the causal autoregressive training objective as the conditional dependence is bidirectional: all masked tokens are predicted conditioned on all unmasked tokens.,1,related,1,positive
"Our model can surpass the most competitive MAE [19] with a clear margin, i.",1,related,0,negative
We follow MAE [19] and adopt an encoder-decoder structure to perform MIM.,1,related,1,positive
"In this paper, we follow MAE [19] to adopt the most simple and intuitive raw pixels regression.",1,related,1,positive
"Inspired by the excellent performance of masked autoencoding [9, 18, 26], we design the masked voxel autoencoding network for 3D perception.",1,related,1,positive
MAE Masked autoencoders.,1,related,0,negative
"Motivated by the above analyses and recent computer vision models [9], especiallyMasked AutoEncoder (MAE) [13], we propose a masked autoencoding model for time series based on Transformer blocks (i.e., TSFormer).",1,related,1,positive
"Specifically, we design an efficient unsupervised pre-training model for Time Series based on TransFormer blocks [33] (TSFormer), which is trained through the masked autoencoding strategy [13].",1,related,1,positive
"Unlike MAE [13], we no longer add positional embeddings here since all patches already have positional information added in the encoder.",1,related,1,positive
"Moreover, unlike the deterministic, sinusoidal embeddings used in MAE [13], we use learnable positional embeddings.",1,related,1,positive
"Motivated by the above analyses and recent computer vision models [9], especiallyMasked AutoEncoder (MAE) [13], we propose a masked autoencoding model for time series based on Transformer blocks (i.",1,related,1,positive
"We use uniform distribution to initialize the positional embeddings, and we use truncated normal distribution with ùúá = 0 and ùúé = 0.02 to initialize the mask token, similar to MAE [13].",1,related,1,positive
"We evaluate the performances of all baselines by three commonly used metrics in multivariate time series forecasting, including Mean Absolute Error (MAE), Root Mean Squared Error (RMSE) and Mean Absolute Percentage Error (MAPE).",1,related,1,positive
"Inspired by [18, 17], we added the whole initial embedding patches back to the last layer‚Äôs embedding patches to retain global information and maintain the correlations of all patches.",1,related,1,positive
"Notably, we achieve this without the extensive data augmentations methods typically used by SOTA models (Yu et al., 2022a; He et al., 2022).",1,related,1,positive
", running masked auto encoder models like MAE[22] is not straightforward).",1,related,1,positive
"For instance, the distribution of the token values is relatively robust compared to CNNs when we mask (drop) 75% of the tokens in masking auto-encoder (MAE[22]).",1,related,1,positive
We propose to use the state-of-the-art self-supervised learning framework masked autoencoders (MAE) [10] to extract the feature representations and initialize the feature space.,1,related,1,positive
"Specifically, we use the self-supervised masked autoencoders (MAE) [10] to map all instances into an initial feature space and iteratively refine it for better data distribution.",1,related,1,positive
"For the MAE, we only modified its input size of the ViT model in its encoder and decoder (from 224 to 512), and other model details are the same as those in [10].",1,related,1,positive
"Following MAE, our Masked Autoencoder for Generic Event Boundary Detection(MAE-GEBD) is inspired by the recent ImageMAE [5] and Spatiotemporal-MAE [3].",1,related,1,positive
"We compare our model with [1], [2], [24] using objectlevel appearance frames on Avenue dataset.",1,related,1,positive
"Moreover, we are inspired by MAE[24] for the simultaneous multi-frame prediction.",1,related,1,positive
"Following [40], we feed the non-masked N ‚àíM patches (and their positions) to the encoder to produce per-patch embeddings.",1,related,1,positive
"As we follow [40] and only pass unmasked patches to the encoder and have a lightweight decoder, extreme masking leads to a dramatically lower computational cost for training the encoder, and consequently the model as a whole.",1,related,1,positive
"We start from the default 75% masking for each modality [40], and increase it upto 90% for images and 95% for videos.",1,related,1,positive
"Following [40], we re-train OmniMAE without normalizing the pixel targets to obtain easy to visualize RGB reconstructions.",1,related,0,negative
"We compare OmniMAE‚Äôs representations trained jointly on images and videos to MAE, trained solely on images [40].",1,related,1,positive
"In particular, we consider the Masked Auto-Encoding (MAE) approach [40] to train an Omnivorous visual encoder [33].",1,related,1,positive
"The architecture of our encoder is quite flexible since we do not insert any mask tokens on the corrupted non-overlapping patch embeddings as in MIM [1, 29, 71, 68].",1,related,1,positive
We believe that our MFM can also complement recent MIM approaches to further improve the performance.,1,related,0,negative
"In this paper, we mainly use a standard ViT [22] as our encoder for a direct comparison with MIM methods.",1,related,1,positive
"Our work differs from previous approaches in that we perform masking in the frequency domain, which relies on none of the following: (i) extra data [1, 21, 24], (ii) extra model [79, 23, 24, 57, 10], or (iii) mask token [1, 29, 71, 68, 10].",1,related,1,positive
"We use an output dimension of 8192 for the projection heads across all models, and employ random Masked Image Modelling with prediction ratios (0, 0.3) and variances (0, 0.2).",1,related,1,positive
To overcome the lack of such fine-grained annotations we employ self-supervised training with Masked Image Modelling as pretext task [59] and use a Vision Transformer architecture [10] as encoder due to its patch-based nature.,1,related,1,positive
"Moreover, instead of replacing the masked regions with [MASK] tokens[17, 58], we use the feature patches of teachers to fill the masked regions of students, thereby making our distillation teacher knowledge-aware.",1,related,0,negative
"We also consider grid-wise masking, which regularly retains one of every four patches, similar to MAE [17].",1,related,1,positive
"We also demonstrate that our proposed method, SERE, outperforms and complements various masked image modeling (MIM) based methods.",1,related,1,positive
"Additionally, we strengthen the ability to model inter-channel relations, which MIM is missing.",1,related,0,negative
"Next, rather than training the explainer from scratch, we fine-tune an existing ViT: we can use the original classifier or the surrogate as an initialization, or we can use a ViT pre-trained on a different supervised or self-supervised learning task [17, 58, 24].",1,related,1,positive
"Then, the unmasked portion is embedded with the information about their position in the original image, which then goes to Transformer blocks to extract the image features [68].",1,related,1,positive
The encoder employs a masked autoencoder (MAE) with vision Transformer (ViT) architecture [68].,1,related,1,positive
"Then, the pretrained masked autoencoder (MAE) [24] completes the original image relying on the visible image patches.",1,related,1,positive
"In this paper, we closely follow the model architecture of MAE [24].",1,related,1,positive
We pretrain the autoencoder module on ImageNet [15] for 200 epochs following the hyper-parameters of MAE [24].,1,related,1,positive
"1, we first revisit the pretraining framework based on masked autoencoder [24].",1,related,1,positive
"In detail, we pretrain an extremely light-weight autoencoder via a self-supervised mask-reconstruct strategy [24].",1,related,1,positive
"Closely following the recent selfsupervised method MAE [24], we first divide the images into patches and mask out a set of patches from the input images, meaning only portions of the images are input to the autoencoder.",1,related,1,positive
"Inspired from this, we adapt SAC+AE into SAC+MAE by replacing the augmented input image with its masked version, and only penalizing the reconstruction error for the masked patches.",1,related,1,positive
For MAE we start from augmented SAC+AE and first divide the augmented image into nonoverlapping patches in the spatial domain with a size of 4 √ó 4.,1,related,1,positive
"For transformers, we leverage pretrained models on ImageNet [16] from ViT [17], DeiT [58], DINO [4], MoCo-v3 [11], and MAE [26] (which makes an interesting comparison as it is based on reconstruction rather than contrasting).",1,related,1,positive
"Aside from addressing the above limitations, we also point out that our methods could be extended in a number of directions, including: 1) training the whole network in a self-supervised way, perhaps using recent advances in transformers for vision [58, 59] and contrastive approaches for neural activity [34, 39]; 2) improving domain transfer through structured transport methods [60].",1,related,1,positive
"Our model is even comparable to MAE, which is trained with a much heavier schedule (300 epochs vs. 1600 epochs).",1,related,0,negative
"We compare with representative contrastive methods MoCo-v3 [16] and DINO [15], as well as masked image modeling methods BeiT [2] and MAE [11] on ImageNet classification.",1,related,1,positive
The evaluation protocol mainly follows BEiT and MAE.,1,related,0,negative
"This allows us to train ViT-Base models of 300 epochs using a single node of 8√óV100 GPUs for 29 hours
Table 9: Semantic segmentation on ADE20K.
methods epochs mIoU DINO 300 47.2
MoCo-v3 300 47.3 BEiT 800 47.1 MAE 1600 48.1 ExtreMA (1k) 300 47.9 ExtreMA (22k) 30 48.4
Table 10: Semi-supervised classification.
methods epochs 1% 10% scratch - 9.0 44.8 BEiT 800 35.9 69.7 DINO 400 64.7 75.9
MoCo-v3 300 57.2 75.8 MAE 1600 52.7 72.1 ExtreMA (1k) 300 67.3 76.1
to 60 hours depending on the choice of multi-masking.",1,related,1,positive
"We vary the masking ratio and compare results with supervised ViT [41], DINO [15] and MAE [11] using this inversion method.",1,related,1,positive
"Specifically, we use ViT-B/16, ViT-L/16 and ViTH/14 according to the settings from [18].",1,related,1,positive
"Similar to SwinV2 models pre-trained with SimMIM, we observe the same
overfitting phenomenon when training with small datasets or large models, which makes MAE still demand for large-scale data.",1,related,1,positive
We also conduct experiments with other MIM frameworks like MAE [18] and other vision encoder like the widely used ViT [13] to verify the generalizability of our findings.,1,related,0,negative
"In addition, we conduct experiments using MAE as the masked image modeling approach to verify the methodological generalizability of our findings.",1,related,1,positive
"ImageNet-1K We follow [3] to evaluate the quality of learnt representations by fine-tuning the pre-trained models on ImageNet-1K [11] image classification task, which is the most commonly used scenario and evaluation criterion for pre-trained models [18, 44].",1,related,1,positive
"In this work, we use SimMIM as the default masked image modeling approach, because of its simplicity and no restrictions on the architecture of vision encoder like MAE.",1,related,1,positive
Here we mainly consider the masked autoencoder (MAE) structure He et al. (2021).,1,related,1,positive
"We use SimMIM rather than MAE (He et al., 2021), as MAE removes the masked patches before encoder but the convolution operations in CNN encoder cannot handle masked input, while SimMIM replaces the masked patches by a mask token and can use CNNs.",1,related,1,positive
"In this work, we are particularly interested in the recently proposed mask-reconstruction pretraining (MRP) of SSL families (Xie et al., 2021; Dong et al., 2021), e.g. MAE (He et al., 2021) and data2vec (Baevski et al., 2022).",1,related,1,positive
"After pretraining, following the practice in (Wei et al., 2021; Dong et al., 2021; He et al., 2021; Xie et al., 2021; Baevski et al., 2022), we only fine-tune the student encoder with an extra linear layer on the labeled training data of the downstream datasets.",1,related,0,negative
"Therefore, we think the unsatisfactory performance of the fine-tuned model is because MAE is unsuitable as an M3DO pre-training task.",1,related,0,negative
"A.4 MAE Reconstruction Results of CNN
In Section 5.2, we pre-train a model on KITTI-raw using MAE.",1,related,1,positive
"To confirm which one is the real cause, we randomly mask 80% pixels of images from the KITTI-3D validation set, and use the model pre-trained on KITTI-raw with the MAE pre-training task to recover them.",1,related,0,negative
"Following Bao et al. (2022); He et al. (2021), we use a layer-wise learning rate decay (Clark et al., 2020) of 0.65 for both ViT models.",1,related,1,positive
"Both encoders in the siamese encoder share the same weights, and we adopt the vision transformer (ViT) [2,6] and convolutional neural network (CNN) [7] as the backbone of the siamese encoder.",1,related,1,positive
"We apply the AdamW optimizer [11] to train our model, in which CNN encoders adopt ResNet34-3D [7] and are trained from scratch, and ViT encoders are trained on the pre-trained model which uses the MAE method [6].",1,related,1,positive
"Here, m indicates the learnable embedding of the mask token, which follows [24].",1,related,1,positive
"We further apply sin(¬∑) and cos(¬∑) operators to get the 2-D sin-cos positional embeddings, following the practice in MAE [24].",1,related,1,positive
"For masking strategy, we follow MAE [24] to use random masking with a masking ratio of 75%.",1,related,1,positive
"Using the MAE pretraining method, we found the VIT could be well initialized with dataset-specific hidden representation, which significantly improved the result in the small dataset.",1,related,1,positive
"Specifically, when using MAE as the pretraining method, the performance of VIT was further improved with the DICE value of 0.895 and outperformed CNNs by 2%.",1,related,0,negative
"Instead of using the weight pretrained in the ImageNet directly, we adopt a pretraining method, namely Masked AutoEncoder (MAE) [29].",1,related,1,positive
We follow the same experimental settings as MAE [24]; detailed hyperparameters can be found in the supplementary material.,1,related,0,negative
"The Masked Autoencoder (MAE) model [24], shown on the left side of Fig.",1,related,1,positive
"Figure 1: We visualize the attention patterns employed by MAELarge [24] in the reconstruction of a random target patch, indicated by orange.",1,related,1,positive
"Thereby, we focus on VITs to test the effectiveness of our method (see footnote3 for more reasons), similar to [3, 10, 16, 26].",1,related,0,negative
"ViTs are very big (‚â´ResNets) and tends to overfit [15, 26, 52], so we choose ViTs rather than ResNets.",1,related,1,positive
"1Following [26], we refer to semantics as visual concepts, e.",1,related,1,positive
"In viewing patch-based sequence modeling of WSIs in relation to ViTs, we note that the architectural design choice of using Transformer attention enables pretraining of both the tokenization and aggregation layers in ViT models, which is important in preventing MIL models from over- or underfitting in low-data regimes [5, 13, 23, 33, 46].",1,related,1,positive
We choose MAE [20] with ViT base [13] backbone as our baseline.,1,related,1,positive
"6.1 Experimental setup
We choose MAE [20] with ViT - base [13] backbone as our baseline.",1,related,1,positive
"B.3 Masked Image Modeling
B.3.1 Pre-training
We employ MAE [20] with ViT-base [13] backbone as our baseline.",1,related,1,positive
", the encoder), and appends a decoder to it, aiming at recovering the original image contents, either tokenized features [1] or pixels [20], at the end of the decoder.",1,related,1,positive
"6% over ViT-B (using MAE [20], pre-training for 1600 epochs) and +0.",1,related,0,negative
"We are interested in a particular generation-based method named masked image modeling (MIM) [1, 20].",1,related,1,positive
"In Figure 4, we show the results of training from scratch and uptraining from the MAE checkpoint [27].",1,related,0,negative
"Our experimental setup on MAE-lite also largely follows those of MAE [8] which include optimizer, learning rate, batch size, argumentation, etc.",1,related,1,positive
"We introduce MAE-lite to facilitate our study, which largely follows the design of MAE [8] except that the encoder is altered to ViT-Tiny.",1,related,0,negative
"We further examine a large pre-trained model, MAE-Base [8], and find it achieves a better alignment to DeiT-Tiny, as shown in the left column of Fig.",1,related,1,positive
"Following He et al. (2021), we mask a large portion of patches (e.g., 75%).",1,related,0,negative
"Following prior work (He et al. 2021; Devlin et al. 2018), we compute the loss only on masked patches.",1,related,1,positive
"Among them, the Masked Autoencoder (MAE) (He et al. 2021) is the stateof-the-art method that adopts a BERT-type masked autoencoding scheme (Devlin et al.",1,related,1,positive
"We compare SupMAE with three supervised counterparts: ViT (Dosovitskiy et al. 2020), DeiT (Touvron et al. 2021) and naive supervised results from He et al. (2021).",1,related,1,positive
"We use a one-layer transformer decoder for SupMAE, unlike an 8-layer transformer decoder for MAE (He et al. 2021).",1,related,1,positive
"In this section, we compare the robustness performance of MAE (He et al. 2021), DeiT (Touvron et al.",1,related,1,positive
We follow the hyperparameters in MAE (He et al. 2021).,1,related,1,positive
"Among them, the Masked Autoencoder (MAE) (He et al. 2021) is the stateof-the-art method that adopts a BERT-type masked autoencoding scheme (Devlin et al. 2018).",1,related,1,positive
"We basically follow MAE
(He et al. 2021) for the setup and training hyper-parameters.",1,related,1,positive
"For a fair comparison, we follow the same training configuration and code as MAE (He et al. 2021) with MMSegmentation (MMSegmentation 2020) framework.",1,related,0,negative
"Naive supervised indicates the supervised pre-training done from scratch, in which we directly use the reported mIoU from He et al. (2021).",1,related,0,negative
"Robustness evaluation on ImageNet variants In this section, we compare the robustness performance of MAE (He et al. 2021), DeiT (Touvron et al. 2021) and our SupMAE on four ImageNet variants.",1,related,1,positive
"Overview of our ObjMAE based on MAE (He et al., 2021) for fast pre-training.",1,related,0,negative
"Unlike MAE (He et al.,
2021) that shuffles or unshuffles the token list, we gather the vector of target patches in the correct order.",1,related,1,positive
"Unlike MAE (He et al., 2021) that shuffles or unshuffles the token list, we gather the vector of target patches in the correct order.",1,related,1,positive
"For occlusion, we consider the patch based random masking as adopted in most MIM works [21, 51, 44].",1,related,1,positive
"For the masking strategy, we follow the common practice of existing works [16, 21, 51, 44] where the input image is divided into non-overlapping patches, and a random subset of patches are masked.",1,related,1,positive
"For evaluation on Transformer, we follow MAE [21], which efficiently fine-tunes Mask R-CNN with ViT-B backbone using 1√ó schedule.",1,related,1,positive
We experiment on ADE-20K [58] using UperNet [48] following MAE [21] to fine-tune the model for 100-epoch with the batch size of 16.,1,related,1,positive
"For evaluation on Transformer, we employ the fine-tuning as MAE [21], which uses DeiT [39] augmentation setting, an AdamW optimizer for 100-epoch training, and adopt a layer-wise learning rate decay of 0.",1,related,1,positive
"Similar to MAE [17] and BERT [10], we compute the loss only on the masked image",1,related,1,positive
"Following MAE [17], our decoder is lightweight and has 8 blocks of width 512.",1,related,1,positive
"For image patches, we use a learnable linear projection to convert them to image embeddings that have the same dimension as the language embeddings, and then apply 2D positional encodings, following the practice of MAE [17].",1,related,1,positive
"Following MAE [17], we use a lightweight Transformer-based decoder on the full set of tokens consisting of (i) encoded visible image patches, (ii) encoded visible text tokens, and (iii) mask tokens.",1,related,1,positive
"Thus, while our results cannot be directly compared to the original MAE results [17] pre-trained on ImageNet due to distribution mismatch, they demonstrate the strengths of multimodal training of M3AE for learning transferable representations across datasets.",1,related,1,positive
"We consider 5 pre-training methods of DINO [3], EsViT [26], CLIP [34], DeiT [36], and MAE [17], using their public checkpoints.",1,related,0,negative
We follow [17] to use the LARS optimizer [48] with a base learning rate of 0.,1,related,1,positive
"We made the following reflections to these approaches:
‚Ä¢ Masked image modeling (MIM).",1,related,1,positive
"Following the prior work MAE [22], we use a lightweight decoder that consists of nd (by default nd = 1) transformer blocks with an embedding dimension of 512.",1,related,1,positive
"We use MAE [43] as our self-supervised pre-training method in the image domain, a simple yet effective method that first masks nearly 75% patches of the input image and then reconstructs the missing pixels.",1,related,1,positive
"Our default configurations follow the linear probing settings in [21, 43], which do not utilize many common regularization strategies, such as mixup [94], cutmix [91], color jittering and so on.",1,related,1,positive
"We use VideoMAE [78] as our self-supervised pre-training method in the video domain, which is an direct extension of MAE to the video domain.",1,related,1,positive
"For video, we take both supervised and self-supervised pre-trained models from VideoMAE [78].",1,related,0,negative
"Specifically, for image, we directly use the ImageNet-21k [26] supervised pre-trained model3 and MAE [43] self-supervised model4.",1,related,1,positive
"Inspired by [12, 11], we add the whole initial embedding patches back to the last layer‚Äôs embedding patches to retain global information and maintain the correlations of all patches.",1,related,1,positive
"For self-supervised models, we use the linear probing result as the IN accuracy as reported in their paper [59, 66, 57].",1,related,1,positive
"In the training stage, we adapt masked autoencoders (MAE) [21] to reconstruct a new dataset from randomly masked face dataset.",1,related,1,positive
"Then, we briefly revisit the masked autoencoders (MAE) [21] and formally introduce masked face reconstruction.",1,related,1,positive
"The decoder fD maps the latent code H back to the input X , and its design would depend on the semantic level [12] of target X .",1,related,1,positive
"For large and huge models, we fine-tune them for 50 epochs following existing work (Bao et al., 2021; He et al., 2021).",1,related,1,positive
"For training ViT from scratch, we report the original results (Dosovitskiy et al., 2020) and the results with strong data augmentation (He et al., 2021).",1,related,0,negative
"To compare these two objectives, we use supervised vision transformer (ViT) (Dosovitskiy et al., 2020) and vision masked autoencoder (MAE) (He et al., 2021) as two representative platforms to show our insights.",1,related,1,positive
"For supervised learning models (i.e., ViT), we train base scale models for 300 epochs and large scale models for 200 epochs following He et al. (2021).",1,related,1,positive
"We compared CMAE with MOCO-v1 [16], MOCO-v2 [5], and MAE [15].",1,related,1,positive
proposed a new auto-encoder called Masked AutoEncoders [15] for training transformer by partially masking the image.,1,related,1,positive
"In this work, we seek to continue the success of MLM and MIM by introducing masked graph modeling (MGM) as a principled pretext task for graph-structured data.",1,related,1,positive
"In addition, MaskGAE also outperforms the state-of-the-art method GraphMAE on six out of seven datasets.",1,related,1,positive
"In this section, we present the MaskGAE framework for the MGM pretext task, which is developed by taking inspiration fromMLM [4] and MIM [11].",1,related,1,positive
"Since GraphMAE is not initially designed for link prediction tasks, we instead trained an MLP decoder on the learned representations for GraphMAE to solve the link prediction task.",1,related,1,positive
"Cora CiteSeer Pubmed Photo Computer arXiv MAG
MLP 47.90 ¬± 0.40 49.30 ¬± 0.30 69.10 ¬± 0.20 78.50 ¬± 0.20 73.80 ¬± 0.10 56.30 ¬± 0.30 22.10 ¬± 0.30 GCN 81.50 ¬± 0.20 70.30 ¬± 0.40 79.00 ¬± 0.50 92.42 ¬± 0.22 86.51 ¬± 0.54 70.40 ¬± 0.30 30.10 ¬± 0.30 GAT 83.00 ¬± 0.70 72.50 ¬± 0.70 79.00 ¬± 0.30 92.56 ¬± 0.35 86.93 ¬± 0.29 70.60 ¬± 0.30 30.50 ¬± 0.30 GAE 74.90 ¬± 0.40 65.60 ¬± 0.50 74.20 ¬± 0.30 91.00 ¬± 0.10 85.10 ¬± 0.40 63.60 ¬± 0.50 27.10 ¬± 0.30 VGAE 76.30 ¬± 0.20 66.80 ¬± 0.20 75.80 ¬± 0.40 91.50 ¬± 0.20 85.80 ¬± 0.30 64.80 ¬± 0.20 27.90 ¬± 0.20 ARGA 77.95 ¬± 0.70 64.44 ¬± 1.19 80.44 ¬± 0.74 91.82 ¬± 0.08 85.86 ¬± 0.11 67.34 ¬± 0.09 28.36 ¬± 0.12 ARVGA 79.50 ¬± 1.01 66.03 ¬± 0.65 81.51 ¬± 1.00 91.51 ¬± 0.09 86.02 ¬± 0.11 67.43 ¬± 0.08 28.32 ¬± 0.18 GraphMAE 84.20 ¬± 0.40 73.40 ¬± 0.40 81.10 ¬± 0.40 93.23 ¬± 0.13 89.51 ¬± 0.08 71.75 ¬± 0.17 32.25 ¬± 0.37 DGI 82.30 ¬± 0.60 71.80 ¬± 0.70 76.80 ¬± 0.60 91.61 ¬± 0.22 83.95 ¬± 0.47 65.10 ¬± 0.40 31.40 ¬± 0.30 GMI 83.00 ¬± 0.30 72.40 ¬± 0.10 79.90 ¬± 0.20 90.68 ¬± 0.17 82.21 ¬± 0.31 68.20 ¬± 0.20 29.50 ¬± 0.10 GRACE 81.90 ¬± 0.40 71.20 ¬± 0.50 80.60 ¬± 0.40 92.15 ¬± 0.24 86.25 ¬± 0.25 68.70 ¬± 0.40 31.50 ¬± 0.30 GCA 81.80 ¬± 0.20 71.90 ¬± 0.40 81.00 ¬± 0.30 92.53 ¬± 0.16 87.85 ¬± 0.31 68.20 ¬± 0.20 31.40 ¬± 0.30 MVGRL 82.90 ¬± 0.30 72.60 ¬± 0.40 80.10 ¬± 0.70 91.70 ¬± 0.10 86.90 ¬± 0.10 68.10 ¬± 0.10 31.60 ¬± 0.40 BGRL 82.86 ¬± 0.49 71.41 ¬± 0.92 82.05 ¬± 0.85 93.17 ¬± 0.30 90.34 ¬± 0.19 71.64 ¬± 0.12 31.11 ¬± 0.11 SUGRL 83.40 ¬± 0.50 73.00 ¬± 0.40 81.90 ¬± 0.30 93.20 ¬± 0.40 88.90 ¬± 0.20 69.30 ¬± 0.20 32.40 ¬± 0.10 CCA-SSG 83.59 ¬± 0.73 73.36 ¬± 0.72 80.81 ¬± 0.38 93.14 ¬± 0.14 88.74 ¬± 0.28 69.22 ¬± 0.22 27.57 ¬± 0.41 MaskGAEùëíùëëùëîùëí 83.77 ¬± 0.33 72.94 ¬± 0.20 82.69 ¬± 0.31 93.30 ¬± 0.04 89.44 ¬± 0.11 70.97 ¬± 0.29 32.75 ¬± 0.43 MaskGAEùëùùëéùë°‚Ñé 84.30 ¬± 0.39 73.80 ¬± 0.81 83.58 ¬± 0.45 93.31 ¬± 0.13 89.54 ¬± 0.06 71.16 ¬± 0.33 32.79 ¬± 0.32
BGRL [33], SUGRL [20], and CCA-SSG [48].",1,related,0,negative
"[29, 23], we add the embeddings of all image patches before the first encoder layer to the input of the last encoder layer as in Figure 2.",1,related,1,positive
"[23], we add an image patch mask (detailed in",1,related,1,positive
"Different from masking random patches in MAE [23], our design masks the task-irrelevant ones to focus on the task-relevant prior knowledge for better generalizability.",1,related,1,positive
We follow MAE [19] where the operation units are 16√ó 16 image patches.,1,related,1,positive
"We follow the masking strategy of MAE [16], i.",1,related,1,positive
"We achieve much better performance than MAE [16], especially on the ViT-Small backbone.",1,related,0,negative
"To accelerate training, we follow MAE (He et al., 2022) and skip the mask token [MASK] in the encoder and only apply it in the lightweight decoder.",1,related,1,positive
"To this end, we experiment with our VLC checkpoint as well as other three pretrained checkpoints: ViT (Dosovitskiy et al., 2021), ViLT (Kim et al., 2021) and MAE (He et al., 2022).",1,related,0,negative
"In contrast, we follow MAE (He et al., 2022) to randomly mask image patches with a probability of 0.",1,related,1,positive
", 2021) initialized from MAE (He et al., 2022) (ImageNet-1K without labels) as our Transformer backbone; (3) Task-specific decoder.",1,related,1,positive
"In contrast, we follow MAE (He et al., 2022) to randomly mask image patches with a probability of 0.6, and reconstruct the missing pixels based on both non-masked tokens w\m and patches v\m.",1,related,1,positive
"We show that this is not necessary and introduce a new model Vision-Language from Captions (VLC) built on top of Masked Auto-Encoders (He et al., 2022) that does not require this supervision.",1,related,1,positive
"An unsupervised visual semantics is learned via Masked Auto-Encoders (He et al., 2022) before language is integrated.",1,related,1,positive
"To this end, we choose encoder-decoders pre-trained by MAE [11] and migrate them to conventional two-stage detectors, e.",1,related,1,positive
The extracted features are then embedded with location information by summarizing with position embeddings [11].,1,related,1,positive
"1(lower), imTED employs the ViT encoder pre-trained with MAE [11] as backbone, and uses the decoder as the detector head.",1,related,1,positive
"Although there are numerous papers on zero-shot learning (ZSL) in CV and NLP [33, 34, 35], we notice that ZSL was hardly mentioned in tabular domain.",1,related,1,positive
"Our study suggests that the general framework of masked autoencoding (BERT [15], MAE [31], etc.",1,related,1,positive
"Following this philosophy, we study extending Masked Autoencoders (MAE) [31] to the problem of spatiotemporal representation learning.",1,related,1,positive
"In 2021, as an extensible SSL method, Mask Autoencoder (MAE) achieved SOTA results in ImageNet dataset [14].",1,related,1,positive
"For example, simply replacing the DeiT weights [71] with the MAE weights [27] gives us an extra gain of 0.8 APb and 0.5 APm.",1,related,1,positive
"Specifically, we also use the MAE [27] pre-trained weights to initialize the ViTB, and apply the upgraded Mask R-CNN [28] and a stronger training formula (i.e., large scale jitter [25] and cosine learning rate decay).",1,related,1,positive
"Specifically, we also use the MAE [27] pre-trained weights to initialize the ViTB, and apply the upgraded Mask R-CNN [28] and a stronger training formula (i.",1,related,1,positive
"For example, simply replacing the DeiT weights [71] with the MAE weights [27] gives us an extra gain of 0.",1,related,1,positive
"Specifically, we integrate two popular masked image modeling methods, BEiT [29] and MAE [30], into our generalized federated framework.",1,related,1,positive
"We implement two popular masked image modeling methods, BEiT [29] and MAE [30], as the SSL module in our federated framework.",1,related,1,positive
"Specifically, we implement two masked image modeling methods, BEiT [29] and MAE [30], as the SSL module in our federated framework.",1,related,1,positive
"Furthermore, by equipping a masked autoencoder (MAE) [18] as a backbone, our proposed model is more robust in capturing occluded text instance regions, which makes it more suitable for visual place recognition.",1,related,1,positive
It is worth mentioning that we use a pre-trained MAE [18] (ViT-Base/16) as the backbone for feature extraction.,1,related,1,positive
"Model Recall 0.2 0.4 0.6 0.8 0.9 Our model 1 1 1 0.97 0.93 TextPlace 1 1 1 0.96 0.91 NetVLAD-10 1 1 1 0.95 0.93 NetVLAD-20 1 1 1 0.91 0.87 NetVLAD-30 1 1 0.97 0.85 0.83 ToDayGAN-10 0.50 0.55 0.58 0.57 0.56 ToDayGAN-20 0.40 0.40 0.40 0.38 0.38 ToDayGAN-30 0.26 0.24 0.24 0.25 0.24 FAB-MAP-10 0.79 0.69 0.67 0.65 0.63 FAB-MAP-20 0.76 0.69 0.67 0.63 0.60 FAB-MAP-30 0.68 0.67 0.67 0.62 0.58 SeqSLAM 0.30 0.24 0.18 0.13 0.13
has leveraged a robust and SOTA backbone of pre-trained MAE and a modified multi-task transformer detector.",1,related,1,positive
"Following recent advancement of self-supervised learning for Vision Transformers [11], we would like to test and scale up the CV4Code transformer with the abundance of unlabelled sourcecode snippets in the public domain.",1,related,1,positive
"Given content representation z, to reconstruct the patches in the i-th domain, we feed both the content representation z and the learnable masked tokens [17] into the i-th domain specific decoder Gi, i.",1,related,1,positive
"Similar to MAE [17], our content encoder also follows the vision transformer design, which extracts content representations only by visible patches.",1,related,1,positive
"Alternatively, based on ViT [12] framework, we optimize the objective function on both pixel-level reconstruction [4, 16, 35] and features-level regression [2] to predict the content of masked regions.",1,related,1,positive
"x = {xi : i / ‚àà M}i=1 ‚à™ {ei : i ‚àà M}i=1, where e is the learnable embedding replacing for masked patches, we feed only the unmasked patches xU = {xi : i / ‚ààM}i=1 which is similar to MAE [16].",1,related,1,positive
Mask Autoencoders (MAE) [26] is the recent representative self-supervised method for training ViT.,1,related,0,negative
"Figure 9 supplements our analysis of self-supervised methods with the recent MAE models of (He et al., 2021).",1,related,1,positive
We fine-tune the MAE [22] using cropped characters of SynthText [66] and applied it to the occluded characters.,1,related,1,positive
"We train all our final STR and scene text spotting models on 4 GPUs of NVidia A100, and we use a pre-trained encoder backbone of MAE (ViT-Base/16) [22] and fine-tune",1,related,1,positive
"We utilize MAE [22, 23] as our feature extraction backbone, which is based on a standard Transformers [49] architecture, namely ViT [64].",1,related,1,positive
"In our proposed scene recognition pipeline, we first fine-tune the pre-trained version of MAE (ViT-Base) [22] on 36 classes of cropped characters of Synth-Text [66], We then remove the decoder,",1,related,1,positive
"Next, we use a pre-trained encoder model of MAE [22] that use a ViT Transfomer (ViT B/16) as our Network‚Äôs backbone to extracts the 2D features from these input patches.",1,related,1,positive
"Downstream Image End-to-end Fine-tuning In addition to the five downstream tasks mentioned in the main text, following MAE [50], we also conduct the end-to-end fine-tuning experiments on downstream classification task.",1,related,1,positive
"In addition, to encourage the networks to more focus on context information, we artificially hide a patch Ipatch in the image I and make the networks recover the patch, which is proven to be effective in vision tasks [69, 70].",1,related,1,positive
"[23] for image self-supervised learning, we present masked spectrogram prediction (MaskSpec), a pre-training objective that directly recovers the masked patches of spectrogram.",1,related,1,positive
"We explore different reconstruction targets for MVM in video-text pre-training including raw frame pixels as in [21], discrete visual tokens from a learned image ‚Äútokenizer‚Äù [39] as in [7], and the textaligned features in this work.",1,related,1,positive
"A reasonably deep decoder can help make latent representations from the encoder output more abstract (He et al., 2022; Cao et al., 2022).",1,related,1,positive
"Figure 1: MAE (He et al., 2022) pre-training ow; we redraw Figure 1 in the MAE paper, in which we replaced the input image with a spectrogram and added loss calculation ow.",1,related,1,positive
"In addition, we reconstruct each pixel on the masked image including the masked pixel and the unmasked pixel, which is similar to the [He et al., 2021] method.",1,related,1,positive
"Motivated by a recent paper [He et al., 2021] which restores missing pixels of an image, we devise a new adversarial defense scheme called a Mask-based Adversarial Defense training method (MAD) for DNNs that classify images.",1,related,1,positive
04 MAE [19] Autoregressive Transformer 77.,1,related,1,positive
"For autoregressivebased method, we choose MAE [19].",1,related,1,positive
"Thus, in this work, we aim to maximize the performance of the model on downstream tasks with an end-to-end fine-tuning protocol [3, 9, 19].",1,related,1,positive
"Following the commonly adopted fine-tuning (FT) protocol [17, 19] for fully-supervised and semi-supervised scenarios, we fine-tune the full model for 40 epochs on labeled data.",1,related,1,positive
58 MAE [19] Autoregressive Transformer 68.,1,related,1,positive
"The Pretrain may be any SSL model, while we experimant with MoCo-v2, SwAV, SimCLR, DINO and MAE.",1,related,0,negative
"We validate our approach over both CNNs (ResNets [34]) and vision transformers (ViT [25]), several self-supervised pre-training methods (e.g., MoCov2 [14], SimCLR [13], SwAV [10], DINO [10] and MAE [31]), two formulations of Feature Diversity, several downstream vision tasks, including multi-label classification on the MS-COCO [50] dataset and a variety of 14 single-label classification datasets.",1,related,1,positive
"Alternatively, we come up with a solution by directly replacing the pixel target in MAE [18] with the Fourier spectrum where each component carries the global information.",1,related,1,positive
"For the pre-training on ImageNet-1K (IN1K) training set, we inherit the experimental settings in MAE [18].",1,related,1,positive
"Following the baseline method MAE [18], the IN1K pre-trained encoder is adopted for initializing the ViTlike backbone of Mask R-CNN [20] framework.",1,related,1,positive
"For the pixel loss Lpix, we compute Mean Square Error (MSE) between the reconstructed and raw images in pixel space, which is similar to MAE [18].",1,related,1,positive
"To achieve this goal and substantially retain the merit of ‚Äúsimple yet effective‚Äù, we build our MIM method, termed Geminated Gestalt AutoEncoder (Ge(2)-AE), upon canonical MAE [18] and simply modify it with one extra lightweight frequency decoder (FD) added to simultaneously perform gestalt tasks of the local masked region and global frequency.",1,related,1,positive
The new autoencoder training mechanism makes it a type of Denoising Autoencoder [16].,1,related,1,positive
The selfsupervised approach referred to as masked auto-encoder (MAE) [19] proposes an improved supervised baseline for the larger ViT models.,1,related,1,positive
For BerT like pre-training we compare our method with MAE [19] and BeiT [2] because they remain relatively simple approaches with very good performance.,1,related,0,negative
"For comparison, we also report the performance of a fine-tuned ViT-B/16 pre-trained using MAE (He et al., 2021), along with a supervised ResNet50 baseline, which is available in the PyTorch Torchvision package6.",1,related,0,negative
"We compare MSN to the joint-embedding approach, DINO (Caron et al., 2021), the auto-encoding approach, MAE (He et al., 2021), and the hybrid approach, iBOT (Zhou et al., 2021),",1,related,1,positive
"All low-shot evaluations (including the 1% ImageNet-1K evaluation) are computed with this procedure, except for models pre-trained using MAE (He et al., 2021), which benefit from using partial fine-tuning (He et al., 2021).",1,related,0,negative
"Specifically, we follow the setup of (Touvron et al., 2021; Bao et al., 2021; He et al., 2021).",1,related,1,positive
"Auto-regressive models and denoising auto-encoders instantiate this principle in vision by predicting the missing parts at the pixel or token level (Chen et al., 2020a; Vincent et al., 2010; He et al., 2021; Bao et al., 2021; Baevski et al., 2022).",1,related,1,positive
"To explain this observation, we refer to the Masked Auto-Encoders paper (He et al., 2021) which conjectures that using a pixel reconstruction loss results in encoder representations of a lower semantic level than other methods.",1,related,1,positive
"For MAE, we rely on partial fine-tuning (He et al., 2021), except for the 1 image per class setting, and all results with the ViT-H/14 architecture, which use a linear classifier.",1,related,1,positive
"We compare both protocols in more detail in Appendix C.
A.3 Linear Evaluation
For linear evaluation, we use a similar procedure as He et al. (2021).",1,related,1,positive
"For auxiliary ViT tasks proposed in [38], [35], we apply random masking as the augmentation stategy, and we adopt another randomly cropped view as augmented sample for contrastive learning [35], [12].",1,related,1,positive
"More specifically, we employ the idea of Data2Vec [38], MAE [35], and momentum contrastive learning [32], [12].",1,related,1,positive
"Please refer to the original paper for more details [38], [35], [32], [12].",1,related,0,negative
"We evaluate different choices of self-supervised learning losses, including Data2Vec [38], MAE [35], and contrastive",1,related,1,positive
"In this short experiment (50 epochs of pretraining, and 70 epochs of linear training) we only used random resized cropping (like MAE (He et al. 2021)) on IN-1K for both MoCoV3 and DILEMMA.",1,related,1,positive
"In all the linear probing experiments, we use the embedding of the CLS token of the last layer and perform a coarse grid search over learning rates, batch sizes and whether to normalize the data before feeding it to the linear layer or not (similarly to the added BatchNorm layer (Ioffe and Szegedy 2015) in MAE (He et al. 2021)).",1,related,1,positive
"This property of ViT encourages us to act bold: we uniformly random sample a subset of patch embeddings serving as the input set of a MAE [20] pre-trained vanilla ViT encoder, i.",1,related,1,positive
"Inspired by the MIM pre-training [4, 20], this work pursues a different solution to transfer a vanilla ViT for object-level recognition: We feed the MIM pre-trained ViT encoder with only a partial input, e.",1,related,1,positive
We initialize the vanilla ViT encoder via MAE [20] pre-trained weight on ImageNet-1K [35].,1,related,1,positive
"B.3 Self-Supervised Learning
Here, we also utilize a simple experiment to evaluate BatchFormerV2 on recent self-supervised learning framework, i.e., Masked Auto Encoder [16](MAE).",1,related,1,positive
Table 10 demonstrates BatchFormerV2 is also beneficial for MAE.,1,related,0,negative
We insert BatchFormerV2 into all layers in the decoder in MAE [16].,1,related,1,positive
"Just as in the RGB-only MAE [28], we only pass the small randomly sampled subset of all tokens to the Transformer encoder as part of the masked autoencoding objective.",1,related,1,positive
"Since all our modalities have a 2D structure, we add 2D sine-cosine positional embeddings [14, 28] after the linear projection.",1,related,1,positive
"Besides the supervised training methods, we also compare with the methods that pre-train on the ImageNet1K training set in a self-supervised manner and then perform supervised finetuning, including DINO [8], MoCo v3 [12], BEiT [1], and MAE [25].",1,related,1,positive
"Specifically, we use the hyperparameters in [25] for training ViT-B, and train ViT-T and ViT-S with the same hyper-parameters except for throwing away EMA, resulting in strong baselines.",1,related,1,positive
"For training ViT-B, we adopt the hyperparameters in [25] in all cases.",1,related,1,positive
We follow almost the same protocol in MAE [11] to train our SD-MAE.,1,related,0,negative
MAE [11] is leveraged as our masked image modeling block.,1,related,1,positive
"For a fair comparison, we use the respective pre-training methods to train the models [5,7,20,11], and then use a unified approach [11] to fine-tune them.",1,related,0,negative
"With Masked Autoencoder (MAE) [23] pretraining, our plain-backbone detector can outperform the hierarchical counterparts that are pre-trained on ImageNet-1K/21K [11] with supervision (Figure 3).",1,related,1,positive
We implement a n√§ƒ±ve extension of MAE pre-training [23] for the hierarchical backbone ablation (Sec.,1,related,1,positive
We use normalized pixels as the MAE reconstruction target [23] and set the decoder depth as 2.,1,related,1,positive
We initialize the backbone with MAE [23] pre-trained on IN-1K without labels.,1,related,0,negative
MAE enjoys the efficiency benefit from plain ViT by skipping the encoder mask token [23].,1,related,1,positive
"Of particular relevence to us is the Masked Autoencoder [12], which completely discards masked input tokens during the encoding step, resulting in significant increases in computational efficiency.",1,related,1,positive
"Consistent with [12], we use fixed sinusoidal positional embeddings for both patch-based and frame-based tokenization.",1,related,1,positive
"Following the MAE [12], by default we throw away the decoder during fine-tuning, training solely with the encoder.",1,related,0,negative
"Given the observation of the above two issues, we argue that performing MIM with a strict mapping between patch predictions and unique token ids in the form of a hard-label classification loss in BEiT limits the visual context capturing and the pre-training performance.",1,related,1,positive
We will go over how to produce such refined multi-choice answers for MIM pre-training in the following section.,1,related,0,negative
"Most of the hyper-parameters of MAE pre-training are followed by (He et al., 2021), while we follow (Radford et al.",1,related,1,positive
"Instead, we employ the self-supervised method of Masked Autoencoder (MAE) (He et al., 2021) with our collected 1.",1,related,1,positive
"Instead, we employ the self-supervised method of Masked Autoencoder (MAE) (He et al., 2021) with our collected 1.1 billion images, which has been shown fast training and powerful performance.",1,related,0,negative
"Most of the hyper-parameters of MAE pre-training are followed by (He et al., 2021), while we follow (Radford et al., 2021b) for the hyper-parameters of KELIP fine-tuning.",1,related,1,positive
"Following the convention [24,2], we fine-tune the pre-trained models for image classification on ImageNet-1K (the same dataset used for pre-training).",1,related,1,positive
"The setting follows MAE [24], where we set a ratio (e.",1,related,1,positive
"Similar to MAE [7], we find that the occlusion ratio of 75% performs the best on both the linear accuracy and supervised fine-tuning accuracy.",1,related,1,positive
"Partly inspired by MAE, we design a new self-supervised learning framework to recover the complete shapes from the highly occluded shapes.",1,related,1,positive
"First, we pre-train the ViT-B on ImageNet-1K for 1600 epochs, following the recipes in [31].",1,related,1,positive
1 Revisiting Image Masked Autoencoders ImageMAE [31] performs the masking and reconstruction task with an asymmetric encoder-decoder architecture.,1,related,1,positive
"3 Proposed Method In this section, we first revisit ImageMAE [31].",1,related,1,positive
We are inspired by the recent ImageMAE [31] and propose customized video tube masking with an extremely high ratio.,1,related,1,positive
"Following the success of masked autoencoding in NLP [18] and images [31, 4], we present a new selfsupervised video pre-training (SSVP) method, termed as Video Masked Autoencoder (VideoMAE).",1,related,1,positive
"In addition to the backbones pre-trained with labeled data, we experiment with two self-supervised objectives: MAE [26] and MoCo v3 [9].",1,related,1,positive
"We compare AttMask with random block-wise masking [2], which is the default in iBOT, random patch masking with the same ratio, as well as with a more aggressive ratio, following MAE [24].",1,related,1,positive
"We compare RandSAC with contrastive transformer training approaches (DINO [5] & MoCo v3 [11]), masked image encoding (BEIT [2] & MAE [22]), and our autoregressive counterpart iGPT [6].",1,related,1,positive
"Masked image modeling & autoregressive image encoding, of which our method is an instance, tend to perform better in such circumstances [2, 22].",1,related,1,positive
"We pretrain blob-RandSAC with hierarchy 11 ‚Üí7 ‚Üí3 using ViT-Base [17] on ImageNet1K following [2, 22].",1,related,1,positive
"We also set the decoder for MAE [22] to have the same depth, attention head, and dimension as ours.",1,related,1,positive
"For both benchmarks, we use the normalized pixel loss introduced from [22] as our patch regression target.",1,related,1,positive
We simply adopt a mean squared error (MSE) between the predicted and target pixel values for all our experiments following [22].,1,related,1,positive
The MAE and DINO are pretrained using their official implementations.,1,related,0,negative
We adopt minimal data augmentation strategy following [22]: resize cropping with scale range of [0.,1,related,1,positive
For MAE we use a 75% masking ratio as suggested in their paper.,1,related,0,negative
"‚Äì To the best of our knowledge, this is the first memory-based UAD method that relies on MAE [9]; ‚Äì A new memory-augmented self-attention operator for our MAE transformer encoder to explicitly encode and memorise the normality patterns; and ‚Äì A novel decoder architecture that uses the learned multi-level memoryaugmented encoder information as prior features to a cross-attention operator.",1,related,1,positive
"Implementation Details For the transformer, we follow ViT-B [6, 9] for designing the encoder and decoder, consisting of stacks of transformer blocks.",1,related,1,positive
We also adopt a linear projection layer after the encoder to match the different width between encoder and decoder [9].,1,related,1,positive
We further investigate the effect of different pre-training methods on the tracking performance by comparing four different pre-training strategies: no pre-training; ImageNet-1k [8] pre-trained model provided by [37]; ImageNet21k [35] pre-trained model provided by [36]; unsupervised pre-training model MAE [14].,1,related,1,positive
"In this section, we delve into the details on why a reconstruction objective (i.e., reconstructing the original point cloud from the unmasked points) as used in the related Masked AutoEncoder (MAE) [20] approach for images would not work for our point cloud setting.",1,related,1,positive
", reconstructing the original point cloud from the unmasked points) as used in the related Masked AutoEncoder (MAE) [20] approach for images would not work for our point cloud setting.",1,related,1,positive
Autoencoder; k-NN ST data; scRNA-Seq Computationally scalable to large sample sizes or gene numbers.,1,related,1,positive
"Visual Transformer Masked Autoencoder (ViT-MAE) [16] performs a different kind of augmentation from the above methods by splitting the original image into patches, heavily masking a significant portion of them and using the remaining patches to reconstruct the input through a visual transformer (ViT) [23].",1,related,1,positive
An obvious cross can be seen in each attention map because we utilize the same implementation as [16].,1,related,1,positive
"Similar to [16], to improve the efficiency of the model, we only use the unmasked frames as inputs to the encoder, excluding the temporal padding embeddings.",1,related,1,positive
"Following [16], we adopt an asymmetric encoder-decoder design.",1,related,1,positive
"Our autoencoder‚Äôs backbone is entirely based on standard Transformers, with an asymmetric encoder-decoder design [17].",1,related,0,negative
"Specifically, our autoencoder‚Äôs backbone is entirely built by standard Transformer blocks and adopts an asymmetric encoder-decoder structure [17].",1,related,1,positive
"We use the MAE framework for the selfsupervised counterpart (He et al., 2021).",1,related,1,positive
"Left: We first pre-train visual representations using self-supervision through masked image modeling (He et al., 2021) from real-world images.",1,related,1,positive
"We adopt masked modeling as our self-supervision objective‚Äîspecifically, we use masked autoencoder (MAE) (He et al., 2021).",1,related,1,positive
"Specifically, we adopt the Masked Autoencoders (MAE) (He et al., 2021) have shown excellent performance on recognition tasks.",1,related,1,positive
"We use an auxiliary dummy classification token in the MAE for downstream finetuning and transfer (He et al., 2021).",1,related,1,positive
"We learn the visual representation by performing masked image modeling through the masked autoencoder (MAE) (He et al., 2021).",1,related,1,positive
"Therefore, we use sine-cosine [4, 10] position embedding in the pre-training stage.",1,related,1,positive
"Inspired by the form of the HSI sample, we propose the Center Mask (CM) Pre-training pretask, which is similar to MAE and adopts an asymmetric structure but is simpler and easier to implement.",1,related,1,positive
"‚Ä¶model as an initialization like most existing methods on domain generalization, this paper seeks a better way to leverage the vast amount of the existing pretrained models [He et al., 2016, Krizhevsky et al., 2012, Iandola et al., 2014, Zoph et al., 2018, He et al., 2021, Radford et al., 2021].",1,related,1,positive
We trained with the setting of MAE on ImageNet but slightly modified it on Cifar.,1,related,0,negative
Here our ViT-P uses the same training hyperparameters as MAE [12].,1,related,1,positive
"On this basis, we can easily and confidently apply selfsupervised learning methods [12, 1, 19, 8, 20], as these methods often increase training data diversity, and predicting or comparing image details allows the network to learn local information autonomously.",1,related,1,positive
"Next, we use the Mask R-CNN framework to compare different backbone networks (CNN and ViT) including ResNeXt-101, DeiT, BEiT, MAE, and DiT.",1,related,1,positive
"For image Transformers, we choose the base version of DeiT [36], BEiT [3] and MAE [17] which are pre-trained on ImageNet-1K dataset with a 224√ó224 input size.",1,related,1,positive
"Unlike an end-to-end unsupervised pre-training in [13], which requires an extremely large model and high mask rate to avoid cheating model, a much smaller decoder is trained in our method for a specific encoder.",1,related,0,negative
Compared to previous work we did not try layerwise learning rate decay in finetuning [17] which may boost performance further.,1,related,0,negative
For masked auto-encoding pre-training we use the optimization hyperparameters from [17] and pretrain for 500 epochs with 85% masking for most experiments because that seems to do slightly better than 75% on ImageNet in preliminary experiments.,1,related,0,negative
"We consider both the classic masking approach [14] of replacing the masked inputs with a learned token (uniform-masking), and the more efficient alternative of dropping the masked inputs [17] ‚Äì we call this groupwise-masking.",1,related,1,positive
We experimented also masking random 75% of 16x16 patches [17] instead of pixels (while still operating on pixels) and finetuning performance was about 2% worse.,1,related,0,negative
"In this paper, we adopt MAE [27] to train the scaled-up ViTAE model due to its simplicity and efficiency.",1,related,1,positive
"It should be noted that the original MAE is trained on the TPU machines with Tensorflow, while our implementation adopts PyTorch as the framework and uses NVIDIA GPU for the training.",1,related,1,positive
"In this paper, we adopt MAE (He et al., 2022) to train the scaled-up ViTAE model due to its simplicity and efficiency.",1,related,1,positive
"Inspired by BERT [24] and MAE [26], we propose a selfsupervised graph transformer model named Graph Masked Autoencoders (GMAE).",1,related,1,positive
"We distill ViT-T or ViT-S using a regular setting of training for 300 epochs, and distill ViT-base or ViT-large for 600 epochs (MAE (He et al., 2021) uses much more epochs,
1https://github.com/HobbitLong/RepDistiller
i.e. 1600).",1,related,1,positive
"We distill ViT-T or ViT-S using a regular setting of training for 300 epochs, and distill ViT-base or ViT-large for 600 epochs (MAE (He et al., 2021) uses much more epochs,",1,related,1,positive
"To compare with state-of-the-art ViT performances (He et al., 2021; Steiner et al., 2021), we use a stronger teacher BEiT-L (Bao et al., 2021) with students ranging from ViT-T to ViT-L (Dosovitskiy et al., 2020).",1,related,1,positive
"Our MKD distilled ViT-L obtains 86.5% accuracy, +3.9% better than training without distillation (He et al., 2021).",1,related,0,negative
"To compare with state-of-the-art ViT performances (He et al., 2021; Steiner et al., 2021), we use a stronger teacher BEiT-L (Bao et al.",1,related,1,positive
"We first follow [12, 42] to use a larger masking ratio of 50% (instead of 15% as in [5, 25, 36]).",1,related,1,positive
"Following the practice in [31], we also omit the shared class token added to the embedding that can be removed from the",1,related,1,positive
"Figure 1: Architecture of the Masked Autoencoder (MAE) in [31], with Vision Transformer (ViT) backbone.",1,related,1,positive
"with non-overlap window‚Äù is proposed in (He et al., 2021).",1,related,1,positive
"As shown in Table 4, we observe CIM works better with simple random masking (He et al., 2021; Xie et al., 2021) compared with the blockwise masking strategy proposed in BEiT.",1,related,1,positive
"For each image triplet, we visualize the original image (left), the template of using non-overlapping window normalization (He et al., 2021), and the template of the proposed sliding window normalization paradigm.",1,related,1,positive
"5: The computational graphs for (a) a context autoencoder (CAE), (b) BEiT [4], (c) a denoising autoencoder (DAE), and (d) MAE [38] and the one stream in SplitMask [29].",1,related,1,positive
"Following [38], we adopt an extra BatchNorm layer [48] without affine transformation",1,related,1,positive
"The subsequent method, masked autoencoder (MAE) [38] adopts an encoder-decoder architecture, partially decoupling the two tasks.",1,related,1,positive
"We provide the computational graph for CAE, BEiT [4], denoising autoencoder, Masked Autoencoder [38] and SplitMask [29] (one stream) in Figure 5.",1,related,1,positive
"The technical report 1 of our approach was initially published as an arXiv paper [19], and was concurrent to data2vec [3], MAE [38], and other methods, such as [29,91].",1,related,0,negative
"2, we employ the MAE with vision Transformer (ViT) architecture, where we randomly mask patches from input images and aim to reconstruct the missing patches [14].",1,related,1,positive
"‚Ä¶which permits unrestricted re-use, distribution and reproduction, provided the original article is properly cited.
image understanding (e.g., Caron et al., 2021; Bao et al., 2022; He et al., 2022), and has led there to significantly improved performance on a variety of tasks.",1,related,0,negative
"We follow the same setting as the visual transformer (ViT) which recently attracts much attention and shows remarkable performance in the computer vision area [6, 15, 20, 46, 72, 73].",1,related,0,negative
"Similar to the designs in (He et al., 2021; Xie et al., 2021), it is appropriate to use a lightweight decoder in MLR, because we actually expect the predicting masked information to be mainly completed by the encoder instead of the decoder.",1,related,1,positive
"Inspired by MLM and MIM, we explore the masked modeling for RL to exploit the high correlation in vision data to improve the awareness of agents to global-scope dynamics in learning state repesentations.",1,related,1,positive
"Most importantly, we propose to predict the masked contents in the latent space, instead of the pixel space like aforementioned MIM works, which better coordinates the representation learning and the policy learning in RL.",1,related,1,positive
"Similar to the encoder designs in (He et al., 2021; Xie et al., 2021), the online encoder in our proposed MLR reconstructs the representations of masked contents based on visible ones in an implicit way.",1,related,1,positive
"Compared with supervised pretrained ResNet used in previous methods (14; 17), we argue that the MAE is more powerful for extracting features of the H&E stain images.",1,related,0,negative
The detailed explanation of MAE can be found in the supplementary material.,1,related,0,negative
"Inspired by MAE, we also use the subset of non-overlapping patches to train the feature extractor
xÃÉi = RM(xi;m), (1)
where RM denotes random masking operation and m denotes the masking ratio.",1,related,1,positive
"We also prune stacked Transformer blocks [20] which are pretty popular in previous masking-based models [17, 18, 19].",1,related,1,positive
"Unlike [18, 19] which only computes loss on masked patches, we compute loss on all patches.",1,related,1,positive
"We use the supervised training results from DeiT [73] for ViT-S/B and MAE [26] for ViT-L, as they employ improved training procedures over the original ViTs [20].",1,related,0,negative
"Inspired by the recent work [16], we randomly drop a part of input pixels to speed-up the training of image encoders.",1,related,1,positive
Note that we can alternatively add a normalization layer before the linear classifier following [30] instead of using larger learning rate values.,1,related,1,positive
"2 https://github.com/facebookresearch/mae 3 https://github.com/facebookresearch/swav
Method Bedroom-28 FFHQ-34 Cat-15 Horse-21 CelebA-19‚àó ADE Bedroom-30‚àó
ALAE 20.0 ¬± 1.0 48.1 ¬± 1.3 ‚Äî ‚Äî 49.7 ¬± 0.7 15.0 ¬± 0.5 VDVAE ‚Äî 57.3 ¬± 1.1 ‚Äî ‚Äî 54.1 ¬± 1.0 ‚Äî
GAN Inversion 13.9 ¬± 0.6 51.7 ¬± 0.8 21.4 ¬± 1.7 17.7 ¬± 0.4 51.5 ¬± 2.3 11.1 ¬± 0.2 GAN Encoder 22.4 ¬± 1.6 53.9 ¬± 1.3 32.0 ¬± 1.8 26.7 ¬± 0.7 53.9 ¬± 0.8 15.7 ¬± 0.3
SwAV 42.4 ¬± 1.7 56.9 ¬± 1.3 45.1 ¬± 2.1 54.0 ¬± 0.9 52.4 ¬± 1.3 30.6 ¬± 1.6 MAE 45.0 ¬± 2.0 58.8 ¬± 1.1 52.4 ¬± 2.3 63.4 ¬± 1.4 57.8 ¬± 0.4 31.7 ¬± 1.8
DatasetGAN 31.3 ¬± 2.3 57.0 ¬± 1.1 36.5 ¬± 2.3 45.4 ¬± 1.4 ‚Äî ‚Äî DatasetDDPM (Ours) 47.9 ¬± 2.9 56.0 ¬± 0.9 47.6 ¬± 1.5 60.8 ¬± 1.0 ‚Äî ‚Äî
DDPM (Ours) 49.4 ¬± 1.9 59.1 ¬± 1.4 53.7 ¬± 3.3 65.0 ¬± 0.8 59.9 ¬± 1.0 34.6 ¬± 1.7
Generative pretrained models.",1,related,1,positive
"First, we learn pixel classifiers on the clean images using the DDPM, SwAV and MAE representations on the Bedroom-28 and Horse-21 datasets.",1,related,1,positive
"Meanwhile, motivated by the great potential of masking in [4], we mask random patches of original frames and reconstruct them in the loss function.",1,related,1,positive
"Different from the ran-
dom masking used in BERT [8] and MAE [13], we adopt a block-wise masking strategy like [2].",1,related,1,positive
"dom masking used in BERT [8] and MAE [13], we adopt a block-wise masking strategy like [2].",1,related,1,positive
"All views contain one subregion representing the same content, but with different pixel appearances and surrounding context.
fŒ∏ (XÃÉ (m))' Z ‚àÄm ‚àà (1, . . . ,M) (1)
We relate our approach to discovering semantic meanings for pixels to discovering semantic meanings for words in NLP similar to recent MIM works [5, 21, 106, 116].",1,related,1,positive
"‚Äò*‚Äô denotes that we end-to-end finetune RegionCL-M pretrained models for 50 epochs [2, 17].",1,related,1,positive
"Importantly, we witness an exciting new result: masking-based methods (BEiT and MAE) show considerable gains over both supervised and random initialization and these gains increase as model size increases.",1,related,1,positive
We find that MAE and BEiT provide the first convincing results of substantial COCO AP improvements due to pretraining.,1,related,0,negative
"We use the DeiT released weights [36] for ViT-B and the ViT-L weights from [16], which uses an even stronger training formula than DeiT to avoid overfitting (moreover, the DeiT release does not include ViT-L).",1,related,1,positive
"(4) For ViT-B, BEiT and MAE outperform both random initialization by up to 1.4 APbox (50.3 vs. 48.9) and supervised initialization by up to 2.4 APbox (50.3 vs. 47.9).",1,related,1,positive
MAE: We use the ViT-B and ViT-L weights pre-trained on unsupervised ImageNet-1k from the authors of [16].,1,related,1,positive
"We overcome these obstacles and present strong ViT-based Mask R-CNN baselines on COCO when initializing ViT from-scratch [18], with pre-trained ImageNet [8] supervision, and with unsupervised pre-training using recent methods like MoCo v3 [7], BEiT [1], and MAE [16].",1,related,1,positive
"For these experiments, we use MAE and 50 epoch fine-tuning by default.",1,related,1,positive
In Figure 3 we study the impact of MAE pre-training epochs on COCO APbox by sweeping pre-training epochs from 100 to 1600 (the default).,1,related,0,negative
"In summary, we observe that all initializations lead
to roughly the same classification performance for correctly localized objects, however the MAE and BEiT initializations improve localization compared to the other initializations.",1,related,1,positive
"Self-Supervised Transformer Generative: iGPT [69], MST [70], BEIT [71], MAE [72].",1,related,1,positive
"In this section, we discuss how to apply our framework to MAE (He et al., 2022), CLIP (Radford et al.",1,related,1,positive
"We also provide a variant of the MAE model finetuned on our data, denoted as MAE-sor.",1,related,1,positive
‚Ä¢ MAE-sor is a variant of MAE that is trained on our data (Leonardo and Kitchen) using the masked autoencoding objective.,1,related,0,negative
"Interestingly, we did not find a big difference compared to the MAE model pre-trained on ImageNet, which indicates that dataset bias is not the key issue in our comparisons.",1,related,0,negative
MAE We use the ViT-B/16 model in the official MAE implementation https://github.com/ facebookresearch/mae.,1,related,0,negative
"Backdoor attack on MAE: We attack Masked Autoencoders (MAE) [21], which is a very recent SSL method.",1,related,1,positive
We also try masked autoencoders (MAE) [21] which is a very recent SSL method.,1,related,1,positive
"Similar to the recent unsupervised learning methods [57], [58], [59], we compare the effectiveness of the features extraction using a ResNet50 backbone with our approach to those obtained with a supervised ImageNet pre-trained model.",1,related,1,positive
"In our work, we pre-train the model on ImageNet dataset at patch level, as this is a common practice in pre-training on ImageNet and transferring to COCO or VOC datasets [10], [37].",1,related,1,positive
"Differently, our PLM is optimized to model the action distribution while MAE is optimized for general purposes and requires extra task-speciÔ¨Åc tuning for downstream applications.",1,related,1,positive
"We show in Figure 1 some visualizations with Generic Attribution to different types of Vision Transformers (Dosovitskiy et al., 2021; He et al., 2022).",1,related,1,positive
"Besides ViT-Base in Sub-section 4.2, we conduct evaluation experiments on another two variants: ViT-large-16-224 (Dosovitskiy et al., 2021), denoted ViT-Large and ViT-MAE with the global pooling strategy instead of [CLS] pooling in vanilla ViT-base-16-224 model.",1,related,1,positive
"Our method is further validated qualitatively and quantitatively through the faithfulness evaluations across different settings: single modality (BERT and ViT) and bi-modality (CLIP), different model sizes (ViT-L) and different pooling strategies (ViT-MAE) to demonstrate the broad applicability and clear improvements over existing methods.",1,related,1,positive
"Most Transformers perform the classification task via the last [CLS] embedding, while some use global pooling instead (He et al., 2022).",1,related,1,positive
"We can see the proposed Edge MAE achieves the highest results, and link prediction models perform better as they model the whole return process entirely.",1,related,1,positive
"For pre-training Edge MAE, we tuned the learning rate as 1.5e-4, weight decay as 0.05, the mask ratio as 1/3, and batch size as 2048.",1,related,0,negative
"From the two tables, we can see that, Edge Transformer outperforms almost all baseline models, and the results can be further improved by pre-training Edge MAE on unlabeled edges.",1,related,1,positive
"Furthermore, we propose a novel Edge Transformer model and pre-train the model via Masked Auto-Encoders (MAE).",1,related,1,positive
"In this section, we evaluate our Edge Transformer and Edge MAE in two settings.",1,related,1,positive
"4.2 Parameter Settings For Edge Transformer and Edge MAE, we set the latent vector size ùê∑ as 256, the dropout rate as 0.0, and the number of attention heads as 3.",1,related,1,positive
"Inspired by MAE [7] in CV which pre-trains masked auto-encoders on unlabeled images, we propose to improve Edge Transformer with an Edge MAEmodel.",1,related,1,positive
"1CDTL, 3CDTL, MAE, BERT, and PEAMATL are Python 3.7 applications built on the Pytorch framework, while PLSUS is a MATLAB R2019a application (Mathworks, Inc., Natwick, MA, USA).",1,related,1,positive
"From Tables s4 and s5 in the supplementary material, for the three datasets with domain shift, 1CDTL, 3CDTL, PLSUS, MAE, and BERT models obtain R (RPD) values of 0.866‚Äì0.987 (1.79‚Äì8.61), 0.855‚Äì0.988 (1.78‚Äì9.24), 0.870‚Äì 0.988 (1.84‚Äì9.38), 0.862‚Äì0.985 (1.77‚Äì9.22), and 0.867‚Äì0.986 (1.78‚Äì8.98), respectively.",1,related,1,positive
3) The PEAMATL outperforms the MAE method with 4.31%‚Äì34.25% and the BERT method with 3.32%‚Äì32.13% on the test RMSE of all scenarios.,1,related,0,negative
"We mainly compare our method with ClusterFit (Yan et al., 2020), SNCA+ (Wu et al., 2018), Grafit (Touvron et al., 2021b), iBOT (Zhou et al., 2022), DeiT (Touvron et al., 2021a) and MAE (He et al., 2021).",1,related,1,positive
"However, for the previous methods like MAE (He et al., 2021), there‚Äôs no regularization added on the [CLS] token.",1,related,1,positive
"In our experiments, the block-wise dividing strategy gets a bit lower accuracy than random dividing, which is in line with previous methods MAE (He et al., 2021) and SimMIM (Xie et al., 2021).",1,related,1,positive
"‚Ä¶over MAE: Using ViT-B (Dosovitskiy et al., 2020) as a standard protocol in MIM, ccMIM achieves 83.6%, 84.2% top-1 accuracy with 300 and 800 epochs pre-training, outperforming MAE (He et al., 2021) 0.8% (82.8% for 300 epochs) and 0.6% (83.6% for 1600 epochs) accuracy on ImageNet-1K, respectively.",1,related,1,positive
"‚Ä¶classifier over the image-level representation output from the pretrained encoder by using the labels of the images, and then tests the performance on the validation set. ii) Fine tuning is often used to evaluate the backbone in reconstructed-based methods (He et al., 2021; Chen et al., 2022).",1,related,1,positive
"We perform self-supervised pre-training on the ImageNet-1K (Deng et al., 2009) training set, as commonly used in SSL (He et al., 2021; Zhang et al., 2021).",1,related,1,positive
"In this paper, following MAE (He et al., 2021), we mainly consider ViT (Dosovitskiy et al., 2020) as backbones.",1,related,1,positive
"We mainly compare our method with transformer-based contrastive learning methods (Caron et al., 2021; Chen et al., 2021) and MIM-based (He et al., 2021; Xie et al., 2021) methods.",1,related,1,positive
"In this paper, we mainly follow the settings in MAE (He et al., 2021).",1,related,1,positive
"The decoder
of ccMIM follows the settings of MAE (He et al., 2021), which includes two-layer MHSA blocks.",1,related,1,positive
"In this paper, following MAE (He et al., 2021), we mainly consider ViT (Dosovitskiy et al.",1,related,1,positive
"Note that, as a reminiscent method, MAE (He et al., 2022) expects to reconstruct all patches, so the number of mask tokens is 147 (take ViT-B/16 as an example).",1,related,1,positive
We follow the implementation of He et al. (2022) for the decoder part.,1,related,1,positive
"We follow the setting of hyper-parameters of MAE (He et al., 2022).",1,related,1,positive
"We follow the fine-tuning schedule and hyper-parameters in MAE (He et al., 2022) out of fair comparison.",1,related,1,positive
"The settings of data augmentation and optimization keep consistent with MAE (He et al., 2022).",1,related,1,positive
"We are interested in a particular visual pre-training method named masked image modeling (MIM) (Bao et al., 2021; He et al., 2021).",1,related,1,positive
"Under the MAE framework (He et al., 2021), with 1600 epochs of pre-training and 100 epochs of fine-tuning, HiViT-B reports a 84.6% top-1 accuracy on ImageNet1K, which is +1.0% over ViT-B (trained with MAE) and +0.6% over Swin-B (trained with SimMIM (Xie et al., 2021b)).",1,related,0,negative
"When fine-tuning, we follow the settings from (He et al., 2021) where the models are trained for 100 epochs using the AdamW optimizer with a warm-up for 5 epochs, a weight decay 0.05, and the input size 224√ó 224.",1,related,1,positive
"Inspired by MAE [13], we propose a naive background reconstruction method (NBR).",1,related,1,positive
"For BR, we directly use pre-trained MAE with extra GAN loss [13].",1,related,0,negative
"Therefore, we utilize a mature but large Vision Transformer (MAE-ViT-large [13]) for background reconstruction.",1,related,0,negative
This ‚Äògoal‚Äô is also encoded by the same MAE encoder and concatenated with the bounding box feature.,1,related,1,positive
"A masked auto-encoder (MAE) (He et al., 2022), with pre-trained weights, was applied as a feature encoder.",1,related,0,negative
We compared CLIP-based extractors to the other two methods (masked autoencoders (MAE) [14] and a simple framework for masked image modeling (SimMIM) [15]).,1,related,1,positive
"Following existing siamese frameworks [61, 45, 25], we introduce a target encoder to generate contrastive supervision for the online encoder to further strengthen the representation learned by MIM with semantic discriminability.",1,related,1,positive
"Following MAE [21], we use the normalized pixel values as targets in the reconstruction task.",1,related,1,positive
"Following MAE [21], we only feed the visible, unmasked patches to the online encoder Es.",1,related,0,negative
"Now we analyze the drawbacks and irrationalities of existing hybrid SSL approaches, which combine CL with MIM, from the following aspects:
ar X
iv :2
30 2.",1,related,1,positive
"In this work, we propose a novel self-supervised learning method (Alice) for improving the learned image representation of contrastive learning and MIM by modeling the class-specific invariance of intrinsic anatomical semantics in 3D medical images.",1,related,1,positive
"For MIM tasks, we apply augmentations and masking strategy following MAE [21] to the input of the online encoder.",1,related,1,positive
"Driven by the aforementioned limitations, we present a simple, effective, and dedicated selfsupervised learning framework for 3D medical segmentation tasks, Alice, by explicitly fulfilling Anatomical invariance modeling and semantic alignment through elaborately combined contrastive learning and MIM.",1,related,1,positive
"In this subsection, we discuss the details of self pre-training and transfer learning using MaskSL, by introducing an instance based on MAE.",1,related,1,positive
"As mentioned previously, we use MAE as the loss function of reconstruction tasks in this paper.",1,related,1,positive
The emergence of the masked autoencoder (MAE) [16] has greatly influenced our community due to its simplicity and effectiveness.,1,related,0,negative
"To evaluate the effectiveness of the modification, we compare the proposed strategy with that of MAE [16].",1,related,0,negative
We modify the masking strategy in MAE [16] to improve the generalization.,1,related,1,positive
Our masked autoencoder is based on an asymmetric encoder-decoder architecture [16].,1,related,1,positive
"In this stage, we customize MAE [16] and its masking strategy with minimal but effective modifications, so that the model can learn generic facial part consistency features and can also prevent over-fitting to specific forgery patterns.",1,related,1,positive
"The original MAE [16] masks random patches of the input image, but the same strategy is not suitable for our DeepfakeMAE for the following reasons.",1,related,1,positive
"Inspired by [16], we propose a new self-supervised pretraining method based on masked autoencoder, named DeepfakeMAE.",1,related,1,positive
We customize the original masking strategy from MAE [16] and design a facial part masking strategy to ensure that the model can learn the consistencies of all facial parts.,1,related,1,positive
"By simplifying the notation, the equation can be equivalent to the optimization problem in [17], [44], and the following equation is derived:",1,related,1,positive
"of masked autoencoders (MAE) [17], the mask operation is introduced to alleviate the overfitting problem.",1,related,1,positive
"MAEBase (He et al. 2021), in our paper, since it is the fastest model to train with our resources.",1,related,0,negative
"For feature extraction, we tested two types of self-supervised learning, adversarial contrastive learning (AdCo) and masked autoencoder (MAE)[31], and compared their performance.",1,related,1,positive
"2.7 Contrastive learning and attentive pooling improve performance We also conducted a comparative analysis of two self-supervised learning models, namely masked autoencoder (MAE) and adversarial contrastive learning (AdCo), with respect to their performance in extracting tile-level features.",1,related,1,positive
"For feature extraction, we tested two types of self-supervised learning, adversarial contrastive learning (AdCo) and masked autoencoder (MAE)[30], and compared their performance.",1,related,1,positive
We follow the parameters setting of MAE [13] and CCT [42] for our model training.,1,related,1,positive
"Motivated by MAE, we adopt a ViT-based [12] encoder‚Äì decoder network as the backbone of each channel network.",1,related,1,positive
"Following MAE, we concatenate the mask token and rest token and then forward it to the online decoder.",1,related,1,positive
"We sample random patches without replacement
following MAE.",1,related,1,positive
"Loss Function: Consistent with MAE and bidirectional encoder representation from transformers (BERT) [42], we use the mean square error on the reconstructed feature maps and the original image space.",1,related,1,positive
This article uses l2 regression loss following MAE [43].,1,related,1,positive
"At a high level, our method simply substitutes MAE [72] for the self-supervised part of TTT [171].",1,related,1,positive
We intentionally avoid modifying them to make clean comparison with [72].,1,related,0,negative
"In this chapter, we show that masked autoencoders (MAE) [72] is well-suited for test-time training.",1,related,0,negative
"Following standard practice, we start from the MAE model provided by the authors of [72], with a ViT-Large encoder, pre-trained for reconstruction on ImageNet-1k [40].",1,related,1,positive
"Our default setup, during training-time training, only uses image cropping and horizontal flips for augmentations, following the protocol in [72] for pre-training and linear probing.",1,related,0,negative
"Notably, Masked AutoEncoder (MAE) [65] masks out 75% of patches to reconstruct and makes the plain, nonhierarchical ViT model back state-of-the-art via fine-tuning.",1,related,1,positive
The model parameter is initialized by MAE for 10 epochs and fine-tuned for another 10 epochs using the AdamW optimizer [76].,1,related,1,positive
"Based on this dataset, we design the RingMo training method to apply the MIM method to train an RS foundation model.",1,related,1,positive
"Similar to other MIM methods [32], we only compute the loss for masked regions.",1,related,1,positive
"However, to our best knowledge, there is little exploration of MIM-based
SSL for the RS foundation model.",1,related,0,negative
"In addition, for deriving general RS feature representation, we further propose the RingMo MIM method, which enhances the issue of dense and small objects being easily disregarded in complicated RS scenes.",1,related,1,positive
"(4) where e = 10000 is the pre-defined parameter, which is also commonly used in MAE (He et al., 2021).",1,related,1,positive
"We use standard self-supervised learning protocols, including learning a linear classifier on frozen features (Chen et al., 2020a; He et al., 2020) and
finetune on downstream tasks (He et al., 2021; Chen et al., 2022).",1,related,1,positive
"‚Ä¶(Vaswani et al., 2017) to generate positional embedding:
Wh,w = [ sin ( w e2‚àó1/d ) , cos ( w e2‚àó2/d ) , ¬∑ ¬∑ ¬∑ , sin (w e ) ,
sin
( h
e2‚àó1/d
) , cos ( h
e2‚àó2/d
) , ¬∑ ¬∑ ¬∑ , sin ( h
e )] (4)
where e = 10000 is the pre-defined parameter, which is also commonly used in MAE (He et al., 2021).",1,related,1,positive
"We conduct self-supervised pre-training on the ImageNet-1K (Deng et al., 2009) training set with 1,000 classes, as used in SSL for both MIM (He et al., 2021) and contrastive learning (Chen et al., 2020a).",1,related,1,positive
"Standard SSL protocols is to either learn a linear classifier on frozen features (Chen et al., 2020a; He et al., 2020) or to finetune on downstream tasks (He et al., 2021; Chen et al., 2022).",1,related,1,positive
"We conduct self-supervised pre-training on the ImageNet-1K (Deng et al., 2009) training set, as commonly used in SSL methods for both MIM (He et al., 2021) and contrastive learning (Chen et al., 2020a).",1,related,1,positive
"Differences. iBOT learns the local information by adding MIM objective, which is now widely developed in MAE (He et al., 2021), SimMIM (Xie et al., 2021d).",1,related,1,positive
"iBOT learns the local information by adding MIM objective, which is now widely developed in MAE (He et al., 2021), SimMIM (Xie et al.",1,related,0,negative
"To accelerate the training and reduce the difficulty of fitting, we use the pre-trained MAE [34] model for model initialization.",1,related,0,negative
"Compared with OSTrack‚Äôs and MAE blocks, our MNM block has a more significant advantage in performance.",1,related,0,negative
‚Ä¢ Masked autoencoders (MAE).,1,related,0,negative
"Following (He et al., 2022), during linear probing we include BatchNorm without the affine transformation before the final classifier.",1,related,1,positive
"For audio, we use a fixed 2D sinusoidal positional encoding as described in (Huang et al., 2022) and (He et al., 2022).",1,related,1,positive
", 2022) and the retained natural information in adversarial samples, we leveraged a masking-related generative network (He et al., 2022) as the denoising model to restore missing natural representation from preserved neighboring pixels.",1,related,1,positive
"Based on the spatial redundancy of images (Bastani et al., 2010; Hu et al., 2020; He et al., 2022) and the retained natural information in adversarial samples, we leveraged a masking-related generative network (He et al., 2022) as the denoising model to restore missing natural representation from‚Ä¶",1,related,1,positive
"‚Ä¶of images (Bastani et al., 2010; Hu et al., 2020; He et al., 2022) and the retained natural information in adversarial samples, we leveraged a masking-related generative network (He et al., 2022) as the denoising model to restore missing natural representation from preserved neighboring
pixels.",1,related,1,positive
"Based on the spatial redundancy of images (Bastani et al., 2010; Hu et al., 2020; He et al., 2022) and the retained natural information in adversarial samples, we leveraged a masking-related generative network (He et al.",1,related,1,positive
"For the experiments in Section B of the Appendix, we train all models using the Vanilla training strategy to perform fair comparisons with other methods such as Masked Autoencoders [6] and our Inpainting-based augmentation training strategy.",1,related,1,positive
"To compare with Masked Autoencoders (MAE) [6], we pretrain using the masked image modelling technique proposed by them [6].",1,related,1,positive
"We replaced iBOT ImageNet 1K pre-trained weights with that of MAE [5], DINO [2].",1,related,1,positive
"1: Masked Autoencoders (MAE) architecture of [15] (reprinted): In training, only 25 % of all patches are fed into the large-scale encoder, facilitating efficient training.",1,related,0,negative
1 Masked Autoencoders (MAE) architecture of [15].,1,related,1,positive
", the same as fine-tuning datasets), we pretrain a ResNet50 with several typical SOTA SSL methods in computer vision including MoCo v2 [8], SimCLR [11], Simsiam [12], and DenseCL [10] while a ViT-Base [46] is pretrained with MAE [9],",1,related,1,positive
"We also Ô¨Ånd that in our experimental settings, even though other SSL methods such as MoCo v2 and MAE pretrained directly on the training and testing sets of the downstream datasets, they did not work well here, which we attributed to the fact that the insufÔ¨Åcient size of the Vaihingen dataset resulted in these pretrained backbones did not learn good representations.",1,related,0,negative
"For comparison of performance, using RSIs in target segmentation datasets (i.e., the same as Ô¨Åne-tuning datasets), we pretrain a ResNet50 with several typical SOTA SSL methods in computer vision including MoCo v2 [8], SimCLR [11], Simsiam [12], and DenseCL [10] while a ViT-Base [46] is pretrained with MAE [9], whichisaninÔ¨Çuentialrepresentativeofmaskedimagemodeling.",1,related,1,positive
"4, we visualize the MAE [27, 31] reconstruction results on a few Ego4D [29] examples with a ViT-B [24] trained for 200 epochs without per-patch normalization.",1,related,1,positive
Our pipeline applies the original MAE [31] and video MAE [27] algorithms.,1,related,1,positive
"For the downstream OAR segmentation task, the foundation ViT model and the UperNet (Xiao et al., 2018) decoder were used as the encoder and the decoder, respectively, following the implementation in Bao et al. (2021); He et al. (2022).",1,related,1,positive
"We use the zero-shot classifier for CLIP, linear probing for DINO, and fine-tuned classifier for MAE.",1,related,1,positive
"We compare ViT-B trained by different methods: supervised model (ERM), multimodal learning - CLIP (Radford et al., 2021), and self-supervised learning - DINO (Caron et al., 2021) and MAE (He et al., 2022).",1,related,1,positive
"For the downstream OAR segmentation task, the foundation ViT model and the UperNet decoder were used as the encoder and the decoder, respectively, following the implementation in [12].",1,related,1,positive
"(Yang et al., 2022)
Oriented RCNN ViTAE 81.24 Use MAE (He et al., 2022) to pretrain the plain ViTAE transformer.",1,related,1,positive
"To further emphasize that c2i is a feature-specific embedding, we align c 2 i with the original node feature embedding v0i , shown as following:
LAL = 1 N N‚àë i=1 (v0i ‚àí c 2 i ) 2 (9)
The overall disentangled loss LD is expressed as follow:
LD = LRD + LAL (10)
VOLUME 11, 2023 23983
C. UNIFORMITY AUTOENCODER Inspired by the MAE [26] and BERT [31], We designed an Uniformity Autoencoder model based on the masked mechanism, to learn the internal structure of equipment alignment.",1,related,1,positive
"Inspired by the MAE [26] and BERT [31], We designed an Uniformity Autoencoder model based on the masked mechanism, to learn the internal structure of equipment alignment.",1,related,1,positive
"We use a MAE (He et al., 2021) unsupervised pre-training model in ImageNet for 1600 epochs to ensure labels are not available during the whole pretraining process.",1,related,0,negative
"Furthermore, we find that CycleMAE gets higher performance gains on finetuning evaluation protocol, which is consistent with other unsupervised learning researches (He et al., 2021; Xie et al., 2022).",1,related,1,positive
"To tackle the aforementioned obstacles, we endeavor to develop a comprehensive model architecture able to flexibly admit the satellite and radar images for real-world storm prediction, resorting to the vision transformer (ViT) [12] and masked autoencoders (MAE) [15].",1,related,0,negative
"In Figure 2, we provide relation maps of three samples from ImageNet, using 10 checkpoints of MAE-Large (He et al., 2022).",1,related,1,positive
"Specifically, we measure the detection performance with MAE, BEIT, and ConvNeXt (He et al., 2022; Bao et al., 2022; Liu et al., 2022).",1,related,1,positive
"ImageNet We measure the label error detection performance on ImageNet with the synthetic label noise by train-
ing an MAE-Large model (He et al., 2022).",1,related,1,positive
"To better understand the BPBA setting, we give an analysis and comparison for BPBA, and self-supervised learning [6,9,11,12,32].",1,related,1,positive
"First of all, we train a Masked Autoencoder (MAE) [5, 18] on our private large-scale face dataset in a self-supervised manner.",1,related,1,positive
"For the first challenge, we follow the successful practices in language [12, 29, 88] and vision [7, 35] modeling to construct the supervision signals, i.",1,related,1,positive
"The concatenation of unmasked patches‚Äôs embedding and masked tokens are processed by the decoder to predict the unmasked patches [17, 43].",1,related,1,positive
"Since Vision Transformers pretrained with and without SAM on ImageNet-1k [58] are available as public checkpoints, we evaluate the efficacy of token compression techniques on the larger ImageNet dataset.",1,related,1,positive
"Xiao et al. (2022) demonstrates that masked-autoencoder (He et al., 2022) trained on diverse video data (Goyal et al., 2017; Shan et al., 2020) can be an effective visual embedding for online RL.",1,related,1,positive
"But how can we use these large self-supervised models in visual representation come at use for robotics tasks? In this paper, we study Masked Autoencoder (MAE) [12] for helping with predictions for top-down images.",1,related,1,positive
Masked autoencoders (MAE) [34] are an effective and,1,related,0,negative
"Inspired by MAE (He et al., 2022), we observe a different information density between sentences and documents ‚Äî for an event, most parts of the document are irrelevant, leading to a low information density.",1,related,1,positive
"This operation is the same as in other MIM methods (e.g., MAE (He et al. (2022)) ).",1,related,1,positive
The encoder model architecture follows closely that of the MAE paper [1].,1,related,1,positive
We build our VideoTrack model on top of the vanilla ViT-Base [5] model pre-trained with MAE [8].,1,related,0,negative
"Inspired by recent progress in high-fidelity efficient restoration through an efficient MLP [42] or a small transformer [22], we adopt an asymmetric encoder-decoder framework, which enables real-time reconstruction.",1,related,1,positive
"Specifically, we use the ViT-B/16 weights from the Masked AutoEncoding pre-training [10].",1,related,0,negative
We propose to use a generalized form of masking based self-supervision [42] based on predicting shuffled future features.,1,related,1,positive
"For example, while Œ≤-VAE learned a sufficient representation for the domains we considered, a version of LS(3) learned on top of masked autoencoders [17] would likely be applicable to a wider variety of situations.",1,related,1,positive
"This can partially be motivated by the reduced time needed for pre-training, but we also find the encoder to achieve higher downstream task performance when trained in conjunction with a smaller decoder, similar to the results in [18].",1,related,0,negative
"For the pre-training, we follow the training paradigm of MAE [18] and equip the model with a lightweight decoder used for reconstructing masked input.",1,related,0,negative
"In this paper, we demonstrate that a vanilla ViT-based backbone pretrained by MAE method (He et al. 2022) without extra data is comparable to the SOTA work.",1,related,0,negative
"In this paper, we demonstrate that a vanilla ViT-base backbone pretrained by MAE method (He et al. 2022) without extra data is comparable to the SOTA work.",1,related,1,positive
"We initialize the original ViT as a Masked Autoencoder (MAE) (He et al. 2022) on AffectNet (Mollahosseini, Hasani, and Mahoor 2017) dataset such that it can extract a stronger representation of expressive semantic information.",1,related,1,positive
"182 Following the MAE SSL algorithm [9], we mask 75% of 183 patches and then encode the unmasked patches with the SatViT 184 encoder.",1,related,1,positive
"For each314 downstream dataset, we sweep mask ratios {50%, 75%, 90%}315 and fine-tune our models starting from seven checkpoints to316 investigate the role of TAPT steps on downstream accuracy.317 Performing TAPT with a mask ratio of 75% for 50k‚Äì100k318 steps results in our most accurate models after fine-tuning.319
Self-pretraining [37] found that MAE pretraining on a small 320 target dataset for 10k epochs was optimal.",1,related,1,positive
Our work differs by: 1) pretraining on 143 a dataset with 7√ó more images; 2) pretraining with a higher 144 performing SSL algorithm in MAE; and 3) using a higher 145 capacity model (ViT-Base versus Swin-Tiny).,1,related,1,positive
"In this research, we select ViT as our model128 architecture and masked autoencoding (MAE) [9] as our129 SSL algorithm.130 With four recent exceptions, all transformer-based CV mod-131 els used in RS have either initialized their parameters ran-132 domly [12], [13], [14], [15], [16] or from models pretrained on133 ImageNet [17], [18], [19], [20], [21], [22].",1,related,1,positive
"273 To compare SatViT with an SOTA RGB model, we fine-tune 274 a ViT-Base model pretrained using MAE on ImageNet-1k 275 (IN1K).",1,related,1,positive
"182
Following the MAE SSL algorithm [9], we mask 75% of 183 patches and then encode the unmasked patches with the SatViT 184 encoder.",1,related,1,positive
"In this research, we select ViT as our model 128 architecture and masked autoencoding (MAE) [9] as our 129 SSL algorithm.",1,related,1,positive
"For the masking strategy, we follow the random mask sampling of a 75% ratio as in MAE (He et al., 2021).",1,related,1,positive
"On the right, we provide two simplified computational graphs of MAE (He et al., 2021) and our method to illustrate the difference.",1,related,1,positive
"During pre-training, we follow MAE (He et al., 2021) to use the Xavier initialization (Glorot & Bengio, 2010) and choose not to adopt color jittering and drop path.",1,related,0,negative
"Our pre-training setup generally follows the configurations in MAE (He et al., 2021) with AdamW (Loshchilov & Hutter, 2017) and cosine learning rate decay applied.",1,related,1,positive
"Following the sampling strategy (He et al., 2021), we randomly sample a subset without replacement and mask the remaining tokens.",1,related,1,positive
"‚Ä¶(Touvron et al., 2021) - 45.6 BEiT w Inter FT‚Ä† - 47.7 without labeled data: BEiT (Bao et al., 2021) 800 43.2 PeCo (Dong et al., 2021) 300 46.7 MAE (He et al., 2021) 1600 48.1 CAE (Chen et al., 2022) 800 48.8 CrossMAE (Ours) 300 50.4
Table 3: Results of semantic segmentation on ADE20K dataset,‚Ä¶",1,related,1,positive
"To encode multi-scale information into the detection pipeline, we integrate FPN (Lin et al., 2017) into the backbone following the setup in (Zhou et al., 2021; He et al., 2021).",1,related,1,positive
"To further allow a consistent comparison, we report the wall-clock pre-training time on the same platforms to better illustrate this advantage over the baseline MAE (He et al., 2021).",1,related,0,negative
"Note that our proposed CrossMAE is general and could be compatible with various MIM prediction targets, such as HOG (Wei et al., 2021), discrete tokens (Bao et al., 2021) and
even the pixel reconstruction as in (Xie et al., 2022; He et al., 2021).",1,related,1,positive
"The backbone transformer fnet extracts useful semantics from given xdraw and a follow-up predictor fpred maps extracted features to the target modality, such as pixels (He et al., 2021; Xie et al., 2022) and HOG (Wei et al.",1,related,1,positive
"The backbone transformer fnet extracts useful semantics from given xdraw and a follow-up predictor fpred maps extracted features to the target modality, such as pixels (He et al., 2021; Xie et al., 2022) and HOG (Wei et al., 2021).",1,related,1,positive
"Existing MIM methods majorly map the observed signal xM to a latent representation using the backbone, and then reconstruct the tokenized (Bao et al., 2021) or original signal (He et al., 2021).",1,related,1,positive
"ImageNet Experiments For the experiments on ImageNet, we follow the most standard settings and hyper-parameters in MAE (He et al., 2021) when using ViT (Dosovitskiy et al., 2020) as the backbone.",1,related,1,positive
"To further verify the superiority of the mixed multi-modality approach with ViT(MAE) and DenseNet for FGVC, we performed a new round of experiments based on the fine-grained shoe object dataset.",1,related,1,positive
"We also ran experiments by using two-CNNs (MnasNet and DenseNet), and two-ViTs (MAE and MAE-L), mixed CNNViT approach (ViTMAE and DenseNet).",1,related,1,positive
"Following the above procedure, we first constructed car object embeddings based on ViT(MAE), DenseNet, and mixed multi-model, and used k-NN classifier.",1,related,1,positive
"5(toprow)shows that, in general, our approach with both CNN and ViT obtained better classification accuracy than ViT(MAE)only and DenseNet-only models.",1,related,1,positive
"To better analyze the effect of multi-modality representations on fine-grained instance accuracy, we first conduct a t-SNE analysis with mix-network representations based on
DenseNet [44] and ViTMAE [45], followed by recognition results on the fine-grained car and shoe data.",1,related,1,positive
"2 (top-row), we extract the RGB, depth views of the object, and then fed the RGB view into ViTMAE and the depth view into DenseNet, respectively.",1,related,1,positive
"For a fair comparison, we compare our DMAE ViT-B model with the MAE ViT-B checkpoint released by He et al. (2022) in the linear probing setting on ImageNet.",1,related,1,positive
"Following He et al. (2022); Xie et al. (2022), we use ImageNet-1k as the pre-training corpus which contains 1.28 million images.",1,related,0,negative
"Like MAE (He et al., 2022), we adopt the asymmetric encoder-decoder design for DMAE.",1,related,1,positive
"To enforce the encoder (but not the decoder) to learn robust semantic features, we control the capacity of the decoder by setting a smaller value of the hidden dimension and depth following He et al. (2022).",1,related,1,positive
"For the pre-training of the two DMAE models, we set the masking ratio to 0.75 following He et al. (2022).",1,related,1,positive
"Motivated by the Masked AutoEncoder (MAE) (He et al., 2022), which learns latent representations by reconstructing missing pixels from masked images, we design a new self-supervised task called Denoising Masked AutoEncoder (DMAE).",1,related,1,positive
"For RGMIM and MAE, we used the same settings in all experiments, except for the masking strategy.",1,related,0,negative
We also conducted hyperparameter studies on the masking ratio for RGMIM and MAE.,1,related,0,negative
"And we can verify that RGMIM is more robust than MAE, especially when the masking ratio is relatively low.",1,related,0,negative
"Method Architecture 1% 5% 10% 50% 100% RGMIM ViT-Base 0.771 0.893 0.919 0.957 0.962 MAE [9] ViT-Base 0.754 0.875 0.903 0.948 0.956 SKD [24] ResNet-50 0.742 0.812 0.896 0.947 0.957 Cross [25] ResNet-50 0.747 0.795 0.817 0.934 0.953 BYOL [26] ResNet-50 0.683 0.754 0.790 0.933 0.954 SimSiam [27] ResNet-50 0.623 0.700 0.781 0.929 0.949 Transfer ViT-Base 0.689 0.861 0.893 0.940 0.953 Transfer ResNet-50 0.539 0.619 0.665 0.913 0.936 From Scratch ViT-Base 0.413 0.580 0.645 0.810 0.848 From Scratch ResNet-50 0.284 0.496 0.532 0.619 0.774
Fig.",1,related,1,positive
"We used five SOTA self-supervised learning methods as comparative methods, including masked autoencoder (MAE) [9], selfknowledge distillation based self-supervised learning (SKD) [24], cross-view self-supervised learning (Cross) [25], bootstrap your own latent (BYOL) [26], and simple siamese self-supervised learning (SimSiam) [27].",1,related,1,positive
We compared our complementary masking strategy with the random masking strategy proposed in MAE [9] in Table 2d.,1,related,1,positive
"To address this challenge, we further design a spatiotemporal autoencoder (STAR) inspired by the recent masked autoencoders (MAE) [9].",1,related,1,positive
We compared our complementary masking strategy with the random masking strategy proposed in MAE [9] in Table 2d. Results show that switching from complementary to random masking leads to a degradation in the completion performance.,1,related,1,positive
"Inspired by the idea of ‚Äùmasking‚Äù in MAE [9], we employ a similar asymmetric design as MAE yet with different purposes: MAE is to design a nontrivial self-supervisory task for pre-training via randomly masking, while the goal of STAR is to reduce the communication volume in multi-robot systems via partial broadcasting.",1,related,0,negative
"Similar to MAE [7], we find that the occlusion ratio of 75% performs the best on both the linear accuracy and supervised fine-tuning accuracy.",1,related,1,positive
"Partly inspired by MAE, we design a new self-supervised learning framework to recover the complete shapes from the highly occluded shapes.",1,related,1,positive
"In contrast, we follow MAE [15] to randomly mask image patches with a probability of 0.",1,related,1,positive
"To further show the generalization ability of our pre-trained model, we examine our model on ImageNet-1K classification task following common practice [11, 15].",1,related,1,positive
We show that this is not necessary and introduce a new model Vision-Language from Captions (VLC) built on top of Masked Auto-Encoders [15] that does not require this supervision.,1,related,1,positive
An unsupervised visual semantics is learned via Masked Auto-Encoders [15] before language is integrated.,1,related,1,positive
"To accelerate training, we follow MAE [15] and skip the mask token [MASK] in the encoder and only apply it in the lightweight decoder.",1,related,1,positive
We use a 12-layer ViT [11] initialized from MAE [15] (ImageNet-1K without labels) as our backbone; (3) Task-specific decoder.,1,related,1,positive
"For supervised learning models (i.e., ViT), we train base scale models for 300 epochs and large scale models for 200 epochs following He, Chen, Xie, Li, Doll√°r, and Girshick [13].",1,related,1,positive
"For training ViT from scratch, we report the original results [9] and the results with strong data augmentation [13].",1,related,0,negative
"The first one is from original ViT paper [9], and the second one is from He, Chen, Xie, Li, Doll√°r, and Girshick [13]‚Äôs re-implementation with strong data augmentation.",1,related,1,positive
We use supervised vision transformer (ViT) [9] and vision masked autoencoder (MAE) [13] as two representative platforms to show our insights in both theoretical and experimental analysis.,1,related,1,positive
"For large and huge models, we fine-tune them for 50 epochs following existing work [1, 13].",1,related,1,positive
"We use the per-patch normalization following He, Chen, Xie, Li, Doll√°r, and Girshick [13] for better representations.",1,related,1,positive
", ViT), we train base scale models for 300 epochs and large scale models for 200 epochs following He, Chen, Xie, Li, Doll√°r, and Girshick [13].",1,related,1,positive
"Inspired by this, we adopt the 3D Spatially Sparse Convolution proposed in SECOND [25] to build the encoder network to aggregate information from only the unmasked voxels that contain point clouds with the positional encoding module, thus our voxel masking strategy can reduce the memory complexity for training, similar to Transformer network in MAE [9].",1,related,1,positive
"Masked Autoencoders for image [9] (a), video [21] (b), (c) synthetic point clouds [13] (d) and large-scale point clouds of our Voxel-MAE.",1,related,1,positive
"Inspired by the great self-supervised learning performance of MAE [9] in 2D images, we design the pretraining network of masked autoencoders for LiDAR-based 3D object detectors to learn representative features.",1,related,1,positive
It is worth noting that our Voxel-MAE applies the 3D Spatially Sparse Convolutions to aggregate information from the unmasked data as the Transformers used in MAE [9] can not handle the largescale unmasked voxels.,1,related,1,positive
"In particular, 1) we utilize the recent ViTs pre-trained with self-supervised (MAE [25], DINO [7]) and supervised (Twins [11]) tasks, while it is an interesting future work of exploring the influence of different pre-training tasks on MVS.",1,related,0,negative
"Specifically, by considering two typical ViT backbones ‚Äì plain-ViT [16, 7, 25] and hierarchical-ViT [50, 37, 11], we implement MVSFormer-P and MVSFormer-H as in Fig.",1,related,1,positive
"In particular, 1) we utilize the recent ViTs pre-trained with self-supervised (MAE [25], DINO [7]) and supervised (Twins [11]) tasks, while it is an interesting future work of exploring the influence of different pre-training tasks on MVS.",1,related,0,negative
"Specifically, by considering two typical ViT backbones ‚Äì plainViT [16, 7, 25] and hierarchical-ViT [50, 37, 11], we implement MVSFormer-P and MVSFormer-H as in Fig.",1,related,1,positive
The effective learning rate is obtained following MAE: lr= base_lr√óglobalbatchsize / 256.,1,related,1,positive
The fine-tuning codes and checkpoints refer to the MAE repository3.,1,related,0,negative
"Based on the MAE pretrained models, we finetune 50 epochs on the SnakeCLEF 2022 dataset, and the default setting is depicted in Table 1.",1,related,1,positive
"In this paper, we use the Masked autoencoder (MAE) [27] pretrained ViT [28] models conducted on ImageNet-1K [29] training set for 800 epochs.",1,related,1,positive
We borrow the ViT-large MAE model pretrained in ImageNet1k dataset.,1,related,1,positive
"To be more specific, a ViT-based masked autoencoder (MAE) pretrained in a self-supervised manner in ImageNet is finetuned and tested in the PlantCLEF2022 dataset.",1,related,0,negative
"Another one is our model in this paper, finetuning the MAE model in PlantCLEF2022 dataset.",1,related,1,positive
Figure 7 displays the complete performance of our submissions via fine-tuning a MAE model.,1,related,0,negative
We report seven official submissions by fine-tuning the MAE model with different epochs.,1,related,0,negative
"As finetuning the ViT-large MAE model with only four RTX 3090 GPUs, we set the actual batch size 512 and train the model 100 epochs.",1,related,0,negative
Our method using MAE base all achieve a better performance with about 2% improvement over all different width and stride.,1,related,0,negative
Our methods with masked autoencoder (MAE) achieves state-of-the-art performance on all related tasks.,1,related,1,positive
"Also, we test MAE large with 30-epoch finetuning.",1,related,1,positive
"Compared with about 2% improvement over MAE base and 5% improvement over DeiT on clean accuracy, we find that certified robustness benefits more from larger model.",1,related,0,negative
"This static information can be easily inferred from other unmasked patches in the same single frames (He et al., 2022).",1,related,1,positive
"To help the model better model the trajectory, we adopt the patch norm method to normalize the trajectory values in each patch into a standard normal distribution, as has also been done in He et al. (2022).",1,related,1,positive
"For computation efficiency, we follow He et al. (2022) to only feed the unmasked patches (and their positions) to the encoder.",1,related,1,positive
This inspires us to use such an approach called masked autoencoders (MAE) [21] to pre-train transformer-based models on the target dataset.,1,related,1,positive
"We follow the fine-tuning setting almost the same as MAE [9] to use layer-wise learning rate decay, weight decay, and AdamW.",1,related,1,positive
"For a fair comparison, we directly follow most of the hyperparameters of MAE [9] in our fine-tuning experiments.",1,related,1,positive
"Masked Video Modeling
In this section, we first introduce Masked Image Modeling (MIM) for image representation learning, and then extend it to our video representation learning scheme, Masked Video Modeling (MVM) (Sections 3.1 and 3.2).",1,related,1,positive
"To obtain the patch weight Œ±, we generate the binary mask based on the random sampling strategy [35].",1,related,1,positive
"Following MAE (He et al., 2022) and BEiT v2 (Peng et al., 2022), we test the robustness of MaskDistill on three ImageNet validation sets, i.e., ImageNet-Adversarial (Hendrycks et al., 2021b), ImageNet-Rendition (Hendrycks et al., 2021a) and ImageNet-Sketch (Wang et al., 2019).",1,related,1,positive
"We evaluate our proposed K-way disjoint masking as augmentation in comparison to different MIM methods, including SimMIM [74] and MAE [32], and another data augmentation method, CutOut [18] in either fully supervised or semi-supervised settings with a base-
line, which is trained in a supervised manner and without any augmentation.",1,related,1,positive
"To apply enough perturbations to an input image in a consistency regularization framework, we propose to adopt a token masking technique as data augmentation, inspired by recent Masked Image Modeling (MIM) strategies for Transformers [6,32,74].",1,related,1,positive
"Our key ingredient is inspired by recent MIM techniques for Transformers [6,32,74].",1,related,0,negative
"We evaluate our proposed K-way disjoint masking as augmentation in comparison to different MIM methods, including SimMIM [74] and MAE [32], and another data augmentation method, CutOut [18] in either fully supervised or semi-supervised settings with a base-",1,related,1,positive
1 A Brief Revisit of MAE Masked Autoencoders (MAE) [28] is a self-supervised method for pretraining ViT by reconstructing masked RGB patches from visible patches.,1,related,1,positive
", 2019) of replacing the masked inputs with a learned token, we choose the more efficient alternative of dropping the masked inputs (He et al., 2021), which we refer as groupwise-masking.",1,related,1,positive
For masked auto-encoding pre-training we use the optimization hyperparameters from He et al. 2021 and pre-train for 500 epochs with 85% masking for most experiments because that seems to do slightly better than 75% on ImageNet in preliminary experiments.,1,related,1,positive
"Instead of using the classic masking approach (Devlin et al., 2019) of replacing the masked inputs with a learned token, we choose the more efficient alternative of dropping the masked inputs (He et al., 2021), which we refer as groupwise-masking.",1,related,1,positive
"In all the linear probing experiments, we use the embedding of the CLS token of the last layer (unlike in DINO [7], which uses the CLS token of the last four attention layers of the network and concatenates them) and perform a coarse grid search over learning rates, batch sizes and whether to normalize the data before feeding them to the linear layer or not (similarly to the added BatchNorm layer [41] in MAE [36]).",1,related,1,positive
"One practical technique to reduce the computational load, which we adopt, is to sparsify the input tokens as in VATT [1] and MAE [36].",1,related,1,positive
We adopt ViT-B pretrained by MAE on the ImageNet dataset Deng et al. (2009) as the baseline in the ablation study.,1,related,0,negative
"To alleviate the data-hungry issue of vision transformers, we use MAE pretrained vision transformers to initialize the backbone networks for human pose estimation.",1,related,0,negative
"To this end, we train a ViT-B model from scratch using MAE on the unlabelled person instances from COCO and AI Challenger human pose datasets for 1600 epochs following the same setting in He et al. (2021). Then, the pretrained model is used to initialize the backbone for pose estimation and finetuned with training images from COCO for 210 epochs.",1,related,1,positive
"To this end, we train a ViT-B model from scratch using MAE on the unlabelled person instances from COCO and AI Challenger human pose datasets for 1600 epochs following the same setting in He et al. (2021).",1,related,1,positive
"In this paper, we initialize the vision transformer backbones with MAE He et al. (2021) pretrained weights on ImageNet1K Deng et al. (2009), which contains 1M image data.",1,related,1,positive
We demonstrate that a plain vision transformer with MAE pretraining can obtain superior performance after finetuning on human pose estimation datasets.,1,related,0,negative
"x = {xi : i / ‚ààM}i=1 ‚à™ {ei : i ‚ààM}i=1, where e is the learnable embedding replacing for masked patches, we feed only the unmasked patches xU = {xi : i / ‚ààM}i=1 which similar to MAE [16].",1,related,1,positive
"Our method maximizes the mutual information between masked inputs [16,4,2] and self-supervised signals.",1,related,1,positive
"Alternatively, based on ViT [13] framework, we optimize the objective function on both pixel-level reconstruction [16,34,4] and features-level regression [2] to predict the content of masked regions.",1,related,1,positive
ImageNet-C [47] CLIP ST MUST MAE+Supervised [6] mCE ‚Üì 70.,1,related,1,positive
"Following [5, 6], we use a layer-wise learning rate decay [35] of 0.",1,related,1,positive
All networks are trainable expect that we keep frozen the parameters of pretrained BERT and MAE.,1,related,0,negative
"For image semantic analyzer and text analyzer,
we respectively use a pretrained MAE [6] and BERT [7] model to generate ris ‚àà R768√ó196 and rt ‚àà R768√ó196, where 196 and 768 are respectively the amount of tokens and the feature dimensionality.",1,related,1,positive
"Similarly, we respectively preserve the image semantics branch and the text branch, and find that both of the partial settings provide decent classifications result that can defeat MVAE [8], which verifies the power of transfer learning using pretrained BERT and MAE model.",1,related,1,positive
"we respectively use a pretrained MAE [6] and BERT [7] model to generate ris ‚àà R768√ó196 and rt ‚àà R768√ó196, where 196 and 768 are respectively the amount of tokens and the feature dimensionality.",1,related,1,positive
Our work will leverage the encoder of a pre-trained model of MAE [16] and use it as a starting point for transformer branch.,1,related,1,positive
The experiment is done with pre-trained model released by Conformer [26] to initialize the Conv Branch and pre-trained model released by MAE [16] to initialize the Model Epochs Top-1(%),1,related,1,positive
"We propose a novel SSL framework that optimizes both pixel [4, 17, 36], and feature-level losses [2, 21, 31] for brain cell image analysis, called DAMA, see Fig.",1,related,1,positive
"Our DAMA employs the image masked autoencoder modeling approach similar to [2,4,17,36] instead of augmenting the input.",1,related,1,positive
"Alternatively, based on ViT [13] framework, we optimize the objective function on both pixel-level reconstruction [4, 17, 36] and features-level regression [2] to predict the content of masked regions.",1,related,1,positive
"x = {xi : i / ‚àà M}i=1 ‚à™ {ei : i ‚àà M}i=1, where e is the learnable embedding replacing for masked patches, we feed only the unmasked patches xU = {xi : i / ‚àà M}i=1 which similar to MAE [17].",1,related,1,positive
"‚Ä¶al. (2022) requires access to an unlabeled public dataset for all models to measure the distance, and requires the communication of some datasets; He et al. (2021a) experiments with multiple methods to do personalization in decentralized learning, and our clustering-based approach can be viewed‚Ä¶",1,related,1,positive
"For masked autoencoder (He et al., 2021b) experiment on CIFAR-10, we use Vit-Tiny (Dosovitskiy et al., 2020) with AdamW optimizer for 1000 epochs with batch size 256.",1,related,1,positive
"Masked autoencoders (MAE) are scalable self-supervised learners for CV, and the approach is clear: we just need to mask random patches of the input image and reconstruct the missing pixels [29].",1,related,1,positive
"We provide a grayscale image and colorized images from 10 different methods: CIC [32], DeOldify[1], ChromaGAN [28], InstColor [26], GCP [31], SwinIR [21], Uformer [29], MAE [13], ColTran [17] and ours.",1,related,1,positive
"We also compare our method with 4 advanced transformer-based methods, including: (i) two state-of-the-art image restoration approaches, SwinIR [21] and Uformer [29], by retraining models on the colorization task; (ii) the stateof-the-art self-supervised learner MAE [13], by finetuning its pretrained weights to colorization as a downstream task; and (iii) the state-of-the-art colorization methods ColTran [17] with same experiment settings.",1,related,1,positive
"We compare our methods with five representative methods, i.e., supervisedlearning-based pretraining [3], generative-model-based methods (VAE) [39], pretext-task-based algorithm (Jigsaw) [14], contrastive-learning-based method (MoCo) [23], and transformer (MAEs) [28].",1,related,1,positive
"In addition, we compare our pretraining method with representative transformer-based methods, such as ViT [25], Swin [26], BEiTs [27], and MAEs [28].",1,related,1,positive
"Inspired by the excellent performance of masked autoencoding in NLP [7] and 2D vision [13], we design the masked voxel autoencoding network for 3D perception.",1,related,1,positive
"Masked autoencoding in NLP [7] and 2D vision [13] adopts the Transformer network to perform self-attention on the unmasked portions of the training data, which are not influenced by the masked portions.",1,related,1,positive
"In our experiments, we compare our approach to other approaches that attempt to leverage large, diverse datasets via representation learning (Mandlekar et al., 2020; Yang & Nachum, 2021; Yang et al., 2021; Nair et al., 2022; He et al., 2021), as well as other methods for learning from human demonstrations, such as behavioral cloning methods with expressive policy architectures (Shafiullah et al.",1,related,1,positive
"Pre-trained MAE initialization (He et al., 2021): We took a similar training procedure to R3M for our MAE representation.",1,related,0,negative
"Visual and language encoder We explore different pre-trained foundation models including MoCo [22], DenseCL [59], MAE [21], GLIP [30], CLIP [46] and MDETR [27] as the visual and language encoder to train a policy, and evaluate their performances without self-play and self-describe on the compositional generalization experiment with two evaluation protocols.",1,related,1,positive
"In this paper, we study these questions by comparing the representations of a standard ViT-Base model [5] trained with 16x16 image patches (ViT-B/16) on the ImageNet [6] dataset across popular contrastive (MoCo-V3 [2] and DINO [3]) and reconstructive (MAE [4]) methods using Centred",1,related,1,positive
We tackle the challenging task of training GAN with limited data from a perspective of image masking training [21].,1,related,0,negative
"We therefore introduce an additional ‚Äúmask shift‚Äù to allow the masked patches to appear at any image locations (instead of fixed grid locations only as in the spatial masking in MAE [21, 2]), which helps generate images with high-fidelity along spatial dimensions.",1,related,1,positive
"Inspired by BERT [10] and MAE [20], we propose a self-supervised graph transformer model named Graph Masked Autoencoders (GMAE).",1,related,1,positive
"Similarly as [30], we employ random masking of 75% of the patches.",1,related,1,positive
"Secondly, inspired by the success of Masked Autoencoder (MAE) [13], we design a mask-and-replace strategy to intentionally alter the true label to analyze the influence of the accuracy of the annotation.",1,related,1,positive
Recent efforts [He et al. 2021] on robust decoders can help in this goal but we need to further account for the unique geometric context available in our setup.,1,related,1,positive
"A masked auto encoder as proposed by [9] attempts to build suitable latent representation by being trained on cropped/masked portions of the images, forcing it to learn and identify classes based on partial images.",1,related,1,positive
"We‚Äôre specifically looking at [9], a masked autoencoder that we hope may be able to provide suitable latent representations.",1,related,1,positive
"255 As prior work has demonstrated that fine-tuning outper- 256 forms linear classification on most datasets [29], at the second 257 stage, the backbone from the online encoder was frozen for 258 five epochs and then unfroze, and we then trained the whole 259 model with 100 epochs on a single A100 GPU card.",1,related,1,positive
"We compare GPF with other tuning methods described as follows:
‚Ä¢ PARTIAL-k: We finetune the last k layers of the model with the classfication head and freeze other parts, which is utilized in Zhang et al. (2016); He et al. (2021); Jia et al. (2022).",1,related,1,positive
"For transformer, we leverage pre-trained models on ImageNet (Deng et al., 2009) from ViT (Dosovitskiy et al., 2021), DeiT (Touvron et al., 2021), DINO (Caron et al., 2021), MoCo-v3 (Chen et al., 2021b), and MAE (He et al., 2022).",1,related,1,positive
"Inspired by the form of the training sample, we propose the center-masked pre-training task, which is similar to MAE [18] but our method is easier to implement.",1,related,0,negative
"The MQP differs from the standard self-supervised approaches of using unsupervised data (He et al., 2021; Hendrycks et al., 2019; You et al., 2021) in that the supervised signal is used to select relevant passage as the context of the query.",1,related,1,positive
"For the first challenge, we follow the pretrained models in language [11,26,78] and vision [7,32] modeling to construct the supervision signals, i.",1,related,1,positive
"We also hope to incorporate self-supervised methods 499 that learn representations by training on a union of unlabeled data from source and target via proxy 500 tasks like reconstruction [24, 28] and contrastive learning [12, 14].",1,related,1,positive
"Following common practices [76, 77], we design two transfer strategies with different parameters Œòf to optimize: Fixed Feature Extractor Fixed Feature Extractor",1,related,1,positive
"‚Äò*‚Äô denotes that we end-to-end finetune RegionCL-M pretrained models for 50 epochs [8,1].",1,related,1,positive
"‚Äò*‚Äô denotes that we end-to-end finetune RegionCLM pretrained models for 50 epochs [8,1].",1,related,1,positive
"1We denote by MAE the general autoencoder with masked inputs of different modalities, not only masked 2D images [13].",1,related,1,positive
"We believe that the obtained decomposed feature embeddings can make the image fusion easier, so that the fused images are generated by a simple convolutional layer called projector, which just likes the last linear layer for classification in general self-supervised learning [10].",1,related,1,positive
"In addition, we implement a patched masking like [2].",1,related,1,positive
We utilize ViT-base backbone pretrained by MAE [21].,1,related,1,positive
This autoencoder learns to predict masked (unknown) dimensions of an observation based on the recorded (known) ones [14].,1,related,1,positive
"We also compare Model Assembling with the recently proposed improved E2E baselines in [17], where a systematically hyper-parameter search is performed on training configurations.",1,related,1,positive
E2E-ViT [12] E2E-DeiT [36] E2E [17] DeiT + Ours E2E-ViT/DeiT E2E [17] DeiT + Ours,1,related,1,positive
"One can observe that our method shows a better efficiency-accuracy trade-off than all E2E baselines, including the recently proposed improved E2E baselines [17].",1,related,0,negative
"Following [63], we apply the regularization in a denoising autoencoding manner.",1,related,1,positive
"Then we randomly mask the latent patch tokens z by replacing the selected masked tokens with a shared and learned mask token [14, 27].",1,related,1,positive
"We expect methods like self-supervised learning[13, 14, 15] to bring more improvements.",1,related,1,positive
MVP 126 [3] learn a policy with PPO and use a pre-trained visual encoder for feature extraction in addition to 127 proprioceptive state information; the pre-trained representation is an MAE [30] trained on frames 128 from diverse human videos.,1,related,0,negative
MVP126 [3] learn a policy with PPO and use a pre-trained visual encoder for feature extraction in addition to127 proprioceptive state information; the pre-trained representation is an MAE [30] trained on frames128 from diverse human videos.,1,related,0,negative
"We have also tried learnable prototypes similar to learnable tokens in the Transformer-based models [10, 14, 9], but do not observe performance gain.",1,related,1,positive
"In our experiments, we compare our approach to other approaches that attempt to leverage large, diverse datasets via representation learning (Mandlekar et al., 2020; Yang & Nachum, 2021; Yang et al., 2021; Nair et al., 2022; He et al., 2021), as well as other methods for learning from human demonstrations, such as behavioral cloning methods with expressive policy architectures (Shafiullah et al.",1,related,1,positive

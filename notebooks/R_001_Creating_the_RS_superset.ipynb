{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5268711",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "R001_Fall_2023_AI_Reproducibility__\n",
    "|\n",
    "R001_Creating_the_RS_superset.ipynb\n",
    "Created on Tue Sep  5 20:25:12 2023\n",
    "@author: Rochana Obadage\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c61bdf",
   "metadata": {},
   "source": [
    "# ``RS documents download and metadata extraction - Superset`` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb7bf44",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### `dependencies`\n",
    "> <font face=\"consolas\" color='#000080'><b>1. python:</b><br> \n",
    "    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;3.8.17<br><br>\n",
    "<b>2. Jupyter Notebook:</b><br>\n",
    "    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;notebook server: 6.5.4<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Kernel Information: Python 3.8.17 (default, Jul  5 2023, 20:44:21) [MSC v.1916 64 bit (AMD64)]<br><br>\n",
    "<b>3. conda installations:</b><br>\n",
    "    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;pandas==2.0.3<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Beatifulsoup4==4.11.1<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Selenium==4.11.2<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;conda install -c conda-forge selenium<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;webdriver-manager=4.0.0<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;conda install -c conda-forge webdriver-manager<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;bibtexparser-1.4.0<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;conda install -c conda-forge bibtexparser!<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;pdfminer.six-20221105<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;conda install -c conda-forge pdfminer.six<br>\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa3f6c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.chrome.service import Service as ChromeService \n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "from bibtexparser.bparser import BibTexParser\n",
    "from bibtexparser.customization import convert_to_unicode\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import urllib\n",
    "import time\n",
    "import string\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import bibtexparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fc1bb0",
   "metadata": {},
   "source": [
    "## ``All RS documents from 2021 to back ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d314788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cs_oruma001\\AppData\\Local\\Temp\\ipykernel_22112\\1212157067.py:11: DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')\n",
      "  options.headless = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "RS_046_MLRC_2021_01\n",
      "ML Reproducibility Challenge 2021\n",
      "RS_047_MLRC_2021_02\n",
      "[Re] Counterfactual Generative Networks\n",
      "RS_048_MLRC_2021_03\n",
      "[Re] Does Self-Supervision Always Improve Few-Shot Learning?\n",
      "RS_049_MLRC_2021_04\n",
      "[Re] Weakly-Supervised Semantic Segmentation via Transformer Explainability\n",
      "RS_050_MLRC_2021_05\n",
      "[Re] Reproducibility Study of “Counterfactual Generative Networks”\n",
      "RS_051_MLRC_2021_06\n",
      "[Re] Data-Driven Methods for Balancing Fairness and Efficiency in Ride-Pooling\n",
      "RS_052_MLRC_2021_07\n",
      "[¬Re] Hate Speech Detection based on Sentiment Knowledge Sharing\n",
      "RS_053_MLRC_2021_08\n",
      "[Re] Reproducibility Study - SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition\n",
      "RS_054_MLRC_2021_09\n",
      "[Re] AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients\n",
      "RS_055_MLRC_2021_10\n",
      "[Re] GANSpace: Discovering Interpretable GAN Controls\n",
      "RS_056_MLRC_2021_11\n",
      "[Re] Replication study of \"Privacy-preserving Collaborative Learning\"\n",
      "RS_057_MLRC_2021_12\n",
      "[Re] A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space\n",
      "RS_058_MLRC_2021_13\n",
      "[Re] Reproduction and Extension of \"Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation\"\n",
      "RS_059_MLRC_2021_14\n",
      "[Re] Reproduction Study of Variational Fair Clustering\n",
      "RS_060_MLRC_2021_15\n",
      "[Re] Explaining in Style: Training a GAN to explain a classifier in StyleSpace\n",
      "RS_061_MLRC_2021_16\n",
      "[Re] An Implementation of Fair Robust Learning\n",
      "RS_062_MLRC_2021_17\n",
      "[Re] Understanding Self-Supervised Learning Dynamics without Contrastive Pairs\n",
      "RS_063_MLRC_2021_18\n",
      "[Re] Domain Generalization using Causal Matching\n",
      "RS_064_MLRC_2021_19\n",
      "[¬Re] Reproducibility Study of 'Exacerbating Algorithmic Bias through Fairness Attacks' \n",
      "RS_065_MLRC_2021_20\n",
      "[Re] Strategic classification made practical: reproduction\n",
      "RS_066_MLRC_2021_21\n",
      "[Re] Replication study of 'Explaining in Style: Training a GAN to explain a classifier in StyleSpace'\n",
      "RS_067_MLRC_2021_22\n",
      "[Re] Exacerbating Algorithmic Bias through Fairness Attacks\n",
      "RS_068_MLRC_2021_23\n",
      "[Re] Thompson Sampling for Bandits with Clustered Arms\n",
      "RS_069_MLRC_2021_24\n",
      "[Re] Replication Study of \"Fairness and Bias in Online Selection\"\n",
      "RS_070_MLRC_2021_25\n",
      "[Re] Projection-based Algorithm for Updating the TruncatedSVD of Evolving Matrices\n",
      "RS_071_MLRC_2021_26\n",
      "[Re] Background-Aware Pooling and Noise-Aware Loss for Weakly-Supervised Semantic Segmentation\n",
      "RS_072_MLRC_2021_27\n",
      "[Re] Reproducibility Study: Comparing Rewinding and Fine-tuning in Neural Network Pruning\n",
      "RS_073_MLRC_2021_28\n",
      "[Re] Exacerbating Algorithmic Bias through Fairness Attacks\n",
      "RS_074_MLRC_2021_29\n",
      "[Re] Replication study of 'Data-Driven Methods for Balancing Fairness and Efficiency in Ride-Pooling'\n",
      "RS_075_MLRC_2021_30\n",
      "[Re] Replicating and Improving GAN2Shape Through Novel Shape Priors and Training Steps\n",
      "RS_076_MLRC_2021_31\n",
      "[Re] Value Alignment Verification\n",
      "RS_077_MLRC_2021_32\n",
      "[Re] Replication Study of \"Fairness and Bias in Online Selection\"\n",
      "RS_078_MLRC_2021_33\n",
      "[¬Re] Reproducing 'Fair Selective Classification via Sufficiency'\n",
      "RS_079_MLRC_2021_34\n",
      "[Re] Differentiable Spatial Planning using Transformers\n",
      "RS_080_MLRC_2021_35\n",
      "[Re] Solving Phase Retrieval With a Learned Reference\n",
      "RS_081_MLRC_2021_36\n",
      "[Re] Reproducibility Report: Contrastive Learning of Socially-aware Motion Representations\n",
      "RS_082_MLRC_2021_37\n",
      "[Re] From goals, waypoints and paths to longterm human trajectory forecasting\n",
      "RS_083_MLRC_2021_38\n",
      "[Re] Graph Edit Networks\n",
      "RS_084_MLRC_2021_39\n",
      "[Re] Learning to count everything\n",
      "RS_085_MLRC_2021_40\n",
      "[Re] Badder Seeds: Reproducing the Evaluation of Lexical Methods for Bias Measurement\n",
      "RS_086_MLRC_2021_41\n",
      "[Re] Transparent Object Tracking Benchmark\n",
      "RS_087_MLRC_2021_42\n",
      "[Re] Explaining in Style: Training a GAN to explain a classifier in StyleSpace\n",
      "RS_088_MLRC_2021_43\n",
      "[Re] Replication Study of DECAF: Generating Fair Synthetic Data Using Causally-Aware Generative Networks\n",
      "RS_089_MLRC_2021_44\n",
      "[Re] Privacy-preserving collaborative learning with automatic transformation search\n",
      "RS_090_MLRC_2021_45\n",
      "[Re] Robust Counterfactual Explanations on Graph Neural Networks\n",
      "RS_091_MLRC_2021_46\n",
      "[Re] Lifting 2D StyleGAN for 3D-Aware Face Generation\n",
      "RS_092_MLRC_2021_47\n",
      "[Re] Learning Unknown from Correlations: Graph Neural Network for Inter-novel-protein Interaction Prediction\n",
      "RS_093_MLRC_2021_48\n",
      "[Re] Nondeterminism and Instability in Neural Network Optimization\n",
      "RS_094_MLRC_2020_01\n",
      "[Re] Explaining Groups of Points in Low-Dimensional Representations\n",
      "RS_095_MLRC_2020_02\n",
      "[Re] On end-to-end 6DoF object pose estimation and robustness to object scale\n",
      "RS_096_MLRC_2020_03\n",
      "[Re] Neural Networks Fail to Learn Periodic Functions and How to Fix It\n",
      "RS_097_MLRC_2020_04\n",
      "[Re] Deep Fair Clustering for Visual Learning\n",
      "RS_098_MLRC_2020_05\n",
      "[Re] Training Binary Neural Networks using the Bayesian Learning Rule\n",
      "RS_099_MLRC_2020_06\n",
      "[Re] Reproducing Learning to Deceive With Attention-Based Explanations\n",
      "RS_100_MLRC_2020_07\n",
      "[Re] Parameterized Explainer for Graph Neural Network\n",
      "RS_101_MLRC_2020_08\n",
      "[Re] Don't Judge an Object by Its Context: Learning to Overcome Contextual Bias\n",
      "RS_102_MLRC_2020_09\n",
      "[Re] Reimplementation of FixMatch and Investigation on Noisy (Pseudo) Labels and Confirmation Errors of FixMatch\n",
      "RS_103_MLRC_2020_10\n",
      "[Re] A Reproduction of Ensemble Distribution Distillation\n",
      "RS_104_MLRC_2020_11\n",
      "[Re] Learning Memory Guided Normality for Anomaly Detection\n",
      "RS_105_MLRC_2020_12\n",
      "[Re] Spatial-Adaptive Network for Single Image Denoising\n",
      "RS_106_MLRC_2020_13\n",
      "[Re] Can gradient clipping mitigate label noise?\n",
      "RS_107_MLRC_2020_14\n",
      "[Re] Warm-Starting Neural Network Training\n",
      "RS_108_MLRC_2020_15\n",
      "[Re] Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings\n",
      "RS_109_MLRC_2020_16\n",
      "[Re] Explaining Groups of Points in Low-Dimensional Representations\n",
      "RS_110_MLRC_2020_17\n",
      "[Re] Reproducing 'Identifying through flows for recovering latent representations'\n",
      "RS_111_MLRC_2020_18\n",
      "@article{Balsells Rodas:2021,\n",
      "  author = {Balsells Rodas, Carles and Canal Anton, Oleguer and Taschin, Federico},\n",
      "  title = {{[Re] Hamiltonian Generative Networks}},\n",
      "  journal = {ReScience C},\n",
      "  year = {2021},\n",
      "  month = may,\n",
      "  volume = {7},\n",
      "  number = {2},\n",
      "  pages = {{#18}},\n",
      "  doi = {10.5281/zenodo.4835278},\n",
      "  url = {https://zenodo.org/record/4835278/files/article.pdf},\n",
      "  code_url = {https://github.com/CampusAI/Hamiltonian-Generative-Networks},\n",
      "  code_doi = {10.5281/zenodo.4673473},\n",
      "  code_swh = {swh:1:dir:7c7cee4b1c4dc7dfa46b4b4bf5c3b8b20deb5b26},\n",
      "  data_url = {},\n",
      "  data_doi = {},\n",
      "  review_url = {https://openreview.net/forum?id=Zszk4rXgesL},\n",
      "  type = {Replication},\n",
      "  language = {Python},\n",
      "  domain = {ML Reproducibility Challenge 2020},\n",
      "  keywords = {Hamiltonian, generative network, variational, python, pytorch}\n",
      "}\n",
      "\n",
      "RS_112_MLRC_2020_19\n",
      "[Re] Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention\n",
      "RS_113_MLRC_2020_20\n",
      "[Re] Reproducibility report of \"Interpretable Complex-Valued Neural Networks for Privacy Protection\"\n",
      "RS_114_MLRC_2020_21\n",
      "[Re] Rigging the Lottery: Making All Tickets Winners\n",
      "RS_115_MLRC_2020_22\n",
      "[Re] Reproducibility study - Does enforcing diversity in hidden states of LSTM-Attention models improve transparency?\n",
      "RS_116_MLRC_2020_23\n",
      "[Re] Replication Study of 'Generative causal explanations of black-box classifiers'\n",
      "RS_117_MLRC_2020_24\n",
      "ML Reproducibility Challenge 2020\n",
      "RS_118_NeurIPS_2019_01\n",
      "NeurIPS 2019 Reproducibility Challenge\n",
      "RS_119_NeurIPS_2019_02\n",
      "[Re] A comprehensive study on binary optimizer and its applicability\n",
      "RS_120_NeurIPS_2019_03\n",
      "[Re] Generative Modeling by Estimating Gradients of the Data Distribution\n",
      "RS_121_NeurIPS_2019_04\n",
      "[Re] Unsupervised Scalable Representation Learning for Multivariate Time Series\n",
      "RS_122_NeurIPS_2019_05\n",
      "[Re] Tensor Monte Carlo: Particle Methods for the GPU Era\n",
      "RS_123_NeurIPS_2019_06\n",
      "[Re] Hamiltonian Neural Networks\n",
      "RS_124_NeurIPS_2019_07\n",
      "[Re] Zero-Shot Knowledge Transfer via Adversarial Belief Matching\n",
      "RS_125_NeurIPS_2019_08\n",
      "[Re] When to Trust Your Model: Model-Based Policy Optimization\n",
      "RS_126_NeurIPS_2019_09\n",
      "[Re] Unsupervised Representation Learning in Atari\n",
      "RS_127_NeurIPS_2019_10\n",
      "[Re] One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers\n",
      "RS_128_NeurIPS_2019_11\n",
      "[Re] Improved Calibration and Predictive Uncertainty for Deep Neural Networks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RS_129_ICLR_2019_01\n",
      "ICLR Reproducibility Challenge 2019\n",
      "RS_130_ICLR_2019_02\n",
      "[Re] Meta-learning with differentiable closed-form solvers\n",
      "RS_131_ICLR_2019_03\n",
      "[Re] Variational Sparse Coding\n",
      "RS_132_ICLR_2019_04\n",
      "[Re] Learning Neural PDE Solvers with Convergence Guarantees ICLR Reproducibility Challenge 2019\n",
      "RS_133_ICLR_2019_05\n",
      "[Re] h-detach: Modifying the LSTM gradient towards better optimization\n"
     ]
    }
   ],
   "source": [
    "FOLDER_NAME = 'RS_2019_2021_REP_ALL/'\n",
    "\n",
    "if not os.path.exists(FOLDER_NAME):\n",
    "    os.makedirs(FOLDER_NAME)\n",
    "\n",
    "# instantiate options \n",
    "options = webdriver.ChromeOptions() \n",
    " \n",
    "# run browser in headless mode \n",
    "options.headless = True \n",
    " \n",
    "# instantiate driver \n",
    "driver = webdriver.Chrome(service=ChromeService(\n",
    "    ChromeDriverManager().install()), options=options) \n",
    "\n",
    "parser = BibTexParser()\n",
    "parser.customization = convert_to_unicode\n",
    "\n",
    "# load website \n",
    "url = 'https://rescience.github.io/read'\n",
    "\n",
    "# get the entire website content \n",
    "driver.get(url) \n",
    "\n",
    "aop_items = []\n",
    "printable = set(string.printable)\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "print(type(soup))\n",
    "\n",
    "results_MLRC_2021 = soup.find_all(\"ol\", class_=\"bibliography\")[1]\n",
    "results_MLRC_2020 = soup.find_all(\"ol\", class_=\"bibliography\")[3]\n",
    "results_NeurIPS_2019 = soup.find_all(\"ol\", class_=\"bibliography\")[6]\n",
    "results_ICLR_2019 = soup.find_all(\"ol\", class_=\"bibliography\")[9]\n",
    "\n",
    "results_for_all_years = {'2021_MLRC':results_MLRC_2021,'2020_MLRC':results_MLRC_2020,\n",
    "                         '2019_NeurIPS':results_NeurIPS_2019,'2019_ICLR':results_ICLR_2019}\n",
    "\n",
    "challenge_year = '2021'\n",
    "start_key = 46\n",
    "internal_key = 0\n",
    "\n",
    "# meta_data\n",
    "key_for_all_RS = \"\"\n",
    "title_rs = \"\"\n",
    "authors_rs = \"\"\n",
    "article_type = \"\"\n",
    "\n",
    "keywords = \"\"\n",
    "abstract = \"\"\n",
    "pdf_url_rs = \"\"\n",
    "code_url_rs = \"\"\n",
    "\n",
    "org_paper_url = \"\"\n",
    "doi_rs = \"\"\n",
    "code_doi_rs = \"\"\n",
    "doi_org = \"\"\n",
    "venue_name_rs = \"\"\n",
    "challenge_year_rs = \"\"\n",
    "review_url_rs = \"\"\n",
    "\n",
    "journal_rs = \"\"\n",
    "volume_journal_rs = \"\"\n",
    "issue_journal_rs = \"\" \n",
    "published_year_journal_rs = \"\"\n",
    "\n",
    "domain = \"\"\n",
    "entry_type = \"\"\n",
    "\n",
    "all_meta_data_keys = ['key_for_all_RS', 'title_rs', 'authors_rs', 'article_type', 'keywords', 'abstract', \n",
    "                      'pdf_url_rs', 'code_url_rs', 'org_paper_url', 'doi_rs', 'code_doi_rs', 'doi_org', 'venue_name_rs', \n",
    "                      'challenge_year_rs', 'review_url_rs', 'journal_rs', 'volume_journal_rs', 'issue_journal_rs', \n",
    "                      'published_year_journal_rs', 'domain', 'entry_type']\n",
    "\n",
    "row_items = []\n",
    "\n",
    "for year, result_set in results_for_all_years.items():\n",
    "    challenge_year, challenge = year.split('_')\n",
    "    \n",
    "    for key, item in enumerate(result_set):\n",
    "        if isinstance(item, NavigableString):\n",
    "            continue\n",
    "\n",
    "        if isinstance(item, Tag):\n",
    "            internal_key +=1\n",
    "\n",
    "            bib = item.select(\"div pre\")[0]\n",
    "            library_ = bibtexparser.loads(bib.text)\n",
    "            \n",
    "            key_for_all_rs = 'RS_{:0>3}_{}_{}_{:0>2}'.format(start_key,challenge,challenge_year,internal_key)\n",
    "            print(key_for_all_rs)\n",
    "            start_key += 1\n",
    "    \n",
    "            if library_.entries == []:\n",
    "                title_rs = bib.text\n",
    "                print(bib.text)\n",
    "    \n",
    "            else:\n",
    "                details_dict = library_.entries[0]\n",
    "                \n",
    "                key_for_all_RS = key_for_all_rs\n",
    "                title_rs = details_dict['title'].replace('{','').replace('}','')\n",
    "                print(title_rs)\n",
    "                authors_rs = details_dict['author']\n",
    "                article_type = details_dict['type']\n",
    "\n",
    "                keywords = details_dict['keywords']\n",
    "                abstract = 'DO EXTRACT FROM review_url'\n",
    "                pdf_url_rs = details_dict['url']\n",
    "                code_url_rs = details_dict['code_url']\n",
    "\n",
    "                org_paper_url = 'DO EXTRACT FROM review_url'\n",
    "                doi_rs = details_dict['doi']\n",
    "                code_doi_rs = details_dict['code_doi']\n",
    "                doi_org = ''\n",
    "                venue_name_rs = challenge\n",
    "                challenge_year_rs = challenge_year\n",
    "                review_url_rs = details_dict['review_url']\n",
    "\n",
    "                journal_rs = details_dict['journal']\n",
    "                volume_journal_rs = details_dict['volume']\n",
    "                issue_journal_rs = details_dict['number'] \n",
    "                published_year_journal_rs = details_dict['year']\n",
    "\n",
    "                domain = details_dict['domain']\n",
    "                entry_type = details_dict['ENTRYTYPE']\n",
    "                \n",
    "                if start_key > 128:\n",
    "\n",
    "                    response = requests.get(pdf_url_rs)\n",
    "                    file_save_name = FOLDER_NAME+key_for_all_RS +'.pdf'\n",
    "                    pdf = open(file_save_name, 'wb')\n",
    "                    pdf.write(response.content)\n",
    "                    pdf.close()\n",
    "            \n",
    "            dict_ = dict()\n",
    "            \n",
    "            for key in all_meta_data_keys:\n",
    "                dict_.update({key:eval(key)})\n",
    "\n",
    "            row_items.append(dict_)\n",
    "            \n",
    "    internal_key = 0\n",
    "    \n",
    "driver.quit()\n",
    "    \n",
    "df = pd.DataFrame(row_items)\n",
    "df.to_csv(\"RS_ALL_IN_ONE_2019_2021.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'keywords': 'multi-type inter-novel-protein interaction, graph neural networks, fair evaluation, python, rescience c, replication, machine learning', \n",
    "  'domain': 'ML Reproducibility Challenge 2021', 'language': 'Python', \n",
    "  'type': 'Replication', 'review_url': 'https://openreview.net/forum?id=Hc8GOhfmhRF', \n",
    "  'data_doi': '', 'data_url': '', 'code_swh': 'swh:1:dir:6eedc394f714587f35840bee0aac3e675bfa6c5a', \n",
    "  'code_doi': '10.5281/zenodo.6511807', 'code_url': 'https://github.com/zrimseku/Reproducibility-Challenge', \n",
    "  'url': 'https://zenodo.org/record/6574721/files/article.pdf', 'doi': '10.5281/zenodo.6574721', 'pages': '{#47}', \n",
    "  'number': '2', 'volume': '8', 'month': 'May', 'year': '2022', 'journal': 'ReScience C', \n",
    "  'title': '{[Re] Learning Unknown from Correlations: Graph Neural Network for Inter-novel-protein Interaction Prediction}', \n",
    "  'author': 'Zrimšek, Urša', 'ENTRYTYPE': 'article', 'ID': 'Zrimsek:2022'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f8dc7d",
   "metadata": {},
   "source": [
    "## ``All RS documents from MLRC 2022 ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9e21929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "2\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "3\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "4\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "5\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "6\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "7\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "8\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "9\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "10\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "11\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "12\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "13\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "14\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "15\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "16\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "17\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "18\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "19\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "20\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "21\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "22\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "23\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "24\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue_name', 'venue', 'venueid', 'journal', 'doi']\n",
      "25\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "27\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "28\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'TL;DR', 'venue', 'venueid', 'journal', 'doi']\n",
      "29\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "30\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "31\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "32\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "33\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "34\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "35\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "36\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "37\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "38\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "39\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "40\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue_name', 'venue', 'venueid', 'journal', 'doi']\n",
      "41\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paperhash', 'paper_url', 'paper_venue', 'venue_name', 'venue', 'venueid', 'journal', 'doi']\n",
      "42\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "43\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "44\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "45\n",
      "keys_initial_list :\n",
      " ['title', 'authors', 'TL;DR', 'keywords', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'venue_name', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n",
      "\n",
      "valid_keys :\n",
      " ['title', 'authors', 'keywords', 'TL;DR', 'abstract', 'pdf', 'paper_url', 'paper_venue', 'paperhash', 'venue', 'venueid', 'journal', 'doi']\n"
     ]
    }
   ],
   "source": [
    "df_accept_1 = pd.read_csv('MLRC_2022_Accepted_ALL_in_one.csv')\n",
    "\n",
    "challenge_year = '2022'\n",
    "start_key = 1\n",
    "internal_key = 0\n",
    "\n",
    "# meta_data\n",
    "key_for_all_RS = \"\"\n",
    "title_rs = \"\"\n",
    "authors_rs = \"\"\n",
    "article_type = \"\"\n",
    "\n",
    "keywords = \"\"\n",
    "abstract = \"\"\n",
    "pdf_url_rs = \"\"\n",
    "code_url_rs = \"\"\n",
    "\n",
    "org_paper_url = \"\"\n",
    "doi_rs = \"\"\n",
    "code_doi_rs = \"\"\n",
    "doi_org = \"\"\n",
    "\n",
    "venue_name_rs = \"\"\n",
    "challenge_year_rs = \"\"\n",
    "\n",
    "review_url_rs = \"\"\n",
    "\n",
    "journal_rs = \"\"\n",
    "volume_journal_rs = \"\"\n",
    "issue_journal_rs = \"\" \n",
    "published_year_journal_rs = \"\"\n",
    "\n",
    "\n",
    "domain = \"\"\n",
    "entry_type = \"\"\n",
    "\n",
    "all_meta_data_keys = ['key_for_all_RS', 'title_rs', 'authors_rs', 'article_type', 'keywords', 'abstract', \n",
    "                      'pdf_url_rs', 'code_url_rs', 'org_paper_url', 'doi_rs', 'code_doi_rs', 'doi_org', 'venue_name_rs', \n",
    "                      'challenge_year_rs', 'review_url_rs', 'journal_rs', 'volume_journal_rs', 'issue_journal_rs', \n",
    "                      'published_year_journal_rs', 'domain', 'entry_type']\n",
    "\n",
    "row_items = []\n",
    "\n",
    "for index, row in df_accept_1.iterrows():\n",
    "\n",
    "    # Page data extraction\n",
    "    page = requests.get(row[\"re_details\"])\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find(id=\"content\")\n",
    "    \n",
    "    # Meta-Data extraction\n",
    "    details = soup.find(id=\"__NEXT_DATA__\")\n",
    "    full_dict = json.loads(details.text)\n",
    "    content_dict = full_dict['props']['pageProps']['forumNote']['content']\n",
    "    \n",
    "    print(index+1)\n",
    "    key_for_all_rs = 'RS_{:0>3}_{}_{}_{:0>2}'.format(index+1,challenge,challenge_year,index+1)\n",
    "\n",
    "    keys_initial_list = ['title','authors','TL;DR','keywords','abstract','pdf',\n",
    "                        'paper_url','paper_venue','venue_name','paperhash',\n",
    "                         'venue','venueid','journal','doi'\n",
    "                        ]\n",
    "\n",
    "    # test = [key for key in new_list if key in initial_list_] \n",
    "    print(\"keys_initial_list :\\n\",keys_initial_list)\n",
    "    valid_keys = list(filter(lambda x: x in keys_initial_list,content_dict.keys()))\n",
    "    print(\"\\nvalid_keys :\\n\",valid_keys)\n",
    "    \n",
    "    key_for_all_RS = key_for_all_rs\n",
    "    title_rs = content_dict['title'] if 'title' in valid_keys else \"\"\n",
    "    authors_rs = content_dict['authors'] if 'authors' in valid_keys else \"\"\n",
    "    article_type = \"Replication\"\n",
    "\n",
    "    keywords = content_dict['keywords'] if 'keywords' in valid_keys else \"\"\n",
    "    abstract = content_dict['abstract'] if 'abstract' in valid_keys else \"\"\n",
    "    pdf_url_rs = content_dict['pdf'] if 'pdf' in valid_keys else \"\"\n",
    "    code_url_rs = \"\"\n",
    "    org_paper_url = content_dict['paper_url'] if 'paper_url' in valid_keys else \"\"\n",
    "    \n",
    "    doi_rs = content_dict['doi'] if 'doi' in valid_keys else \"\"\n",
    "    code_doi_rs = ''\n",
    "    doi_org = ''\n",
    "    \n",
    "    venue_name_rs = 'MLRC'\n",
    "    challenge_year_rs = '2022'\n",
    "    review_url_rs = row[\"re_details\"]\n",
    "\n",
    "    journal_rs = ''\n",
    "    volume_journal_rs = ''\n",
    "    issue_journal_rs = ''\n",
    "    published_year_journal_rs = ''\n",
    "\n",
    "    domain = 'ML Reproducibility Challenge 2021'\n",
    "    entry_type = 'article'  \n",
    "            \n",
    "    dict_ = dict()\n",
    "\n",
    "    for key in all_meta_data_keys:\n",
    "        dict_.update({key:eval(key)})\n",
    "\n",
    "    row_items.append(dict_)          \n",
    "    \n",
    "df = pd.DataFrame(row_items)\n",
    "df.to_csv(\"RS_ALL_IN_ONE_2022.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f9b23e",
   "metadata": {},
   "source": [
    "## ``Automated original paper url extraction``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05502e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_for_all_RS</th>\n",
       "      <th>title_rs</th>\n",
       "      <th>authors_rs</th>\n",
       "      <th>article_type</th>\n",
       "      <th>keywords</th>\n",
       "      <th>abstract</th>\n",
       "      <th>pdf_url_rs</th>\n",
       "      <th>code_url_rs</th>\n",
       "      <th>org_paper_url</th>\n",
       "      <th>doi_rs</th>\n",
       "      <th>...</th>\n",
       "      <th>challenge_year_rs</th>\n",
       "      <th>review_url_rs</th>\n",
       "      <th>journal_rs</th>\n",
       "      <th>volume_journal_rs</th>\n",
       "      <th>issue_journal_rs</th>\n",
       "      <th>published_year_journal_rs</th>\n",
       "      <th>domain</th>\n",
       "      <th>entry_type</th>\n",
       "      <th>rs_score</th>\n",
       "      <th>rs_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RS_001_MLRC_2022_01</td>\n",
       "      <td>[Re] $\\mathcal{G}$-Mixup: Graph Data Augmentat...</td>\n",
       "      <td>['Ermin Omeragic', 'Vuk ÄuranoviÄ‡']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'machine learning', 'data augm...</td>\n",
       "      <td>Scope of Reproducibility\\nThis paper presents ...</td>\n",
       "      <td>/pdf/8c87cb1e84e1482826c40a3b0c43928eaef747f3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.mlr.press/v162/han22c.html</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173650</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=XxUIomN-ndH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RS_002_MLRC_2022_02</td>\n",
       "      <td>[Re] Exact Feature Distribution Matching for A...</td>\n",
       "      <td>['Mert Erkol', 'Furkan KÄ±nlÄ±', 'BarÄ±ÅŸ Ã–zc...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['feature distribution matching', 'style trans...</td>\n",
       "      <td>Reproducibility Summary:\\n\\nIn this reproducib...</td>\n",
       "      <td>/pdf/7bcb577c2a46db29c48234a5b72368053c7ebed3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openaccess.thecvf.com/content/CVPR2022...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173652</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=a5_hbZf0NB&amp;not...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RS_003_MLRC_2022_03</td>\n",
       "      <td>[Re] End-to-end Algorithm Synthesis with Recur...</td>\n",
       "      <td>['Sean Michael McLeish', 'Long Tran-Thanh']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'Algorithmic Reasoning', 'Deep...</td>\n",
       "      <td>Scope of Reproducibility:\\nIn this report, we ...</td>\n",
       "      <td>/pdf/07d5d68b5873d779bd1fd8c95b9767cb57fe0bc4.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/pdf/2202.05826.pdf</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173654</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=WaZB4pUVTi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RS_004_MLRC_2022_04</td>\n",
       "      <td>[Re] Label-Free Explainability for Unsupervise...</td>\n",
       "      <td>['Eric Langezaal', 'Jesse Belleman', 'Joeri No...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Reproducibility', 'label-free', 'unsupervise...</td>\n",
       "      <td>Scope of Reproducibility â€” This study is an ...</td>\n",
       "      <td>/pdf/7fde4f12c675150699fc376cd097d5b9dad0b4d4.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.mlr.press/v162/crabbe22a.html</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173656</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=bBVZ3pY4z8p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RS_005_MLRC_2022_05</td>\n",
       "      <td>[Re] Exploring the Representation of Word Mean...</td>\n",
       "      <td>['Matteo Brivio', 'Cagri Coltekin']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['BERT', 'fastText', 'embeddings', 'language m...</td>\n",
       "      <td>This report summarizes our efforts to reproduc...</td>\n",
       "      <td>/pdf/78430c5af33bc892d852b49a5a6e93abeb314c6f.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://aclanthology.org/2021.acl-long.281/</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173658</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=Od5dD58libt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        key_for_all_RS                                           title_rs  \\\n",
       "0  RS_001_MLRC_2022_01  [Re] $\\mathcal{G}$-Mixup: Graph Data Augmentat...   \n",
       "1  RS_002_MLRC_2022_02  [Re] Exact Feature Distribution Matching for A...   \n",
       "2  RS_003_MLRC_2022_03  [Re] End-to-end Algorithm Synthesis with Recur...   \n",
       "3  RS_004_MLRC_2022_04  [Re] Label-Free Explainability for Unsupervise...   \n",
       "4  RS_005_MLRC_2022_05  [Re] Exploring the Representation of Word Mean...   \n",
       "\n",
       "                                          authors_rs article_type  \\\n",
       "0              ['Ermin Omeragic', 'Vuk ÄuranoviÄ‡']  Replication   \n",
       "1  ['Mert Erkol', 'Furkan KÄ±nlÄ±', 'BarÄ±ÅŸ Ã–zc...  Replication   \n",
       "2        ['Sean Michael McLeish', 'Long Tran-Thanh']  Replication   \n",
       "3  ['Eric Langezaal', 'Jesse Belleman', 'Joeri No...  Replication   \n",
       "4                ['Matteo Brivio', 'Cagri Coltekin']  Replication   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  ['rescience c', 'machine learning', 'data augm...   \n",
       "1  ['feature distribution matching', 'style trans...   \n",
       "2  ['rescience c', 'Algorithmic Reasoning', 'Deep...   \n",
       "3  ['Reproducibility', 'label-free', 'unsupervise...   \n",
       "4  ['BERT', 'fastText', 'embeddings', 'language m...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Scope of Reproducibility\\nThis paper presents ...   \n",
       "1  Reproducibility Summary:\\n\\nIn this reproducib...   \n",
       "2  Scope of Reproducibility:\\nIn this report, we ...   \n",
       "3  Scope of Reproducibility â€” This study is an ...   \n",
       "4  This report summarizes our efforts to reproduc...   \n",
       "\n",
       "                                          pdf_url_rs code_url_rs  \\\n",
       "0  /pdf/8c87cb1e84e1482826c40a3b0c43928eaef747f3.pdf         NaN   \n",
       "1  /pdf/7bcb577c2a46db29c48234a5b72368053c7ebed3.pdf         NaN   \n",
       "2  /pdf/07d5d68b5873d779bd1fd8c95b9767cb57fe0bc4.pdf         NaN   \n",
       "3  /pdf/7fde4f12c675150699fc376cd097d5b9dad0b4d4.pdf         NaN   \n",
       "4  /pdf/78430c5af33bc892d852b49a5a6e93abeb314c6f.pdf         NaN   \n",
       "\n",
       "                                       org_paper_url  \\\n",
       "0     https://proceedings.mlr.press/v162/han22c.html   \n",
       "1  https://openaccess.thecvf.com/content/CVPR2022...   \n",
       "2               https://arxiv.org/pdf/2202.05826.pdf   \n",
       "3  https://proceedings.mlr.press/v162/crabbe22a.html   \n",
       "4        https://aclanthology.org/2021.acl-long.281/   \n",
       "\n",
       "                                       doi_rs  ... challenge_year_rs  \\\n",
       "0  https://www.doi.org/10.5281/zenodo.8173650  ...              2022   \n",
       "1  https://www.doi.org/10.5281/zenodo.8173652  ...              2022   \n",
       "2  https://www.doi.org/10.5281/zenodo.8173654  ...              2022   \n",
       "3  https://www.doi.org/10.5281/zenodo.8173656  ...              2022   \n",
       "4  https://www.doi.org/10.5281/zenodo.8173658  ...              2022   \n",
       "\n",
       "                                       review_url_rs journal_rs  \\\n",
       "0        https://openreview.net/forum?id=XxUIomN-ndH        NaN   \n",
       "1  https://openreview.net/forum?id=a5_hbZf0NB&not...        NaN   \n",
       "2         https://openreview.net/forum?id=WaZB4pUVTi        NaN   \n",
       "3        https://openreview.net/forum?id=bBVZ3pY4z8p        NaN   \n",
       "4        https://openreview.net/forum?id=Od5dD58libt        NaN   \n",
       "\n",
       "   volume_journal_rs issue_journal_rs published_year_journal_rs  \\\n",
       "0                NaN              NaN                       NaN   \n",
       "1                NaN              NaN                       NaN   \n",
       "2                NaN              NaN                       NaN   \n",
       "3                NaN              NaN                       NaN   \n",
       "4                NaN              NaN                       NaN   \n",
       "\n",
       "                              domain  entry_type  rs_score rs_comment  \n",
       "0  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "1  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "2  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "3  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "4  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xlsx = pd.read_excel('RS_ALL_IN_ONE_metadata.xlsx')\n",
    "\n",
    "df_xlsx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70aed206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RS_001_MLRC_2022_01\n",
      "A value is already there\n",
      "RS_002_MLRC_2022_02\n",
      "A value is already there\n",
      "RS_003_MLRC_2022_03\n",
      "A value is already there\n",
      "RS_004_MLRC_2022_04\n",
      "A value is already there\n",
      "RS_005_MLRC_2022_05\n",
      "A value is already there\n",
      "RS_006_MLRC_2022_06\n",
      "A value is already there\n",
      "RS_007_MLRC_2022_07\n",
      "A value is already there\n",
      "RS_008_MLRC_2022_08\n",
      "A value is already there\n",
      "RS_009_MLRC_2022_09\n",
      "A value is already there\n",
      "RS_010_MLRC_2022_10\n",
      "A value is already there\n",
      "RS_011_MLRC_2022_11\n",
      "A value is already there\n",
      "RS_012_MLRC_2022_12\n",
      "A value is already there\n",
      "RS_013_MLRC_2022_13\n",
      "A value is already there\n",
      "RS_014_MLRC_2022_14\n",
      "A value is already there\n",
      "RS_015_MLRC_2022_15\n",
      "A value is already there\n",
      "RS_016_MLRC_2022_16\n",
      "A value is already there\n",
      "RS_017_MLRC_2022_17\n",
      "A value is already there\n",
      "RS_018_MLRC_2022_18\n",
      "A value is already there\n",
      "RS_019_MLRC_2022_19\n",
      "A value is already there\n",
      "RS_020_MLRC_2022_20\n",
      "A value is already there\n",
      "RS_021_MLRC_2022_21\n",
      "A value is already there\n",
      "RS_022_MLRC_2022_22\n",
      "A value is already there\n",
      "RS_023_MLRC_2022_23\n",
      "A value is already there\n",
      "RS_024_MLRC_2022_24\n",
      "A value is already there\n",
      "RS_025_MLRC_2022_25\n",
      "A value is already there\n",
      "RS_026_MLRC_2022_26\n",
      "A value is already there\n",
      "RS_027_MLRC_2022_27\n",
      "A value is already there\n",
      "RS_028_MLRC_2022_28\n",
      "A value is already there\n",
      "RS_029_MLRC_2022_29\n",
      "A value is already there\n",
      "RS_030_MLRC_2022_30\n",
      "A value is already there\n",
      "RS_031_MLRC_2022_31\n",
      "A value is already there\n",
      "RS_032_MLRC_2022_32\n",
      "A value is already there\n",
      "RS_033_MLRC_2022_33\n",
      "A value is already there\n",
      "RS_034_MLRC_2022_34\n",
      "A value is already there\n",
      "RS_035_MLRC_2022_35\n",
      "A value is already there\n",
      "RS_036_MLRC_2022_36\n",
      "A value is already there\n",
      "RS_037_MLRC_2022_37\n",
      "A value is already there\n",
      "RS_038_MLRC_2022_38\n",
      "A value is already there\n",
      "RS_039_MLRC_2022_39\n",
      "A value is already there\n",
      "RS_040_MLRC_2022_40\n",
      "A value is already there\n",
      "RS_041_MLRC_2022_41\n",
      "A value is already there\n",
      "RS_042_MLRC_2022_42\n",
      "A value is already there\n",
      "RS_043_MLRC_2022_43\n",
      "A value is already there\n",
      "RS_044_MLRC_2022_44\n",
      "A value is already there\n",
      "RS_045_MLRC_2022_45\n",
      "A value is already there\n",
      "RS_046_MLRC_2021_01\n",
      "RS_046_MLRC_2021_01 ----- nan ISSUE\n",
      "RS_047_MLRC_2021_02\n",
      "RS_048_MLRC_2021_03\n",
      "RS_049_MLRC_2021_04\n",
      "RS_050_MLRC_2021_05\n",
      "RS_051_MLRC_2021_06\n",
      "RS_052_MLRC_2021_07\n",
      "RS_053_MLRC_2021_08\n",
      "RS_054_MLRC_2021_09\n",
      "RS_055_MLRC_2021_10\n",
      "RS_056_MLRC_2021_11\n",
      "RS_057_MLRC_2021_12\n",
      "RS_058_MLRC_2021_13\n",
      "RS_059_MLRC_2021_14\n",
      "RS_060_MLRC_2021_15\n",
      "RS_061_MLRC_2021_16\n",
      "RS_062_MLRC_2021_17\n",
      "RS_063_MLRC_2021_18\n",
      "RS_064_MLRC_2021_19\n",
      "RS_065_MLRC_2021_20\n",
      "RS_066_MLRC_2021_21\n",
      "RS_067_MLRC_2021_22\n",
      "RS_068_MLRC_2021_23\n",
      "RS_069_MLRC_2021_24\n",
      "RS_070_MLRC_2021_25\n",
      "RS_071_MLRC_2021_26\n",
      "RS_072_MLRC_2021_27\n",
      "RS_073_MLRC_2021_28\n",
      "RS_074_MLRC_2021_29\n",
      "RS_075_MLRC_2021_30\n",
      "RS_076_MLRC_2021_31\n",
      "RS_077_MLRC_2021_32\n",
      "RS_078_MLRC_2021_33\n",
      "RS_079_MLRC_2021_34\n",
      "RS_080_MLRC_2021_35\n",
      "RS_081_MLRC_2021_36\n",
      "RS_082_MLRC_2021_37\n",
      "RS_083_MLRC_2021_38\n",
      "RS_084_MLRC_2021_39\n",
      "RS_085_MLRC_2021_40\n",
      "RS_086_MLRC_2021_41\n",
      "RS_087_MLRC_2021_42\n",
      "RS_088_MLRC_2021_43\n",
      "RS_089_MLRC_2021_44\n",
      "RS_090_MLRC_2021_45\n",
      "RS_091_MLRC_2021_46\n",
      "RS_092_MLRC_2021_47\n",
      "RS_093_MLRC_2021_48\n",
      "RS_094_MLRC_2020_01\n",
      "RS_095_MLRC_2020_02\n",
      "RS_096_MLRC_2020_03\n",
      "RS_097_MLRC_2020_04\n",
      "RS_098_MLRC_2020_05\n",
      "RS_099_MLRC_2020_06\n",
      "RS_100_MLRC_2020_07\n",
      "RS_101_MLRC_2020_08\n",
      "RS_102_MLRC_2020_09\n",
      "RS_103_MLRC_2020_10\n",
      "RS_104_MLRC_2020_11\n",
      "RS_105_MLRC_2020_12\n",
      "RS_106_MLRC_2020_13\n",
      "RS_107_MLRC_2020_14\n",
      "RS_108_MLRC_2020_15\n",
      "RS_109_MLRC_2020_16\n",
      "RS_110_MLRC_2020_17\n",
      "RS_111_MLRC_2020_18\n",
      "RS_112_MLRC_2020_19\n",
      "RS_112_MLRC_2020_19 ----- nan ISSUE\n",
      "RS_113_MLRC_2020_20\n",
      "RS_114_MLRC_2020_21\n",
      "RS_115_MLRC_2020_22\n",
      "RS_116_MLRC_2020_23\n",
      "RS_117_MLRC_2020_24\n",
      "RS_117_MLRC_2020_24 ----- nan ISSUE\n",
      "RS_118_NeurIPS_2019_01\n",
      "RS_118_NeurIPS_2019_01 ----- nan ISSUE\n",
      "RS_119_NeurIPS_2019_02\n",
      "https://openreview.net/forum?id=y53aaSM5o\n",
      "RS_120_NeurIPS_2019_03\n",
      "https://openreview.net/forum?id=B1lcYrBgLH\n",
      "RS_121_NeurIPS_2019_04\n",
      "https://openreview.net/forum?id=HyxQr65z6S\n",
      "RS_122_NeurIPS_2019_05\n",
      "https://openreview.net/forum?id=BJxUSaczTH\n",
      "RS_123_NeurIPS_2019_06\n",
      "https://openreview.net/forum?id=HJxNSp9MTr\n",
      "RS_124_NeurIPS_2019_07\n",
      "RS_124_NeurIPS_2019_07 ----- nan ISSUE\n",
      "RS_125_NeurIPS_2019_08\n",
      "https://openreview.net/forum?id=rkezvT9f6r\n",
      "RS_126_NeurIPS_2019_09\n",
      "https://openreview.net/forum?id=GAY8Xw9Qzs\n",
      "RS_127_NeurIPS_2019_10\n",
      "https://openreview.net/forum?id=SklFHaqG6S\n",
      "RS_128_NeurIPS_2019_11\n",
      "https://openreview.net/forum?id=JhZOkalsiI\n",
      "RS_129_ICLR_2019_01\n",
      "https://github.com/ReScience/submissions/issues/5\n",
      "RS_130_ICLR_2019_02\n",
      "https://github.com/reproducibility-challenge/iclr_2019/pull/150\n",
      "RS_131_ICLR_2019_03\n",
      "https://github.com/reproducibility-challenge/iclr_2019/pull/146\n",
      "RS_132_ICLR_2019_04\n",
      "https://github.com/reproducibility-challenge/iclr_2019/pull/136\n",
      "RS_133_ICLR_2019_05\n",
      "https://github.com/reproducibility-challenge/iclr_2019/pull/148\n",
      "RS_134_ICDAR_2018_01\n",
      "RS_134_ICDAR_2018_01 ----- nan ISSUE\n",
      "RS_135_ICDAR_2018_02\n",
      "RS_135_ICDAR_2018_02 ----- nan ISSUE\n",
      "RS_136_ICDAR_2018_03\n",
      "RS_136_ICDAR_2018_03 ----- nan ISSUE\n",
      "RS_137_ICDAR_2018_04\n",
      "RS_137_ICDAR_2018_04 ----- nan ISSUE\n",
      "RS_138_ICDAR_2018_05\n",
      "RS_138_ICDAR_2018_05 ----- nan ISSUE\n",
      "RS_139_ICDAR_2018_06\n",
      "RS_139_ICDAR_2018_06 ----- nan ISSUE\n",
      "RS_140_ICDAR_2018_07\n",
      "RS_140_ICDAR_2018_07 ----- nan ISSUE\n",
      "RS_141_ICDAR_2018_08\n",
      "RS_141_ICDAR_2018_08 ----- nan ISSUE\n",
      "RS_142_ICDAR_2018_09\n",
      "RS_142_ICDAR_2018_09 ----- nan ISSUE\n",
      "RS_143_ICDAR_2018_10\n",
      "RS_143_ICDAR_2018_10 ----- nan ISSUE\n",
      "RS_144_ICDAR_2018_11\n",
      "RS_144_ICDAR_2018_11 ----- nan ISSUE\n",
      "RS_145_ICDAR_2018_12\n",
      "RS_145_ICDAR_2018_12 ----- nan ISSUE\n",
      "RS_146_ICDAR_2018_13\n",
      "RS_146_ICDAR_2018_13 ----- nan ISSUE\n",
      "RS_147_ICDAR_2018_14\n",
      "RS_147_ICDAR_2018_14 ----- nan ISSUE\n",
      "RS_148_ICDAR_2018_15\n",
      "RS_148_ICDAR_2018_15 ----- nan ISSUE\n",
      "RS_149_ICDAR_2018_16\n",
      "RS_149_ICDAR_2018_16 ----- nan ISSUE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_for_all_RS</th>\n",
       "      <th>title_rs</th>\n",
       "      <th>authors_rs</th>\n",
       "      <th>article_type</th>\n",
       "      <th>keywords</th>\n",
       "      <th>abstract</th>\n",
       "      <th>pdf_url_rs</th>\n",
       "      <th>code_url_rs</th>\n",
       "      <th>org_paper_url</th>\n",
       "      <th>doi_rs</th>\n",
       "      <th>...</th>\n",
       "      <th>challenge_year_rs</th>\n",
       "      <th>review_url_rs</th>\n",
       "      <th>journal_rs</th>\n",
       "      <th>volume_journal_rs</th>\n",
       "      <th>issue_journal_rs</th>\n",
       "      <th>published_year_journal_rs</th>\n",
       "      <th>domain</th>\n",
       "      <th>entry_type</th>\n",
       "      <th>rs_score</th>\n",
       "      <th>rs_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RS_001_MLRC_2022_01</td>\n",
       "      <td>[Re] $\\mathcal{G}$-Mixup: Graph Data Augmentat...</td>\n",
       "      <td>['Ermin Omeragic', 'Vuk ÄuranoviÄ‡']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'machine learning', 'data augm...</td>\n",
       "      <td>Scope of Reproducibility\\nThis paper presents ...</td>\n",
       "      <td>/pdf/8c87cb1e84e1482826c40a3b0c43928eaef747f3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.mlr.press/v162/han22c.html</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173650</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=XxUIomN-ndH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RS_002_MLRC_2022_02</td>\n",
       "      <td>[Re] Exact Feature Distribution Matching for A...</td>\n",
       "      <td>['Mert Erkol', 'Furkan KÄ±nlÄ±', 'BarÄ±ÅŸ Ã–zc...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['feature distribution matching', 'style trans...</td>\n",
       "      <td>Reproducibility Summary:\\n\\nIn this reproducib...</td>\n",
       "      <td>/pdf/7bcb577c2a46db29c48234a5b72368053c7ebed3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openaccess.thecvf.com/content/CVPR2022...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173652</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=a5_hbZf0NB&amp;not...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RS_003_MLRC_2022_03</td>\n",
       "      <td>[Re] End-to-end Algorithm Synthesis with Recur...</td>\n",
       "      <td>['Sean Michael McLeish', 'Long Tran-Thanh']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'Algorithmic Reasoning', 'Deep...</td>\n",
       "      <td>Scope of Reproducibility:\\nIn this report, we ...</td>\n",
       "      <td>/pdf/07d5d68b5873d779bd1fd8c95b9767cb57fe0bc4.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/pdf/2202.05826.pdf</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173654</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=WaZB4pUVTi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RS_004_MLRC_2022_04</td>\n",
       "      <td>[Re] Label-Free Explainability for Unsupervise...</td>\n",
       "      <td>['Eric Langezaal', 'Jesse Belleman', 'Joeri No...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Reproducibility', 'label-free', 'unsupervise...</td>\n",
       "      <td>Scope of Reproducibility â€” This study is an ...</td>\n",
       "      <td>/pdf/7fde4f12c675150699fc376cd097d5b9dad0b4d4.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.mlr.press/v162/crabbe22a.html</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173656</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=bBVZ3pY4z8p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RS_005_MLRC_2022_05</td>\n",
       "      <td>[Re] Exploring the Representation of Word Mean...</td>\n",
       "      <td>['Matteo Brivio', 'Cagri Coltekin']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['BERT', 'fastText', 'embeddings', 'language m...</td>\n",
       "      <td>This report summarizes our efforts to reproduc...</td>\n",
       "      <td>/pdf/78430c5af33bc892d852b49a5a6e93abeb314c6f.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://aclanthology.org/2021.acl-long.281/</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173658</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=Od5dD58libt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RS_006_MLRC_2022_06</td>\n",
       "      <td>[Re] Intriguing Properties of Contrastive Losses</td>\n",
       "      <td>['Luca Marini', 'Mohamad Nabeel', 'Alexandre L...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['self-supervised learning', 'contrastive lear...</td>\n",
       "      <td>Reproducibility Summary\\n\\nScope of Reproducib...</td>\n",
       "      <td>/pdf/fc591d20ed7c340bbe470789beb95d4cde81e7d5.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.neurips.cc/paper/2021/hash...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173662</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=gb71irTNN7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RS_007_MLRC_2022_07</td>\n",
       "      <td>[Re] Bandit Theory and Thompson Sampling-guide...</td>\n",
       "      <td>['Luka Å½ontar']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Optimization', 'bandit learning', 'Thompson ...</td>\n",
       "      <td>The paper presents a novel DE approach using T...</td>\n",
       "      <td>/pdf/cf9062c0ba6651cb416547df2c1e7e7379d572a9.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://neurips.cc/Conferences/2022/ScheduleMu...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173664</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=NE_x1dpz-Q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RS_008_MLRC_2022_08</td>\n",
       "      <td>[Re] Hypergraph-Induced Semantic Tuplet Loss f...</td>\n",
       "      <td>['Jicheng Yuan', 'Danh Le Phuoc']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Hypergraph', 'Deep Metric Learning', 'Semant...</td>\n",
       "      <td>Reproducibility Summary\\n\\nScope of Reproducib...</td>\n",
       "      <td>/pdf/9a228c78b9d481df388578a8e5d78a90412b9891.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openaccess.thecvf.com/content/CVPR2022...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173666</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=JJQbk2hIQ5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RS_009_MLRC_2022_09</td>\n",
       "      <td>Easy Bayesian Transfer Learning with Informati...</td>\n",
       "      <td>['Martin Å pendl', 'Klementina Pirc']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['transfer learning', 'Bayesian inference', 'i...</td>\n",
       "      <td>REPRODUCIBILITY SUMMARY\\n\\nScope of Reproducib...</td>\n",
       "      <td>/pdf/4247c7fbacadbf0bc1d7e5fc08e2c4ce35315657.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openreview.net/forum?id=ao30zaT3YL</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173668</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=JpaQ8GFOVu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RS_010_MLRC_2022_10</td>\n",
       "      <td>[Re] On the Reproducibility of CartoonX</td>\n",
       "      <td>['Robin Sasse', 'Aniek Eijpe', 'Jona Ruthardt'...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Machine Learning', 'Artificial Intelligence'...</td>\n",
       "      <td>Scope of Reproducibility â€” CartoonX [1] is a...</td>\n",
       "      <td>/pdf/6f7b408230bc01c524943393638a23d4a8f73cc6.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://doi.org/10.1007/978-3-031-19775-8_26</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173672</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=MK4IQJdLLeo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RS_011_MLRC_2022_11</td>\n",
       "      <td>Reproducibility Study of \"Label-Free Explainab...</td>\n",
       "      <td>['Valentinos Pariza', 'Avik Pal', 'Madhura Paw...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Reproducibility', 'Feature Importance', 'Exa...</td>\n",
       "      <td>Scope of Reproducibility\\nIn this work, we eva...</td>\n",
       "      <td>/pdf/b037311c182b52dd01853262e9d4fb09103cddae.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.mlr.press/v162/crabbe22a/c...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173674</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=qP34dvJpHd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RS_012_MLRC_2022_12</td>\n",
       "      <td>[Re] FOCUS: Flexible Optimizable Counterfactua...</td>\n",
       "      <td>['Kyosuke Morita']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Counterfactual explanation']</td>\n",
       "      <td>Scope of Reproducibility\\n\\nThis study aims to...</td>\n",
       "      <td>/pdf/58f0c36da4cbe7eba434f580fb26822b34c6757e.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1911.12199</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173678</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=n1q-iz83S5&amp;not...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RS_013_MLRC_2022_13</td>\n",
       "      <td>[Re] Fairness Guarantees under Demographic Shift</td>\n",
       "      <td>['Valentin Leonhard Buchner', 'Philip Onno Oli...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Fairness and Bias in ML', 'Fair Machine Lear...</td>\n",
       "      <td>\\nScope of Reproducibility: The original autho...</td>\n",
       "      <td>/pdf/a154eb4c4168594c9fec045c44baecf70fac36c0.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://iclr.cc/virtual/2022/poster/6666</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173680</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=xEfg6h1GFmW&amp;no...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RS_014_MLRC_2022_14</td>\n",
       "      <td>[Re] DialSummEval - Evaluation of automatic su...</td>\n",
       "      <td>['Patrick Camara', 'Mojca Catharina Kloos', 'V...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Reproduction', 'Evaluation', 'Dialogue Summa...</td>\n",
       "      <td>Scope of Reproducibility â€” In this paper, we...</td>\n",
       "      <td>/pdf/a49585e9e327fcb0e6dcf55d03569a33782b5777.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://aclanthology.org/2022.naacl-main.418/</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173682</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=3jaZ5tKRyiT&amp;no...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RS_015_MLRC_2022_15</td>\n",
       "      <td>[Re] On the Reproducibility of â€œFairCal: Fai...</td>\n",
       "      <td>['Marga Don', 'Satchit Chatterji', 'Milena Kap...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['ReScience C', 'ReScience X', 'Machine Learni...</td>\n",
       "      <td>Scope of Reproducibility â€” This paper aims t...</td>\n",
       "      <td>/pdf/e37fde64b0642ea7024c23ed0835ce07292722ff.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openreview.net/pdf?id=nRj0NcmSuxb</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173686</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=uVHUy7CWCL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RS_016_MLRC_2022_16</td>\n",
       "      <td>Reproducibility Study: Label-Free Explainabili...</td>\n",
       "      <td>['SÅ‚awomir Garcarz', 'Andreas Giorkatzi', 'An...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Label-Free Explainability', 'Explainability'...</td>\n",
       "      <td>Reproducibility Summary\\n\\nScope of Reproducib...</td>\n",
       "      <td>/pdf/b9d086713ae4efccb1558881e34a3d88acfdb549.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.mlr.press/v162/crabbe22a/c...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173688</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=sF_vYZSxSV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RS_017_MLRC_2022_17</td>\n",
       "      <td>[Re] Numerical influence of ReLU'(0) on backpr...</td>\n",
       "      <td>['Tommaso Martorella', 'Hector Manuel Ramirez ...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'machine learning', 'deep lear...</td>\n",
       "      <td>Neural networks have become very common in mac...</td>\n",
       "      <td>/pdf/e8c787801a1d8bad0fb00b5952625b3115ba6fb3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.neurips.cc/paper/2021/file...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173692</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=YAWQTQZVoA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RS_018_MLRC_2022_18</td>\n",
       "      <td>A Reproducibility Case Study of â€œFairness Gu...</td>\n",
       "      <td>['Zjos van de Sande', 'Dennis Agafonov', 'Jelk...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['fairness', 'demographic shift', 'classificat...</td>\n",
       "      <td>Reproducibility Summary\\n\\nScope of Reproducib...</td>\n",
       "      <td>/pdf/6e8499b6188da557805216d83be1cd029ba00bd5.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openreview.net/forum?id=wbPObLm6ueA</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8206607</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=MMuv-v99Hy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RS_019_MLRC_2022_19</td>\n",
       "      <td>[Re] Hierarchical Shrinkage: Improving the Acc...</td>\n",
       "      <td>['Domen MohorÄiÄ', 'David Ocepek']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'machine learning', 'decision ...</td>\n",
       "      <td>Scope of Reproducibility\\nThe paper presents a...</td>\n",
       "      <td>/pdf/8e7ec4f4b707646fa2406a3a1b46e6361ec7caaf.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://icml.cc/virtual/2022/poster/16829</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173696</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=NgPQSqpz-Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RS_020_MLRC_2022_20</td>\n",
       "      <td>Reproducibility study of \"Joint Multisided Exp...</td>\n",
       "      <td>['Alessia Hu', 'Oline Ranum', 'Chrysoula Pozri...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Reproducibility', 'Information Retrieval', '...</td>\n",
       "      <td>Scope of Reproducibility \\nIn this work, we st...</td>\n",
       "      <td>/pdf/8634a0b0fe52eb23f0fa54633413fd4f9176c966.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2205.00048</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173698</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=A0Sjs3IJWb-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>RS_021_MLRC_2022_21</td>\n",
       "      <td>Exploring the Explainability of Bias in Image ...</td>\n",
       "      <td>['DaniÃ«l Van Dijk', 'Marten TÃ¼rk', 'L Busser...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['reproducibility', 'fairness', 'gender', 'rac...</td>\n",
       "      <td>Scope of Reproducibility â€” The main objectiv...</td>\n",
       "      <td>/pdf/57b829f2270fa6cc431997add0162fba06a9bc91.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2203.15395</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173703</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=N9Wn91tE7D0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RS_022_MLRC_2022_22</td>\n",
       "      <td>Reproducibility study of 'Proto2Proto: Can you...</td>\n",
       "      <td>['Gerson de Kleuver', 'David Matthias Bikker',...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Reproducibility', 'Interpretability', 'Proto...</td>\n",
       "      <td>Scope of Reproducibility â€” \\n\\nThis paper an...</td>\n",
       "      <td>/pdf/0cad70c6cd1e3d75ccf19794d4af2967be60ede5.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2204.11830</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173705</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=a_9YF58u61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RS_023_MLRC_2022_23</td>\n",
       "      <td>[Re] Reproducibility study of â€Focus On The ...</td>\n",
       "      <td>['Walter Simoncini', 'Ioanna Gogou', 'Marta Fr...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'machine learning', 'deep lear...</td>\n",
       "      <td>Scope of Reproducibility\\n\\nThis paper attempt...</td>\n",
       "      <td>/pdf/be89dd7facfe263fd65d756feb4b3e75b715b2f2.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openreview.net/forum?id=irARV_2VFs4</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173707</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=ye8PftiQLQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RS_024_MLRC_2022_24</td>\n",
       "      <td>Reproducibility Study of â€Label-Free Explain...</td>\n",
       "      <td>['Julius Wagenbach', 'Gergely Papp', 'Niklas M...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Explainable AI', 'Unsupervised Learning', 'F...</td>\n",
       "      <td>In this work, we present our reproducibility s...</td>\n",
       "      <td>/pdf/34fdaf29b0edaa22d1258444847ba34b4b9c9fe9.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2203.01928</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173711</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=n2qXFXiMsAM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RS_025_MLRC_2022_25</td>\n",
       "      <td>[Re] Reproducibility study of â€œExplaining De...</td>\n",
       "      <td>['Erik Buis', 'Sebastiaan Dijkstra', 'Bram Hei...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Reproduce', 'Interpretability', 'Convolution...</td>\n",
       "      <td>Scope of Reproducibility\\nIn this work, we aim...</td>\n",
       "      <td>/pdf/d2f5afd089bc7f1472bb80dc9e9933df836b4433.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ieeexplore.ieee.org/document/9878481</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173713</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=nsrHznwHhl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RS_026_MLRC_2022_26</td>\n",
       "      <td>Reproducibility Study of â€œQuantifying Societ...</td>\n",
       "      <td>['Farrukh Baratov', 'GÃ¶ksenin YÃ¼ksel', 'Dari...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['bias', 'gender bias', 'racial bias', 'age bi...</td>\n",
       "      <td>Scope of reproducibility - We study the reprod...</td>\n",
       "      <td>/pdf/764521708b35c1dab2f1507074f5b94b3ff70e06.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openaccess.thecvf.com/content/CVPR2022...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173715</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=eJmQJT0Dtt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RS_027_MLRC_2022_27</td>\n",
       "      <td>[Re] On the reproducibility of \"CrossWalk: Fai...</td>\n",
       "      <td>['Eric Zila', 'Jonathan Gerbscheid', 'Luc StrÃ...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'deepwalk', 'crosswalk', 'grap...</td>\n",
       "      <td>Scope of Reproducibility - The original author...</td>\n",
       "      <td>/pdf/8216b02ff557ce4ec1c022cb7a0707519c499e8d.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ojs.aaai.org/index.php/AAAI/article/vi...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173717</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=WnaVgRhlyT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RS_028_MLRC_2022_28</td>\n",
       "      <td>[Re] Reproducing FairCal: Fairness Calibration...</td>\n",
       "      <td>['Jip Greven', 'Simon Stallinga', 'Zirk Seljee']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['fairness', 'bias', 'face verification', 'fai...</td>\n",
       "      <td>Reproducibility Summary\\n===\\n\\nScope of Repro...</td>\n",
       "      <td>/pdf/cd9d92da523749654ff721317617b5150b8f929f.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2106.03761v4</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173719</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=jDBYRwDpeW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>RS_029_MLRC_2022_29</td>\n",
       "      <td>Reproducibility Study of â€CartoonX: Cartoon ...</td>\n",
       "      <td>['Aditya Prakash Patra', 'Sina Taslimi', 'Luke...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'machine learning', 'Explainab...</td>\n",
       "      <td>In this reproducibility study, we verify the c...</td>\n",
       "      <td>/pdf/c95ff07d5f89d48e72218d15f55784039360b4af.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.ecva.net/papers/eccv_2022/papers_E...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173721</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=DWKJpl8s06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RS_030_MLRC_2022_30</td>\n",
       "      <td>Reproducibility Study of â€™Latent Space Smoot...</td>\n",
       "      <td>['Didier Merk', 'Tsatsral Mendsuren', 'Denny S...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Latent space smoothing', 'LASSI', 'reproduci...</td>\n",
       "      <td>- Scope of Reproducibility\\nThe aim of this wo...</td>\n",
       "      <td>/pdf/297b0703140390d112c05d67e700b2dbed1e9278.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2111.13650v3</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173725</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=J-Lgb7Vc0wX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>RS_031_MLRC_2022_31</td>\n",
       "      <td>[Re] Variational Neural Cellular Automata</td>\n",
       "      <td>['Albert Sund Aillet', 'Simon SondÃ©n']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Neural Cellular Automata', 'Cellular Automat...</td>\n",
       "      <td>The main claim of the paper being reproduced i...</td>\n",
       "      <td>/pdf/adc8d78a91a593e12451cda54690da8942220f3f.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2201.12360</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173729</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=d7-ns6SZqp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>RS_032_MLRC_2022_32</td>\n",
       "      <td>[Re] If You Like Shapley Then Youâ€™ll Love th...</td>\n",
       "      <td>['Anes Benmerzoug', 'Miguel de Benito Delgado']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['data valuation', 'least core', 'shapley valu...</td>\n",
       "      <td>We investigate the results of [1] in the field...</td>\n",
       "      <td>/pdf/a998d0fe66a5e322bb8ff2d4d9f9a6773770aa4a.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ojs.aaai.org/index.php/AAAI/article/vi...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173733</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=vWzZQAahuW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RS_033_MLRC_2022_33</td>\n",
       "      <td>A Reproduction of Automatic Multi-Label Prompt...</td>\n",
       "      <td>['Victor Livernoche', 'Vidya Sujaya']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['prompt engineering', 'label engineering', 'm...</td>\n",
       "      <td>We reproduce the results in the paper Automati...</td>\n",
       "      <td>/pdf/b64f0e09258f2c039d39a1ee7db8a6aea63e4799.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://aclanthology.org/2022.naacl-main.401.pdf</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173735</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=t8ZZ2Y356Ix</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>RS_034_MLRC_2022_34</td>\n",
       "      <td>On the Reproducibility of â€œG-Mixup:Graph Dat...</td>\n",
       "      <td>['Dylan Cordaro', 'Shelby Cox', 'Yiman Ren', '...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Python', 'graphical neural networks', 'mixup...</td>\n",
       "      <td>We attempt to reproduce results on a novel gra...</td>\n",
       "      <td>/pdf/290dee2b4604ec276e9382fdb7e9d0ab378bdb23.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.mlr.press/v162/han22c.html</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173737</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=T54wy0ahGLG&amp;no...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>RS_035_MLRC_2022_35</td>\n",
       "      <td>[Re] Exploring the Role of Grammar and Word Ch...</td>\n",
       "      <td>['Priyanka Bose', 'Chandra Shekhar Pandey', 'F...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Machine Learning', 'Bias', 'BERT', 'Word2Vec...</td>\n",
       "      <td>Reproducibility Summary\\nScope of Reproducibil...</td>\n",
       "      <td>/pdf/b2c19856585f093a3b6f8da25e1cf6c5b87bb812.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3531146.353...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173739</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=MjZVx7a0KX-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>RS_036_MLRC_2022_36</td>\n",
       "      <td>RELIC: Reproducibility and Extension on LIC me...</td>\n",
       "      <td>['Martijn van Raaphorst', 'Egoitz Gonzales', '...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'rescience x', 'machine learni...</td>\n",
       "      <td>Scope of Reproducibility\\nIn this work we repr...</td>\n",
       "      <td>/pdf/7e16d4e632092a5ed4cf3092c119212d8fc0c138.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2203.15395</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173741</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=9_hCoP3LXwy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>RS_037_MLRC_2022_37</td>\n",
       "      <td>[Re] VAE Approximation Error: ELBO and Exponen...</td>\n",
       "      <td>['Volodymyr Kyrylov', 'Navdeep Singh Bedi', 'Q...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Python', 'VAE', 'approximation errors', 'ELB...</td>\n",
       "      <td>Scope of Reproducibility â€” Exponential famil...</td>\n",
       "      <td>/pdf/46c6b4f22344f8ae9d8cf06031c31771b5587457.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openreview.net/forum?id=OIs3SxU5Ynl</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173745</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=ozbAwipuZu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RS_038_MLRC_2022_38</td>\n",
       "      <td>Reproducibility study of the Fairness-enhanced...</td>\n",
       "      <td>['Gijs Joppe Moens', 'Job de Witte', 'Tobias P...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Crosswalk', 'Fairness', 'Graph Networks', 'G...</td>\n",
       "      <td>\"CrossWalk: Fairness-Enhanced Node Representat...</td>\n",
       "      <td>/pdf/194c8f8a2ee0dd7329a023d299003f45684a3cf6.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/pdf/2105.02725.pdf</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173747</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=KNp7Zq3KkT0&amp;no...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>RS_039_MLRC_2022_39</td>\n",
       "      <td>[Re] CrossWalk: Fairness-enhanced Node Represe...</td>\n",
       "      <td>['Luca Pantea', 'Andrei Eusebiu Blahovici']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Fairness in Machine Learning', 'Representati...</td>\n",
       "      <td>Scope of Reproducibility\\nThis work aims to re...</td>\n",
       "      <td>/pdf/eb0d993e25d9bf3503d18c37700bfa51744ee1c8.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2105.02725</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173749</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=tpk45Zll8eh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>RS_040_MLRC_2022_40</td>\n",
       "      <td>[Re] Masked Autoencoders Are Small Scale Visio...</td>\n",
       "      <td>['Athanasios Charisoudis', 'Simon Ekman von Hu...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'python', 'pytorch', 'machine ...</td>\n",
       "      <td>Scope of Reproducibility â€” The Masked Autoen...</td>\n",
       "      <td>/pdf/c9616492444c28600ec773aa1c6db9667f51da99.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openaccess.thecvf.com/content/CVPR2022...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173751</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=KXfjZPL5pqr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>RS_041_MLRC_2022_41</td>\n",
       "      <td>[Re] On Explainability of Graph Neural Network...</td>\n",
       "      <td>['Yannik Mahlau', 'Lukas Berg', 'Leo Kayser']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Rescience c', 'Rescience x', 'Explainable AI...</td>\n",
       "      <td>Yuan et al. claim their proposed method Subgra...</td>\n",
       "      <td>/pdf/cbf6c4b95a73a9b9d62e6fc49248af23160caae8.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.mlr.press/v139/yuan21c</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173753</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=zKBJw4Ht8s</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>RS_042_MLRC_2022_42</td>\n",
       "      <td>[Re] â€œTowards Understanding Grokkingâ€</td>\n",
       "      <td>['Alexander Shabalin', 'Ildus Sadrtdinov', 'Ev...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['grokking', 'neural networks', 'MNIST']</td>\n",
       "      <td>Scope of Reproducibility\\nIn this work, we att...</td>\n",
       "      <td>/pdf/9d5c9e36691ac9fe4b65941ca8908c0f8b3d7519.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openreview.net/pdf?id=6at6rB3IZm</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173755</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=Vz9VLcJqKS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>RS_043_MLRC_2022_43</td>\n",
       "      <td>[Re] Reproducibility Study of Behavior Transfo...</td>\n",
       "      <td>['Skander Moalla', 'Manuel Madeira', 'Lorenzo ...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'machine learning', 'reinforce...</td>\n",
       "      <td>Scope of Reproducibility - In this work, we an...</td>\n",
       "      <td>/pdf/184673425e85b10da526e3542760edb71d757856.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.neurips.cc/paper_files/pap...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173757</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=E0qO5dI5aEn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>RS_044_MLRC_2022_44</td>\n",
       "      <td>A Replication Study of Compositional Generaliz...</td>\n",
       "      <td>['Kaiser Sun', 'Adina Williams', 'Dieuwke Hupk...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Reproducibility', 'Compositional Generalizat...</td>\n",
       "      <td>Reproducibility Summary\\nScope of Reproducibil...</td>\n",
       "      <td>/pdf/35fb3e86fa46c3cd82126cb00e4b161353283cac.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://aclanthology.org/2021.acl-long.75/</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173759</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=MF9uv95psps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>RS_045_MLRC_2022_45</td>\n",
       "      <td>[Re] Pure Noise to the Rescue of Insufficient ...</td>\n",
       "      <td>['Seungjae Ryan Lee', 'Seungmin Lee']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['data augmentation', 'computer vision']</td>\n",
       "      <td>Scope of Reproducibility â€” We examine the ma...</td>\n",
       "      <td>/pdf/e83b2fc88fcc0e7d6ce8fc8a73b1877007b716dc.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.mlr.press/v162/zada22a</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173763</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=ErBe4MnsVD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>RS_046_MLRC_2021_01</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>Sinha, Koustuv and Dodge, Jesse and Luccioni, ...</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>rescience c, machine learning, deep learning, ...</td>\n",
       "      <td>DO EXTRACT FROM review_url</td>\n",
       "      <td>https://zenodo.org/record/6574723/files/articl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DO EXTRACT FROM review_url</td>\n",
       "      <td>10.5281/zenodo.6574723</td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ReScience C</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>RS_047_MLRC_2021_02</td>\n",
       "      <td>[Re] Counterfactual Generative Networks</td>\n",
       "      <td>Ankit, Ankit and Ambekar, Sameer and Varadhara...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>rescience c, machine learning, deep learning, ...</td>\n",
       "      <td>DO EXTRACT FROM review_url</td>\n",
       "      <td>https://zenodo.org/record/6574625/files/articl...</td>\n",
       "      <td>https://github.com/ambekarsameer96/FACT_AI/</td>\n",
       "      <td>https://arxiv.org/abs/2101.06046</td>\n",
       "      <td>10.5281/zenodo.6574625</td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>https://openreview.net/forum?id=BSHg22G7n0F</td>\n",
       "      <td>ReScience C</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>RS_048_MLRC_2021_03</td>\n",
       "      <td>[Re] Does Self-Supervision Always Improve Few-...</td>\n",
       "      <td>Ashok, Arjun and Aekula, Haswanth</td>\n",
       "      <td>Replication</td>\n",
       "      <td>rescience c, machine learning, deep learning, ...</td>\n",
       "      <td>DO EXTRACT FROM review_url</td>\n",
       "      <td>https://zenodo.org/record/6574629/files/articl...</td>\n",
       "      <td>https://github.com/ashok-arjun/MLRC-2021-Few-S...</td>\n",
       "      <td>https://openreview.net/forum?id=738x44N7yUE&amp;no...</td>\n",
       "      <td>10.5281/zenodo.6574629</td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>https://openreview.net/forum?id=ScfP3G73CY</td>\n",
       "      <td>ReScience C</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>RS_049_MLRC_2021_04</td>\n",
       "      <td>[Re] Weakly-Supervised Semantic Segmentation v...</td>\n",
       "      <td>Athanasiadis, Ioannis and Moschovis, Georgios ...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>rescience c, machine learning, deep learning, ...</td>\n",
       "      <td>DO EXTRACT FROM review_url</td>\n",
       "      <td>https://zenodo.org/record/6574631/files/articl...</td>\n",
       "      <td>https://github.com/athaioan/ViT_Affinity_Repro...</td>\n",
       "      <td>https://openaccess.thecvf.com/content/CVPR2021...</td>\n",
       "      <td>10.5281/zenodo.6574631</td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>https://openreview.net/forum?id=rcEDhGX3AY</td>\n",
       "      <td>ReScience C</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>RS_050_MLRC_2021_05</td>\n",
       "      <td>[Re] Reproducibility Study of â€œCounterfactua...</td>\n",
       "      <td>Bagad, Piyush and Hilders, Paul and Maas, Jess...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>rescience c, machine learning, python, pytorch...</td>\n",
       "      <td>DO EXTRACT FROM review_url</td>\n",
       "      <td>https://zenodo.org/record/6574635/files/articl...</td>\n",
       "      <td>https://github.com/danilodegoede/fact-team3/</td>\n",
       "      <td>https://openreview.net/pdf?id=BXewfAYMmJw</td>\n",
       "      <td>10.5281/zenodo.6574635</td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>https://openreview.net/forum?id=HNlzT3G720t</td>\n",
       "      <td>ReScience C</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         key_for_all_RS                                           title_rs  \\\n",
       "0   RS_001_MLRC_2022_01  [Re] $\\mathcal{G}$-Mixup: Graph Data Augmentat...   \n",
       "1   RS_002_MLRC_2022_02  [Re] Exact Feature Distribution Matching for A...   \n",
       "2   RS_003_MLRC_2022_03  [Re] End-to-end Algorithm Synthesis with Recur...   \n",
       "3   RS_004_MLRC_2022_04  [Re] Label-Free Explainability for Unsupervise...   \n",
       "4   RS_005_MLRC_2022_05  [Re] Exploring the Representation of Word Mean...   \n",
       "5   RS_006_MLRC_2022_06   [Re] Intriguing Properties of Contrastive Losses   \n",
       "6   RS_007_MLRC_2022_07  [Re] Bandit Theory and Thompson Sampling-guide...   \n",
       "7   RS_008_MLRC_2022_08  [Re] Hypergraph-Induced Semantic Tuplet Loss f...   \n",
       "8   RS_009_MLRC_2022_09  Easy Bayesian Transfer Learning with Informati...   \n",
       "9   RS_010_MLRC_2022_10            [Re] On the Reproducibility of CartoonX   \n",
       "10  RS_011_MLRC_2022_11  Reproducibility Study of \"Label-Free Explainab...   \n",
       "11  RS_012_MLRC_2022_12  [Re] FOCUS: Flexible Optimizable Counterfactua...   \n",
       "12  RS_013_MLRC_2022_13   [Re] Fairness Guarantees under Demographic Shift   \n",
       "13  RS_014_MLRC_2022_14  [Re] DialSummEval - Evaluation of automatic su...   \n",
       "14  RS_015_MLRC_2022_15  [Re] On the Reproducibility of â€œFairCal: Fai...   \n",
       "15  RS_016_MLRC_2022_16  Reproducibility Study: Label-Free Explainabili...   \n",
       "16  RS_017_MLRC_2022_17  [Re] Numerical influence of ReLU'(0) on backpr...   \n",
       "17  RS_018_MLRC_2022_18  A Reproducibility Case Study of â€œFairness Gu...   \n",
       "18  RS_019_MLRC_2022_19  [Re] Hierarchical Shrinkage: Improving the Acc...   \n",
       "19  RS_020_MLRC_2022_20  Reproducibility study of \"Joint Multisided Exp...   \n",
       "20  RS_021_MLRC_2022_21  Exploring the Explainability of Bias in Image ...   \n",
       "21  RS_022_MLRC_2022_22  Reproducibility study of 'Proto2Proto: Can you...   \n",
       "22  RS_023_MLRC_2022_23  [Re] Reproducibility study of â€Focus On The ...   \n",
       "23  RS_024_MLRC_2022_24  Reproducibility Study of â€Label-Free Explain...   \n",
       "24  RS_025_MLRC_2022_25  [Re] Reproducibility study of â€œExplaining De...   \n",
       "25  RS_026_MLRC_2022_26  Reproducibility Study of â€œQuantifying Societ...   \n",
       "26  RS_027_MLRC_2022_27  [Re] On the reproducibility of \"CrossWalk: Fai...   \n",
       "27  RS_028_MLRC_2022_28  [Re] Reproducing FairCal: Fairness Calibration...   \n",
       "28  RS_029_MLRC_2022_29  Reproducibility Study of â€CartoonX: Cartoon ...   \n",
       "29  RS_030_MLRC_2022_30  Reproducibility Study of â€™Latent Space Smoot...   \n",
       "30  RS_031_MLRC_2022_31          [Re] Variational Neural Cellular Automata   \n",
       "31  RS_032_MLRC_2022_32  [Re] If You Like Shapley Then Youâ€™ll Love th...   \n",
       "32  RS_033_MLRC_2022_33  A Reproduction of Automatic Multi-Label Prompt...   \n",
       "33  RS_034_MLRC_2022_34  On the Reproducibility of â€œG-Mixup:Graph Dat...   \n",
       "34  RS_035_MLRC_2022_35  [Re] Exploring the Role of Grammar and Word Ch...   \n",
       "35  RS_036_MLRC_2022_36  RELIC: Reproducibility and Extension on LIC me...   \n",
       "36  RS_037_MLRC_2022_37  [Re] VAE Approximation Error: ELBO and Exponen...   \n",
       "37  RS_038_MLRC_2022_38  Reproducibility study of the Fairness-enhanced...   \n",
       "38  RS_039_MLRC_2022_39  [Re] CrossWalk: Fairness-enhanced Node Represe...   \n",
       "39  RS_040_MLRC_2022_40  [Re] Masked Autoencoders Are Small Scale Visio...   \n",
       "40  RS_041_MLRC_2022_41  [Re] On Explainability of Graph Neural Network...   \n",
       "41  RS_042_MLRC_2022_42          [Re] â€œTowards Understanding Grokkingâ€   \n",
       "42  RS_043_MLRC_2022_43  [Re] Reproducibility Study of Behavior Transfo...   \n",
       "43  RS_044_MLRC_2022_44  A Replication Study of Compositional Generaliz...   \n",
       "44  RS_045_MLRC_2022_45  [Re] Pure Noise to the Rescue of Insufficient ...   \n",
       "45  RS_046_MLRC_2021_01                  ML Reproducibility Challenge 2021   \n",
       "46  RS_047_MLRC_2021_02            [Re] Counterfactual Generative Networks   \n",
       "47  RS_048_MLRC_2021_03  [Re] Does Self-Supervision Always Improve Few-...   \n",
       "48  RS_049_MLRC_2021_04  [Re] Weakly-Supervised Semantic Segmentation v...   \n",
       "49  RS_050_MLRC_2021_05  [Re] Reproducibility Study of â€œCounterfactua...   \n",
       "\n",
       "                                           authors_rs article_type  \\\n",
       "0               ['Ermin Omeragic', 'Vuk ÄuranoviÄ‡']  Replication   \n",
       "1   ['Mert Erkol', 'Furkan KÄ±nlÄ±', 'BarÄ±ÅŸ Ã–zc...  Replication   \n",
       "2         ['Sean Michael McLeish', 'Long Tran-Thanh']  Replication   \n",
       "3   ['Eric Langezaal', 'Jesse Belleman', 'Joeri No...  Replication   \n",
       "4                 ['Matteo Brivio', 'Cagri Coltekin']  Replication   \n",
       "5   ['Luca Marini', 'Mohamad Nabeel', 'Alexandre L...  Replication   \n",
       "6                                    ['Luka Å½ontar']  Replication   \n",
       "7                   ['Jicheng Yuan', 'Danh Le Phuoc']  Replication   \n",
       "8               ['Martin Å pendl', 'Klementina Pirc']  Replication   \n",
       "9   ['Robin Sasse', 'Aniek Eijpe', 'Jona Ruthardt'...  Replication   \n",
       "10  ['Valentinos Pariza', 'Avik Pal', 'Madhura Paw...  Replication   \n",
       "11                                 ['Kyosuke Morita']  Replication   \n",
       "12  ['Valentin Leonhard Buchner', 'Philip Onno Oli...  Replication   \n",
       "13  ['Patrick Camara', 'Mojca Catharina Kloos', 'V...  Replication   \n",
       "14  ['Marga Don', 'Satchit Chatterji', 'Milena Kap...  Replication   \n",
       "15  ['SÅ‚awomir Garcarz', 'Andreas Giorkatzi', 'An...  Replication   \n",
       "16  ['Tommaso Martorella', 'Hector Manuel Ramirez ...  Replication   \n",
       "17  ['Zjos van de Sande', 'Dennis Agafonov', 'Jelk...  Replication   \n",
       "18               ['Domen MohorÄiÄ', 'David Ocepek']  Replication   \n",
       "19  ['Alessia Hu', 'Oline Ranum', 'Chrysoula Pozri...  Replication   \n",
       "20  ['DaniÃ«l Van Dijk', 'Marten TÃ¼rk', 'L Busser...  Replication   \n",
       "21  ['Gerson de Kleuver', 'David Matthias Bikker',...  Replication   \n",
       "22  ['Walter Simoncini', 'Ioanna Gogou', 'Marta Fr...  Replication   \n",
       "23  ['Julius Wagenbach', 'Gergely Papp', 'Niklas M...  Replication   \n",
       "24  ['Erik Buis', 'Sebastiaan Dijkstra', 'Bram Hei...  Replication   \n",
       "25  ['Farrukh Baratov', 'GÃ¶ksenin YÃ¼ksel', 'Dari...  Replication   \n",
       "26  ['Eric Zila', 'Jonathan Gerbscheid', 'Luc StrÃ...  Replication   \n",
       "27   ['Jip Greven', 'Simon Stallinga', 'Zirk Seljee']  Replication   \n",
       "28  ['Aditya Prakash Patra', 'Sina Taslimi', 'Luke...  Replication   \n",
       "29  ['Didier Merk', 'Tsatsral Mendsuren', 'Denny S...  Replication   \n",
       "30            ['Albert Sund Aillet', 'Simon SondÃ©n']  Replication   \n",
       "31    ['Anes Benmerzoug', 'Miguel de Benito Delgado']  Replication   \n",
       "32              ['Victor Livernoche', 'Vidya Sujaya']  Replication   \n",
       "33  ['Dylan Cordaro', 'Shelby Cox', 'Yiman Ren', '...  Replication   \n",
       "34  ['Priyanka Bose', 'Chandra Shekhar Pandey', 'F...  Replication   \n",
       "35  ['Martijn van Raaphorst', 'Egoitz Gonzales', '...  Replication   \n",
       "36  ['Volodymyr Kyrylov', 'Navdeep Singh Bedi', 'Q...  Replication   \n",
       "37  ['Gijs Joppe Moens', 'Job de Witte', 'Tobias P...  Replication   \n",
       "38        ['Luca Pantea', 'Andrei Eusebiu Blahovici']  Replication   \n",
       "39  ['Athanasios Charisoudis', 'Simon Ekman von Hu...  Replication   \n",
       "40      ['Yannik Mahlau', 'Lukas Berg', 'Leo Kayser']  Replication   \n",
       "41  ['Alexander Shabalin', 'Ildus Sadrtdinov', 'Ev...  Replication   \n",
       "42  ['Skander Moalla', 'Manuel Madeira', 'Lorenzo ...  Replication   \n",
       "43  ['Kaiser Sun', 'Adina Williams', 'Dieuwke Hupk...  Replication   \n",
       "44              ['Seungjae Ryan Lee', 'Seungmin Lee']  Replication   \n",
       "45  Sinha, Koustuv and Dodge, Jesse and Luccioni, ...    Editorial   \n",
       "46  Ankit, Ankit and Ambekar, Sameer and Varadhara...  Replication   \n",
       "47                  Ashok, Arjun and Aekula, Haswanth  Replication   \n",
       "48  Athanasiadis, Ioannis and Moschovis, Georgios ...  Replication   \n",
       "49  Bagad, Piyush and Hilders, Paul and Maas, Jess...  Replication   \n",
       "\n",
       "                                             keywords  \\\n",
       "0   ['rescience c', 'machine learning', 'data augm...   \n",
       "1   ['feature distribution matching', 'style trans...   \n",
       "2   ['rescience c', 'Algorithmic Reasoning', 'Deep...   \n",
       "3   ['Reproducibility', 'label-free', 'unsupervise...   \n",
       "4   ['BERT', 'fastText', 'embeddings', 'language m...   \n",
       "5   ['self-supervised learning', 'contrastive lear...   \n",
       "6   ['Optimization', 'bandit learning', 'Thompson ...   \n",
       "7   ['Hypergraph', 'Deep Metric Learning', 'Semant...   \n",
       "8   ['transfer learning', 'Bayesian inference', 'i...   \n",
       "9   ['Machine Learning', 'Artificial Intelligence'...   \n",
       "10  ['Reproducibility', 'Feature Importance', 'Exa...   \n",
       "11                     ['Counterfactual explanation']   \n",
       "12  ['Fairness and Bias in ML', 'Fair Machine Lear...   \n",
       "13  ['Reproduction', 'Evaluation', 'Dialogue Summa...   \n",
       "14  ['ReScience C', 'ReScience X', 'Machine Learni...   \n",
       "15  ['Label-Free Explainability', 'Explainability'...   \n",
       "16  ['rescience c', 'machine learning', 'deep lear...   \n",
       "17  ['fairness', 'demographic shift', 'classificat...   \n",
       "18  ['rescience c', 'machine learning', 'decision ...   \n",
       "19  ['Reproducibility', 'Information Retrieval', '...   \n",
       "20  ['reproducibility', 'fairness', 'gender', 'rac...   \n",
       "21  ['Reproducibility', 'Interpretability', 'Proto...   \n",
       "22  ['rescience c', 'machine learning', 'deep lear...   \n",
       "23  ['Explainable AI', 'Unsupervised Learning', 'F...   \n",
       "24  ['Reproduce', 'Interpretability', 'Convolution...   \n",
       "25  ['bias', 'gender bias', 'racial bias', 'age bi...   \n",
       "26  ['rescience c', 'deepwalk', 'crosswalk', 'grap...   \n",
       "27  ['fairness', 'bias', 'face verification', 'fai...   \n",
       "28  ['rescience c', 'machine learning', 'Explainab...   \n",
       "29  ['Latent space smoothing', 'LASSI', 'reproduci...   \n",
       "30  ['Neural Cellular Automata', 'Cellular Automat...   \n",
       "31  ['data valuation', 'least core', 'shapley valu...   \n",
       "32  ['prompt engineering', 'label engineering', 'm...   \n",
       "33  ['Python', 'graphical neural networks', 'mixup...   \n",
       "34  ['Machine Learning', 'Bias', 'BERT', 'Word2Vec...   \n",
       "35  ['rescience c', 'rescience x', 'machine learni...   \n",
       "36  ['Python', 'VAE', 'approximation errors', 'ELB...   \n",
       "37  ['Crosswalk', 'Fairness', 'Graph Networks', 'G...   \n",
       "38  ['Fairness in Machine Learning', 'Representati...   \n",
       "39  ['rescience c', 'python', 'pytorch', 'machine ...   \n",
       "40  ['Rescience c', 'Rescience x', 'Explainable AI...   \n",
       "41           ['grokking', 'neural networks', 'MNIST']   \n",
       "42  ['rescience c', 'machine learning', 'reinforce...   \n",
       "43  ['Reproducibility', 'Compositional Generalizat...   \n",
       "44           ['data augmentation', 'computer vision']   \n",
       "45  rescience c, machine learning, deep learning, ...   \n",
       "46  rescience c, machine learning, deep learning, ...   \n",
       "47  rescience c, machine learning, deep learning, ...   \n",
       "48  rescience c, machine learning, deep learning, ...   \n",
       "49  rescience c, machine learning, python, pytorch...   \n",
       "\n",
       "                                             abstract  \\\n",
       "0   Scope of Reproducibility\\nThis paper presents ...   \n",
       "1   Reproducibility Summary:\\n\\nIn this reproducib...   \n",
       "2   Scope of Reproducibility:\\nIn this report, we ...   \n",
       "3   Scope of Reproducibility â€” This study is an ...   \n",
       "4   This report summarizes our efforts to reproduc...   \n",
       "5   Reproducibility Summary\\n\\nScope of Reproducib...   \n",
       "6   The paper presents a novel DE approach using T...   \n",
       "7   Reproducibility Summary\\n\\nScope of Reproducib...   \n",
       "8   REPRODUCIBILITY SUMMARY\\n\\nScope of Reproducib...   \n",
       "9   Scope of Reproducibility â€” CartoonX [1] is a...   \n",
       "10  Scope of Reproducibility\\nIn this work, we eva...   \n",
       "11  Scope of Reproducibility\\n\\nThis study aims to...   \n",
       "12  \\nScope of Reproducibility: The original autho...   \n",
       "13  Scope of Reproducibility â€” In this paper, we...   \n",
       "14  Scope of Reproducibility â€” This paper aims t...   \n",
       "15  Reproducibility Summary\\n\\nScope of Reproducib...   \n",
       "16  Neural networks have become very common in mac...   \n",
       "17  Reproducibility Summary\\n\\nScope of Reproducib...   \n",
       "18  Scope of Reproducibility\\nThe paper presents a...   \n",
       "19  Scope of Reproducibility \\nIn this work, we st...   \n",
       "20  Scope of Reproducibility â€” The main objectiv...   \n",
       "21  Scope of Reproducibility â€” \\n\\nThis paper an...   \n",
       "22  Scope of Reproducibility\\n\\nThis paper attempt...   \n",
       "23  In this work, we present our reproducibility s...   \n",
       "24  Scope of Reproducibility\\nIn this work, we aim...   \n",
       "25  Scope of reproducibility - We study the reprod...   \n",
       "26  Scope of Reproducibility - The original author...   \n",
       "27  Reproducibility Summary\\n===\\n\\nScope of Repro...   \n",
       "28  In this reproducibility study, we verify the c...   \n",
       "29  - Scope of Reproducibility\\nThe aim of this wo...   \n",
       "30  The main claim of the paper being reproduced i...   \n",
       "31  We investigate the results of [1] in the field...   \n",
       "32  We reproduce the results in the paper Automati...   \n",
       "33  We attempt to reproduce results on a novel gra...   \n",
       "34  Reproducibility Summary\\nScope of Reproducibil...   \n",
       "35  Scope of Reproducibility\\nIn this work we repr...   \n",
       "36  Scope of Reproducibility â€” Exponential famil...   \n",
       "37  \"CrossWalk: Fairness-Enhanced Node Representat...   \n",
       "38  Scope of Reproducibility\\nThis work aims to re...   \n",
       "39  Scope of Reproducibility â€” The Masked Autoen...   \n",
       "40  Yuan et al. claim their proposed method Subgra...   \n",
       "41  Scope of Reproducibility\\nIn this work, we att...   \n",
       "42  Scope of Reproducibility - In this work, we an...   \n",
       "43  Reproducibility Summary\\nScope of Reproducibil...   \n",
       "44  Scope of Reproducibility â€” We examine the ma...   \n",
       "45                         DO EXTRACT FROM review_url   \n",
       "46                         DO EXTRACT FROM review_url   \n",
       "47                         DO EXTRACT FROM review_url   \n",
       "48                         DO EXTRACT FROM review_url   \n",
       "49                         DO EXTRACT FROM review_url   \n",
       "\n",
       "                                           pdf_url_rs  \\\n",
       "0   /pdf/8c87cb1e84e1482826c40a3b0c43928eaef747f3.pdf   \n",
       "1   /pdf/7bcb577c2a46db29c48234a5b72368053c7ebed3.pdf   \n",
       "2   /pdf/07d5d68b5873d779bd1fd8c95b9767cb57fe0bc4.pdf   \n",
       "3   /pdf/7fde4f12c675150699fc376cd097d5b9dad0b4d4.pdf   \n",
       "4   /pdf/78430c5af33bc892d852b49a5a6e93abeb314c6f.pdf   \n",
       "5   /pdf/fc591d20ed7c340bbe470789beb95d4cde81e7d5.pdf   \n",
       "6   /pdf/cf9062c0ba6651cb416547df2c1e7e7379d572a9.pdf   \n",
       "7   /pdf/9a228c78b9d481df388578a8e5d78a90412b9891.pdf   \n",
       "8   /pdf/4247c7fbacadbf0bc1d7e5fc08e2c4ce35315657.pdf   \n",
       "9   /pdf/6f7b408230bc01c524943393638a23d4a8f73cc6.pdf   \n",
       "10  /pdf/b037311c182b52dd01853262e9d4fb09103cddae.pdf   \n",
       "11  /pdf/58f0c36da4cbe7eba434f580fb26822b34c6757e.pdf   \n",
       "12  /pdf/a154eb4c4168594c9fec045c44baecf70fac36c0.pdf   \n",
       "13  /pdf/a49585e9e327fcb0e6dcf55d03569a33782b5777.pdf   \n",
       "14  /pdf/e37fde64b0642ea7024c23ed0835ce07292722ff.pdf   \n",
       "15  /pdf/b9d086713ae4efccb1558881e34a3d88acfdb549.pdf   \n",
       "16  /pdf/e8c787801a1d8bad0fb00b5952625b3115ba6fb3.pdf   \n",
       "17  /pdf/6e8499b6188da557805216d83be1cd029ba00bd5.pdf   \n",
       "18  /pdf/8e7ec4f4b707646fa2406a3a1b46e6361ec7caaf.pdf   \n",
       "19  /pdf/8634a0b0fe52eb23f0fa54633413fd4f9176c966.pdf   \n",
       "20  /pdf/57b829f2270fa6cc431997add0162fba06a9bc91.pdf   \n",
       "21  /pdf/0cad70c6cd1e3d75ccf19794d4af2967be60ede5.pdf   \n",
       "22  /pdf/be89dd7facfe263fd65d756feb4b3e75b715b2f2.pdf   \n",
       "23  /pdf/34fdaf29b0edaa22d1258444847ba34b4b9c9fe9.pdf   \n",
       "24  /pdf/d2f5afd089bc7f1472bb80dc9e9933df836b4433.pdf   \n",
       "25  /pdf/764521708b35c1dab2f1507074f5b94b3ff70e06.pdf   \n",
       "26  /pdf/8216b02ff557ce4ec1c022cb7a0707519c499e8d.pdf   \n",
       "27  /pdf/cd9d92da523749654ff721317617b5150b8f929f.pdf   \n",
       "28  /pdf/c95ff07d5f89d48e72218d15f55784039360b4af.pdf   \n",
       "29  /pdf/297b0703140390d112c05d67e700b2dbed1e9278.pdf   \n",
       "30  /pdf/adc8d78a91a593e12451cda54690da8942220f3f.pdf   \n",
       "31  /pdf/a998d0fe66a5e322bb8ff2d4d9f9a6773770aa4a.pdf   \n",
       "32  /pdf/b64f0e09258f2c039d39a1ee7db8a6aea63e4799.pdf   \n",
       "33  /pdf/290dee2b4604ec276e9382fdb7e9d0ab378bdb23.pdf   \n",
       "34  /pdf/b2c19856585f093a3b6f8da25e1cf6c5b87bb812.pdf   \n",
       "35  /pdf/7e16d4e632092a5ed4cf3092c119212d8fc0c138.pdf   \n",
       "36  /pdf/46c6b4f22344f8ae9d8cf06031c31771b5587457.pdf   \n",
       "37  /pdf/194c8f8a2ee0dd7329a023d299003f45684a3cf6.pdf   \n",
       "38  /pdf/eb0d993e25d9bf3503d18c37700bfa51744ee1c8.pdf   \n",
       "39  /pdf/c9616492444c28600ec773aa1c6db9667f51da99.pdf   \n",
       "40  /pdf/cbf6c4b95a73a9b9d62e6fc49248af23160caae8.pdf   \n",
       "41  /pdf/9d5c9e36691ac9fe4b65941ca8908c0f8b3d7519.pdf   \n",
       "42  /pdf/184673425e85b10da526e3542760edb71d757856.pdf   \n",
       "43  /pdf/35fb3e86fa46c3cd82126cb00e4b161353283cac.pdf   \n",
       "44  /pdf/e83b2fc88fcc0e7d6ce8fc8a73b1877007b716dc.pdf   \n",
       "45  https://zenodo.org/record/6574723/files/articl...   \n",
       "46  https://zenodo.org/record/6574625/files/articl...   \n",
       "47  https://zenodo.org/record/6574629/files/articl...   \n",
       "48  https://zenodo.org/record/6574631/files/articl...   \n",
       "49  https://zenodo.org/record/6574635/files/articl...   \n",
       "\n",
       "                                          code_url_rs  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "18                                                NaN   \n",
       "19                                                NaN   \n",
       "20                                                NaN   \n",
       "21                                                NaN   \n",
       "22                                                NaN   \n",
       "23                                                NaN   \n",
       "24                                                NaN   \n",
       "25                                                NaN   \n",
       "26                                                NaN   \n",
       "27                                                NaN   \n",
       "28                                                NaN   \n",
       "29                                                NaN   \n",
       "30                                                NaN   \n",
       "31                                                NaN   \n",
       "32                                                NaN   \n",
       "33                                                NaN   \n",
       "34                                                NaN   \n",
       "35                                                NaN   \n",
       "36                                                NaN   \n",
       "37                                                NaN   \n",
       "38                                                NaN   \n",
       "39                                                NaN   \n",
       "40                                                NaN   \n",
       "41                                                NaN   \n",
       "42                                                NaN   \n",
       "43                                                NaN   \n",
       "44                                                NaN   \n",
       "45                                                NaN   \n",
       "46        https://github.com/ambekarsameer96/FACT_AI/   \n",
       "47  https://github.com/ashok-arjun/MLRC-2021-Few-S...   \n",
       "48  https://github.com/athaioan/ViT_Affinity_Repro...   \n",
       "49       https://github.com/danilodegoede/fact-team3/   \n",
       "\n",
       "                                        org_paper_url  \\\n",
       "0      https://proceedings.mlr.press/v162/han22c.html   \n",
       "1   https://openaccess.thecvf.com/content/CVPR2022...   \n",
       "2                https://arxiv.org/pdf/2202.05826.pdf   \n",
       "3   https://proceedings.mlr.press/v162/crabbe22a.html   \n",
       "4         https://aclanthology.org/2021.acl-long.281/   \n",
       "5   https://proceedings.neurips.cc/paper/2021/hash...   \n",
       "6   https://neurips.cc/Conferences/2022/ScheduleMu...   \n",
       "7   https://openaccess.thecvf.com/content/CVPR2022...   \n",
       "8          https://openreview.net/forum?id=ao30zaT3YL   \n",
       "9        https://doi.org/10.1007/978-3-031-19775-8_26   \n",
       "10  https://proceedings.mlr.press/v162/crabbe22a/c...   \n",
       "11                   https://arxiv.org/abs/1911.12199   \n",
       "12           https://iclr.cc/virtual/2022/poster/6666   \n",
       "13      https://aclanthology.org/2022.naacl-main.418/   \n",
       "14          https://openreview.net/pdf?id=nRj0NcmSuxb   \n",
       "15  https://proceedings.mlr.press/v162/crabbe22a/c...   \n",
       "16  https://proceedings.neurips.cc/paper/2021/file...   \n",
       "17        https://openreview.net/forum?id=wbPObLm6ueA   \n",
       "18          https://icml.cc/virtual/2022/poster/16829   \n",
       "19                   https://arxiv.org/abs/2205.00048   \n",
       "20                   https://arxiv.org/abs/2203.15395   \n",
       "21                   https://arxiv.org/abs/2204.11830   \n",
       "22        https://openreview.net/forum?id=irARV_2VFs4   \n",
       "23                   https://arxiv.org/abs/2203.01928   \n",
       "24       https://ieeexplore.ieee.org/document/9878481   \n",
       "25  https://openaccess.thecvf.com/content/CVPR2022...   \n",
       "26  https://ojs.aaai.org/index.php/AAAI/article/vi...   \n",
       "27                 https://arxiv.org/abs/2106.03761v4   \n",
       "28  https://www.ecva.net/papers/eccv_2022/papers_E...   \n",
       "29                 https://arxiv.org/abs/2111.13650v3   \n",
       "30                   https://arxiv.org/abs/2201.12360   \n",
       "31  https://ojs.aaai.org/index.php/AAAI/article/vi...   \n",
       "32   https://aclanthology.org/2022.naacl-main.401.pdf   \n",
       "33     https://proceedings.mlr.press/v162/han22c.html   \n",
       "34  https://dl.acm.org/doi/pdf/10.1145/3531146.353...   \n",
       "35                   https://arxiv.org/abs/2203.15395   \n",
       "36        https://openreview.net/forum?id=OIs3SxU5Ynl   \n",
       "37               https://arxiv.org/pdf/2105.02725.pdf   \n",
       "38                   https://arxiv.org/abs/2105.02725   \n",
       "39  https://openaccess.thecvf.com/content/CVPR2022...   \n",
       "40         https://proceedings.mlr.press/v139/yuan21c   \n",
       "41           https://openreview.net/pdf?id=6at6rB3IZm   \n",
       "42  https://proceedings.neurips.cc/paper_files/pap...   \n",
       "43         https://aclanthology.org/2021.acl-long.75/   \n",
       "44         https://proceedings.mlr.press/v162/zada22a   \n",
       "45                         DO EXTRACT FROM review_url   \n",
       "46                   https://arxiv.org/abs/2101.06046   \n",
       "47  https://openreview.net/forum?id=738x44N7yUE&no...   \n",
       "48  https://openaccess.thecvf.com/content/CVPR2021...   \n",
       "49          https://openreview.net/pdf?id=BXewfAYMmJw   \n",
       "\n",
       "                                        doi_rs  ... challenge_year_rs  \\\n",
       "0   https://www.doi.org/10.5281/zenodo.8173650  ...              2022   \n",
       "1   https://www.doi.org/10.5281/zenodo.8173652  ...              2022   \n",
       "2   https://www.doi.org/10.5281/zenodo.8173654  ...              2022   \n",
       "3   https://www.doi.org/10.5281/zenodo.8173656  ...              2022   \n",
       "4   https://www.doi.org/10.5281/zenodo.8173658  ...              2022   \n",
       "5   https://www.doi.org/10.5281/zenodo.8173662  ...              2022   \n",
       "6   https://www.doi.org/10.5281/zenodo.8173664  ...              2022   \n",
       "7   https://www.doi.org/10.5281/zenodo.8173666  ...              2022   \n",
       "8   https://www.doi.org/10.5281/zenodo.8173668  ...              2022   \n",
       "9   https://www.doi.org/10.5281/zenodo.8173672  ...              2022   \n",
       "10  https://www.doi.org/10.5281/zenodo.8173674  ...              2022   \n",
       "11  https://www.doi.org/10.5281/zenodo.8173678  ...              2022   \n",
       "12  https://www.doi.org/10.5281/zenodo.8173680  ...              2022   \n",
       "13  https://www.doi.org/10.5281/zenodo.8173682  ...              2022   \n",
       "14  https://www.doi.org/10.5281/zenodo.8173686  ...              2022   \n",
       "15  https://www.doi.org/10.5281/zenodo.8173688  ...              2022   \n",
       "16  https://www.doi.org/10.5281/zenodo.8173692  ...              2022   \n",
       "17  https://www.doi.org/10.5281/zenodo.8206607  ...              2022   \n",
       "18  https://www.doi.org/10.5281/zenodo.8173696  ...              2022   \n",
       "19  https://www.doi.org/10.5281/zenodo.8173698  ...              2022   \n",
       "20  https://www.doi.org/10.5281/zenodo.8173703  ...              2022   \n",
       "21  https://www.doi.org/10.5281/zenodo.8173705  ...              2022   \n",
       "22  https://www.doi.org/10.5281/zenodo.8173707  ...              2022   \n",
       "23  https://www.doi.org/10.5281/zenodo.8173711  ...              2022   \n",
       "24  https://www.doi.org/10.5281/zenodo.8173713  ...              2022   \n",
       "25  https://www.doi.org/10.5281/zenodo.8173715  ...              2022   \n",
       "26  https://www.doi.org/10.5281/zenodo.8173717  ...              2022   \n",
       "27  https://www.doi.org/10.5281/zenodo.8173719  ...              2022   \n",
       "28  https://www.doi.org/10.5281/zenodo.8173721  ...              2022   \n",
       "29  https://www.doi.org/10.5281/zenodo.8173725  ...              2022   \n",
       "30  https://www.doi.org/10.5281/zenodo.8173729  ...              2022   \n",
       "31  https://www.doi.org/10.5281/zenodo.8173733  ...              2022   \n",
       "32  https://www.doi.org/10.5281/zenodo.8173735  ...              2022   \n",
       "33  https://www.doi.org/10.5281/zenodo.8173737  ...              2022   \n",
       "34  https://www.doi.org/10.5281/zenodo.8173739  ...              2022   \n",
       "35  https://www.doi.org/10.5281/zenodo.8173741  ...              2022   \n",
       "36  https://www.doi.org/10.5281/zenodo.8173745  ...              2022   \n",
       "37  https://www.doi.org/10.5281/zenodo.8173747  ...              2022   \n",
       "38  https://www.doi.org/10.5281/zenodo.8173749  ...              2022   \n",
       "39  https://www.doi.org/10.5281/zenodo.8173751  ...              2022   \n",
       "40  https://www.doi.org/10.5281/zenodo.8173753  ...              2022   \n",
       "41  https://www.doi.org/10.5281/zenodo.8173755  ...              2022   \n",
       "42  https://www.doi.org/10.5281/zenodo.8173757  ...              2022   \n",
       "43  https://www.doi.org/10.5281/zenodo.8173759  ...              2022   \n",
       "44  https://www.doi.org/10.5281/zenodo.8173763  ...              2022   \n",
       "45                      10.5281/zenodo.6574723  ...              2021   \n",
       "46                      10.5281/zenodo.6574625  ...              2021   \n",
       "47                      10.5281/zenodo.6574629  ...              2021   \n",
       "48                      10.5281/zenodo.6574631  ...              2021   \n",
       "49                      10.5281/zenodo.6574635  ...              2021   \n",
       "\n",
       "                                        review_url_rs   journal_rs  \\\n",
       "0         https://openreview.net/forum?id=XxUIomN-ndH          NaN   \n",
       "1   https://openreview.net/forum?id=a5_hbZf0NB&not...          NaN   \n",
       "2          https://openreview.net/forum?id=WaZB4pUVTi          NaN   \n",
       "3         https://openreview.net/forum?id=bBVZ3pY4z8p          NaN   \n",
       "4         https://openreview.net/forum?id=Od5dD58libt          NaN   \n",
       "5          https://openreview.net/forum?id=gb71irTNN7          NaN   \n",
       "6          https://openreview.net/forum?id=NE_x1dpz-Q          NaN   \n",
       "7          https://openreview.net/forum?id=JJQbk2hIQ5          NaN   \n",
       "8          https://openreview.net/forum?id=JpaQ8GFOVu          NaN   \n",
       "9         https://openreview.net/forum?id=MK4IQJdLLeo          NaN   \n",
       "10         https://openreview.net/forum?id=qP34dvJpHd          NaN   \n",
       "11  https://openreview.net/forum?id=n1q-iz83S5&not...          NaN   \n",
       "12  https://openreview.net/forum?id=xEfg6h1GFmW&no...          NaN   \n",
       "13  https://openreview.net/forum?id=3jaZ5tKRyiT&no...          NaN   \n",
       "14         https://openreview.net/forum?id=uVHUy7CWCL          NaN   \n",
       "15         https://openreview.net/forum?id=sF_vYZSxSV          NaN   \n",
       "16         https://openreview.net/forum?id=YAWQTQZVoA          NaN   \n",
       "17         https://openreview.net/forum?id=MMuv-v99Hy          NaN   \n",
       "18         https://openreview.net/forum?id=NgPQSqpz-Y          NaN   \n",
       "19        https://openreview.net/forum?id=A0Sjs3IJWb-          NaN   \n",
       "20        https://openreview.net/forum?id=N9Wn91tE7D0          NaN   \n",
       "21         https://openreview.net/forum?id=a_9YF58u61          NaN   \n",
       "22         https://openreview.net/forum?id=ye8PftiQLQ          NaN   \n",
       "23        https://openreview.net/forum?id=n2qXFXiMsAM          NaN   \n",
       "24         https://openreview.net/forum?id=nsrHznwHhl          NaN   \n",
       "25         https://openreview.net/forum?id=eJmQJT0Dtt          NaN   \n",
       "26         https://openreview.net/forum?id=WnaVgRhlyT          NaN   \n",
       "27         https://openreview.net/forum?id=jDBYRwDpeW          NaN   \n",
       "28         https://openreview.net/forum?id=DWKJpl8s06          NaN   \n",
       "29        https://openreview.net/forum?id=J-Lgb7Vc0wX          NaN   \n",
       "30         https://openreview.net/forum?id=d7-ns6SZqp          NaN   \n",
       "31         https://openreview.net/forum?id=vWzZQAahuW          NaN   \n",
       "32        https://openreview.net/forum?id=t8ZZ2Y356Ix          NaN   \n",
       "33  https://openreview.net/forum?id=T54wy0ahGLG&no...          NaN   \n",
       "34        https://openreview.net/forum?id=MjZVx7a0KX-          NaN   \n",
       "35        https://openreview.net/forum?id=9_hCoP3LXwy          NaN   \n",
       "36         https://openreview.net/forum?id=ozbAwipuZu          NaN   \n",
       "37  https://openreview.net/forum?id=KNp7Zq3KkT0&no...          NaN   \n",
       "38        https://openreview.net/forum?id=tpk45Zll8eh          NaN   \n",
       "39        https://openreview.net/forum?id=KXfjZPL5pqr          NaN   \n",
       "40         https://openreview.net/forum?id=zKBJw4Ht8s          NaN   \n",
       "41         https://openreview.net/forum?id=Vz9VLcJqKS          NaN   \n",
       "42        https://openreview.net/forum?id=E0qO5dI5aEn          NaN   \n",
       "43        https://openreview.net/forum?id=MF9uv95psps          NaN   \n",
       "44         https://openreview.net/forum?id=ErBe4MnsVD          NaN   \n",
       "45                                                NaN  ReScience C   \n",
       "46        https://openreview.net/forum?id=BSHg22G7n0F  ReScience C   \n",
       "47         https://openreview.net/forum?id=ScfP3G73CY  ReScience C   \n",
       "48         https://openreview.net/forum?id=rcEDhGX3AY  ReScience C   \n",
       "49        https://openreview.net/forum?id=HNlzT3G720t  ReScience C   \n",
       "\n",
       "    volume_journal_rs issue_journal_rs published_year_journal_rs  \\\n",
       "0                 NaN              NaN                       NaN   \n",
       "1                 NaN              NaN                       NaN   \n",
       "2                 NaN              NaN                       NaN   \n",
       "3                 NaN              NaN                       NaN   \n",
       "4                 NaN              NaN                       NaN   \n",
       "5                 NaN              NaN                       NaN   \n",
       "6                 NaN              NaN                       NaN   \n",
       "7                 NaN              NaN                       NaN   \n",
       "8                 NaN              NaN                       NaN   \n",
       "9                 NaN              NaN                       NaN   \n",
       "10                NaN              NaN                       NaN   \n",
       "11                NaN              NaN                       NaN   \n",
       "12                NaN              NaN                       NaN   \n",
       "13                NaN              NaN                       NaN   \n",
       "14                NaN              NaN                       NaN   \n",
       "15                NaN              NaN                       NaN   \n",
       "16                NaN              NaN                       NaN   \n",
       "17                NaN              NaN                       NaN   \n",
       "18                NaN              NaN                       NaN   \n",
       "19                NaN              NaN                       NaN   \n",
       "20                NaN              NaN                       NaN   \n",
       "21                NaN              NaN                       NaN   \n",
       "22                NaN              NaN                       NaN   \n",
       "23                NaN              NaN                       NaN   \n",
       "24                NaN              NaN                       NaN   \n",
       "25                NaN              NaN                       NaN   \n",
       "26                NaN              NaN                       NaN   \n",
       "27                NaN              NaN                       NaN   \n",
       "28                NaN              NaN                       NaN   \n",
       "29                NaN              NaN                       NaN   \n",
       "30                NaN              NaN                       NaN   \n",
       "31                NaN              NaN                       NaN   \n",
       "32                NaN              NaN                       NaN   \n",
       "33                NaN              NaN                       NaN   \n",
       "34                NaN              NaN                       NaN   \n",
       "35                NaN              NaN                       NaN   \n",
       "36                NaN              NaN                       NaN   \n",
       "37                NaN              NaN                       NaN   \n",
       "38                NaN              NaN                       NaN   \n",
       "39                NaN              NaN                       NaN   \n",
       "40                NaN              NaN                       NaN   \n",
       "41                NaN              NaN                       NaN   \n",
       "42                NaN              NaN                       NaN   \n",
       "43                NaN              NaN                       NaN   \n",
       "44                NaN              NaN                       NaN   \n",
       "45                8.0              2.0                    2022.0   \n",
       "46                8.0              2.0                    2022.0   \n",
       "47                8.0              2.0                    2022.0   \n",
       "48                8.0              2.0                    2022.0   \n",
       "49                8.0              2.0                    2022.0   \n",
       "\n",
       "                               domain  entry_type  rs_score rs_comment  \n",
       "0   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "1   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "2   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "3   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "4   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "5   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "6   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "7   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "8   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "9   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "10  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "11  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "12  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "13  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "14  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "15  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "16  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "17  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "18  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "19  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "20  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "21  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "22  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "23  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "24  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "25  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "26  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "27  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "28  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "29  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "30  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "31  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "32  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "33  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "34  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "35  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "36  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "37  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "38  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "39  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "40  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "41  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "42  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "43  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "44  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "45  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "46  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "47  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "48  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "49  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "\n",
       "[50 rows x 23 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_with_doi = df_xlsx.copy()\n",
    "\n",
    "for index, row in df_xlsx.iterrows():\n",
    "    print(row['key_for_all_RS'])\n",
    "    if str(row['review_url_rs']) != 'nan':\n",
    "        if index>116:\n",
    "            print(row['review_url_rs'])\n",
    "            pass\n",
    "        else:\n",
    "            page = requests.get(row['review_url_rs'].strip())\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "            # Meta-Data extraction\n",
    "            details = soup.find(id=\"__NEXT_DATA__\")\n",
    "            full_dict = json.loads(details.text)\n",
    "            content_dict = full_dict['props']['pageProps']['forumNote']['content']\n",
    "\n",
    "            if \"DO EXTRACT FROM\" not in str(row['org_paper_url']):\n",
    "                 print(\"A value is already there\")\n",
    "            else:\n",
    "                new_df_with_doi.at[index, 'org_paper_url'] = content_dict.get('paper_url')   \n",
    "    else:\n",
    "        print(row['key_for_all_RS'], '----- nan ISSUE')\n",
    "        \n",
    "new_df_with_doi.reset_index(drop=True)    \n",
    "\n",
    "new_df_with_doi.to_excel(\"RS_ALL_IN_ONE_metadata_with_org_paper_URL.xlsx\",index=False)  \n",
    "new_df_with_doi.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382c3620",
   "metadata": {},
   "source": [
    "## ``original paper doi search (manual process )``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "929928c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_for_all_RS</th>\n",
       "      <th>title_rs</th>\n",
       "      <th>authors_rs</th>\n",
       "      <th>article_type</th>\n",
       "      <th>keywords</th>\n",
       "      <th>abstract</th>\n",
       "      <th>pdf_url_rs</th>\n",
       "      <th>code_url_rs</th>\n",
       "      <th>org_paper_url</th>\n",
       "      <th>doi_rs</th>\n",
       "      <th>...</th>\n",
       "      <th>challenge_year_rs</th>\n",
       "      <th>review_url_rs</th>\n",
       "      <th>journal_rs</th>\n",
       "      <th>volume_journal_rs</th>\n",
       "      <th>issue_journal_rs</th>\n",
       "      <th>published_year_journal_rs</th>\n",
       "      <th>domain</th>\n",
       "      <th>entry_type</th>\n",
       "      <th>rs_score</th>\n",
       "      <th>rs_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RS_001_MLRC_2022_01</td>\n",
       "      <td>[Re] $\\mathcal{G}$-Mixup: Graph Data Augmentat...</td>\n",
       "      <td>['Ermin Omeragic', 'Vuk ÄuranoviÄ‡']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'machine learning', 'data augm...</td>\n",
       "      <td>Scope of Reproducibility\\nThis paper presents ...</td>\n",
       "      <td>/pdf/8c87cb1e84e1482826c40a3b0c43928eaef747f3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.mlr.press/v162/han22c.html</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173650</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=XxUIomN-ndH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RS_002_MLRC_2022_02</td>\n",
       "      <td>[Re] Exact Feature Distribution Matching for A...</td>\n",
       "      <td>['Mert Erkol', 'Furkan KÄ±nlÄ±', 'BarÄ±ÅŸ Ã–zc...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['feature distribution matching', 'style trans...</td>\n",
       "      <td>Reproducibility Summary:\\n\\nIn this reproducib...</td>\n",
       "      <td>/pdf/7bcb577c2a46db29c48234a5b72368053c7ebed3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openaccess.thecvf.com/content/CVPR2022...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173652</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=a5_hbZf0NB&amp;not...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RS_003_MLRC_2022_03</td>\n",
       "      <td>[Re] End-to-end Algorithm Synthesis with Recur...</td>\n",
       "      <td>['Sean Michael McLeish', 'Long Tran-Thanh']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'Algorithmic Reasoning', 'Deep...</td>\n",
       "      <td>Scope of Reproducibility:\\nIn this report, we ...</td>\n",
       "      <td>/pdf/07d5d68b5873d779bd1fd8c95b9767cb57fe0bc4.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/pdf/2202.05826.pdf</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173654</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=WaZB4pUVTi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RS_004_MLRC_2022_04</td>\n",
       "      <td>[Re] Label-Free Explainability for Unsupervise...</td>\n",
       "      <td>['Eric Langezaal', 'Jesse Belleman', 'Joeri No...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Reproducibility', 'label-free', 'unsupervise...</td>\n",
       "      <td>Scope of Reproducibility â€” This study is an ...</td>\n",
       "      <td>/pdf/7fde4f12c675150699fc376cd097d5b9dad0b4d4.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.mlr.press/v162/crabbe22a.html</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173656</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=bBVZ3pY4z8p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RS_005_MLRC_2022_05</td>\n",
       "      <td>[Re] Exploring the Representation of Word Mean...</td>\n",
       "      <td>['Matteo Brivio', 'Cagri Coltekin']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['BERT', 'fastText', 'embeddings', 'language m...</td>\n",
       "      <td>This report summarizes our efforts to reproduc...</td>\n",
       "      <td>/pdf/78430c5af33bc892d852b49a5a6e93abeb314c6f.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://aclanthology.org/2021.acl-long.281/</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173658</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=Od5dD58libt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RS_006_MLRC_2022_06</td>\n",
       "      <td>[Re] Intriguing Properties of Contrastive Losses</td>\n",
       "      <td>['Luca Marini', 'Mohamad Nabeel', 'Alexandre L...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['self-supervised learning', 'contrastive lear...</td>\n",
       "      <td>Reproducibility Summary\\n\\nScope of Reproducib...</td>\n",
       "      <td>/pdf/fc591d20ed7c340bbe470789beb95d4cde81e7d5.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.neurips.cc/paper/2021/hash...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173662</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=gb71irTNN7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RS_007_MLRC_2022_07</td>\n",
       "      <td>[Re] Bandit Theory and Thompson Sampling-guide...</td>\n",
       "      <td>['Luka Å½ontar']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Optimization', 'bandit learning', 'Thompson ...</td>\n",
       "      <td>The paper presents a novel DE approach using T...</td>\n",
       "      <td>/pdf/cf9062c0ba6651cb416547df2c1e7e7379d572a9.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://neurips.cc/Conferences/2022/ScheduleMu...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173664</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=NE_x1dpz-Q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RS_008_MLRC_2022_08</td>\n",
       "      <td>[Re] Hypergraph-Induced Semantic Tuplet Loss f...</td>\n",
       "      <td>['Jicheng Yuan', 'Danh Le Phuoc']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Hypergraph', 'Deep Metric Learning', 'Semant...</td>\n",
       "      <td>Reproducibility Summary\\n\\nScope of Reproducib...</td>\n",
       "      <td>/pdf/9a228c78b9d481df388578a8e5d78a90412b9891.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openaccess.thecvf.com/content/CVPR2022...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173666</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=JJQbk2hIQ5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RS_009_MLRC_2022_09</td>\n",
       "      <td>Easy Bayesian Transfer Learning with Informati...</td>\n",
       "      <td>['Martin Å pendl', 'Klementina Pirc']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['transfer learning', 'Bayesian inference', 'i...</td>\n",
       "      <td>REPRODUCIBILITY SUMMARY\\n\\nScope of Reproducib...</td>\n",
       "      <td>/pdf/4247c7fbacadbf0bc1d7e5fc08e2c4ce35315657.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openreview.net/forum?id=ao30zaT3YL</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173668</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=JpaQ8GFOVu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RS_010_MLRC_2022_10</td>\n",
       "      <td>[Re] On the Reproducibility of CartoonX</td>\n",
       "      <td>['Robin Sasse', 'Aniek Eijpe', 'Jona Ruthardt'...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Machine Learning', 'Artificial Intelligence'...</td>\n",
       "      <td>Scope of Reproducibility â€” CartoonX [1] is a...</td>\n",
       "      <td>/pdf/6f7b408230bc01c524943393638a23d4a8f73cc6.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://doi.org/10.1007/978-3-031-19775-8_26</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173672</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=MK4IQJdLLeo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RS_011_MLRC_2022_11</td>\n",
       "      <td>Reproducibility Study of \"Label-Free Explainab...</td>\n",
       "      <td>['Valentinos Pariza', 'Avik Pal', 'Madhura Paw...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Reproducibility', 'Feature Importance', 'Exa...</td>\n",
       "      <td>Scope of Reproducibility\\nIn this work, we eva...</td>\n",
       "      <td>/pdf/b037311c182b52dd01853262e9d4fb09103cddae.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.mlr.press/v162/crabbe22a/c...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173674</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=qP34dvJpHd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RS_012_MLRC_2022_12</td>\n",
       "      <td>[Re] FOCUS: Flexible Optimizable Counterfactua...</td>\n",
       "      <td>['Kyosuke Morita']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Counterfactual explanation']</td>\n",
       "      <td>Scope of Reproducibility\\n\\nThis study aims to...</td>\n",
       "      <td>/pdf/58f0c36da4cbe7eba434f580fb26822b34c6757e.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/1911.12199</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173678</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=n1q-iz83S5&amp;not...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RS_013_MLRC_2022_13</td>\n",
       "      <td>[Re] Fairness Guarantees under Demographic Shift</td>\n",
       "      <td>['Valentin Leonhard Buchner', 'Philip Onno Oli...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Fairness and Bias in ML', 'Fair Machine Lear...</td>\n",
       "      <td>\\nScope of Reproducibility: The original autho...</td>\n",
       "      <td>/pdf/a154eb4c4168594c9fec045c44baecf70fac36c0.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://iclr.cc/virtual/2022/poster/6666</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173680</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=xEfg6h1GFmW&amp;no...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RS_014_MLRC_2022_14</td>\n",
       "      <td>[Re] DialSummEval - Evaluation of automatic su...</td>\n",
       "      <td>['Patrick Camara', 'Mojca Catharina Kloos', 'V...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Reproduction', 'Evaluation', 'Dialogue Summa...</td>\n",
       "      <td>Scope of Reproducibility â€” In this paper, we...</td>\n",
       "      <td>/pdf/a49585e9e327fcb0e6dcf55d03569a33782b5777.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://aclanthology.org/2022.naacl-main.418/</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173682</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=3jaZ5tKRyiT&amp;no...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RS_015_MLRC_2022_15</td>\n",
       "      <td>[Re] On the Reproducibility of â€œFairCal: Fai...</td>\n",
       "      <td>['Marga Don', 'Satchit Chatterji', 'Milena Kap...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['ReScience C', 'ReScience X', 'Machine Learni...</td>\n",
       "      <td>Scope of Reproducibility â€” This paper aims t...</td>\n",
       "      <td>/pdf/e37fde64b0642ea7024c23ed0835ce07292722ff.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openreview.net/pdf?id=nRj0NcmSuxb</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173686</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=uVHUy7CWCL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RS_016_MLRC_2022_16</td>\n",
       "      <td>Reproducibility Study: Label-Free Explainabili...</td>\n",
       "      <td>['SÅ‚awomir Garcarz', 'Andreas Giorkatzi', 'An...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Label-Free Explainability', 'Explainability'...</td>\n",
       "      <td>Reproducibility Summary\\n\\nScope of Reproducib...</td>\n",
       "      <td>/pdf/b9d086713ae4efccb1558881e34a3d88acfdb549.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.mlr.press/v162/crabbe22a/c...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173688</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=sF_vYZSxSV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RS_017_MLRC_2022_17</td>\n",
       "      <td>[Re] Numerical influence of ReLU'(0) on backpr...</td>\n",
       "      <td>['Tommaso Martorella', 'Hector Manuel Ramirez ...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'machine learning', 'deep lear...</td>\n",
       "      <td>Neural networks have become very common in mac...</td>\n",
       "      <td>/pdf/e8c787801a1d8bad0fb00b5952625b3115ba6fb3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.neurips.cc/paper/2021/file...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173692</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=YAWQTQZVoA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RS_018_MLRC_2022_18</td>\n",
       "      <td>A Reproducibility Case Study of â€œFairness Gu...</td>\n",
       "      <td>['Zjos van de Sande', 'Dennis Agafonov', 'Jelk...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['fairness', 'demographic shift', 'classificat...</td>\n",
       "      <td>Reproducibility Summary\\n\\nScope of Reproducib...</td>\n",
       "      <td>/pdf/6e8499b6188da557805216d83be1cd029ba00bd5.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openreview.net/forum?id=wbPObLm6ueA</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8206607</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=MMuv-v99Hy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RS_019_MLRC_2022_19</td>\n",
       "      <td>[Re] Hierarchical Shrinkage: Improving the Acc...</td>\n",
       "      <td>['Domen MohorÄiÄ', 'David Ocepek']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'machine learning', 'decision ...</td>\n",
       "      <td>Scope of Reproducibility\\nThe paper presents a...</td>\n",
       "      <td>/pdf/8e7ec4f4b707646fa2406a3a1b46e6361ec7caaf.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://icml.cc/virtual/2022/poster/16829</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173696</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=NgPQSqpz-Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RS_020_MLRC_2022_20</td>\n",
       "      <td>Reproducibility study of \"Joint Multisided Exp...</td>\n",
       "      <td>['Alessia Hu', 'Oline Ranum', 'Chrysoula Pozri...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Reproducibility', 'Information Retrieval', '...</td>\n",
       "      <td>Scope of Reproducibility \\nIn this work, we st...</td>\n",
       "      <td>/pdf/8634a0b0fe52eb23f0fa54633413fd4f9176c966.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2205.00048</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173698</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=A0Sjs3IJWb-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>RS_021_MLRC_2022_21</td>\n",
       "      <td>Exploring the Explainability of Bias in Image ...</td>\n",
       "      <td>['DaniÃ«l Van Dijk', 'Marten TÃ¼rk', 'L Busser...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['reproducibility', 'fairness', 'gender', 'rac...</td>\n",
       "      <td>Scope of Reproducibility â€” The main objectiv...</td>\n",
       "      <td>/pdf/57b829f2270fa6cc431997add0162fba06a9bc91.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2203.15395</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173703</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=N9Wn91tE7D0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RS_022_MLRC_2022_22</td>\n",
       "      <td>Reproducibility study of 'Proto2Proto: Can you...</td>\n",
       "      <td>['Gerson de Kleuver', 'David Matthias Bikker',...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Reproducibility', 'Interpretability', 'Proto...</td>\n",
       "      <td>Scope of Reproducibility â€” \\n\\nThis paper an...</td>\n",
       "      <td>/pdf/0cad70c6cd1e3d75ccf19794d4af2967be60ede5.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2204.11830</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173705</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=a_9YF58u61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RS_023_MLRC_2022_23</td>\n",
       "      <td>[Re] Reproducibility study of â€Focus On The ...</td>\n",
       "      <td>['Walter Simoncini', 'Ioanna Gogou', 'Marta Fr...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'machine learning', 'deep lear...</td>\n",
       "      <td>Scope of Reproducibility\\n\\nThis paper attempt...</td>\n",
       "      <td>/pdf/be89dd7facfe263fd65d756feb4b3e75b715b2f2.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openreview.net/forum?id=irARV_2VFs4</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173707</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=ye8PftiQLQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RS_024_MLRC_2022_24</td>\n",
       "      <td>Reproducibility Study of â€Label-Free Explain...</td>\n",
       "      <td>['Julius Wagenbach', 'Gergely Papp', 'Niklas M...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Explainable AI', 'Unsupervised Learning', 'F...</td>\n",
       "      <td>In this work, we present our reproducibility s...</td>\n",
       "      <td>/pdf/34fdaf29b0edaa22d1258444847ba34b4b9c9fe9.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2203.01928</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173711</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=n2qXFXiMsAM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RS_025_MLRC_2022_25</td>\n",
       "      <td>[Re] Reproducibility study of â€œExplaining De...</td>\n",
       "      <td>['Erik Buis', 'Sebastiaan Dijkstra', 'Bram Hei...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Reproduce', 'Interpretability', 'Convolution...</td>\n",
       "      <td>Scope of Reproducibility\\nIn this work, we aim...</td>\n",
       "      <td>/pdf/d2f5afd089bc7f1472bb80dc9e9933df836b4433.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ieeexplore.ieee.org/document/9878481</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173713</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=nsrHznwHhl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RS_026_MLRC_2022_26</td>\n",
       "      <td>Reproducibility Study of â€œQuantifying Societ...</td>\n",
       "      <td>['Farrukh Baratov', 'GÃ¶ksenin YÃ¼ksel', 'Dari...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['bias', 'gender bias', 'racial bias', 'age bi...</td>\n",
       "      <td>Scope of reproducibility - We study the reprod...</td>\n",
       "      <td>/pdf/764521708b35c1dab2f1507074f5b94b3ff70e06.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openaccess.thecvf.com/content/CVPR2022...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173715</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=eJmQJT0Dtt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RS_027_MLRC_2022_27</td>\n",
       "      <td>[Re] On the reproducibility of \"CrossWalk: Fai...</td>\n",
       "      <td>['Eric Zila', 'Jonathan Gerbscheid', 'Luc StrÃ...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'deepwalk', 'crosswalk', 'grap...</td>\n",
       "      <td>Scope of Reproducibility - The original author...</td>\n",
       "      <td>/pdf/8216b02ff557ce4ec1c022cb7a0707519c499e8d.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ojs.aaai.org/index.php/AAAI/article/vi...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173717</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=WnaVgRhlyT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RS_028_MLRC_2022_28</td>\n",
       "      <td>[Re] Reproducing FairCal: Fairness Calibration...</td>\n",
       "      <td>['Jip Greven', 'Simon Stallinga', 'Zirk Seljee']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['fairness', 'bias', 'face verification', 'fai...</td>\n",
       "      <td>Reproducibility Summary\\n===\\n\\nScope of Repro...</td>\n",
       "      <td>/pdf/cd9d92da523749654ff721317617b5150b8f929f.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2106.03761v4</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173719</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=jDBYRwDpeW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>RS_029_MLRC_2022_29</td>\n",
       "      <td>Reproducibility Study of â€CartoonX: Cartoon ...</td>\n",
       "      <td>['Aditya Prakash Patra', 'Sina Taslimi', 'Luke...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'machine learning', 'Explainab...</td>\n",
       "      <td>In this reproducibility study, we verify the c...</td>\n",
       "      <td>/pdf/c95ff07d5f89d48e72218d15f55784039360b4af.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.ecva.net/papers/eccv_2022/papers_E...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173721</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=DWKJpl8s06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RS_030_MLRC_2022_30</td>\n",
       "      <td>Reproducibility Study of â€™Latent Space Smoot...</td>\n",
       "      <td>['Didier Merk', 'Tsatsral Mendsuren', 'Denny S...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Latent space smoothing', 'LASSI', 'reproduci...</td>\n",
       "      <td>- Scope of Reproducibility\\nThe aim of this wo...</td>\n",
       "      <td>/pdf/297b0703140390d112c05d67e700b2dbed1e9278.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2111.13650v3</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173725</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=J-Lgb7Vc0wX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>RS_031_MLRC_2022_31</td>\n",
       "      <td>[Re] Variational Neural Cellular Automata</td>\n",
       "      <td>['Albert Sund Aillet', 'Simon SondÃ©n']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Neural Cellular Automata', 'Cellular Automat...</td>\n",
       "      <td>The main claim of the paper being reproduced i...</td>\n",
       "      <td>/pdf/adc8d78a91a593e12451cda54690da8942220f3f.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2201.12360</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173729</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=d7-ns6SZqp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>RS_032_MLRC_2022_32</td>\n",
       "      <td>[Re] If You Like Shapley Then Youâ€™ll Love th...</td>\n",
       "      <td>['Anes Benmerzoug', 'Miguel de Benito Delgado']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['data valuation', 'least core', 'shapley valu...</td>\n",
       "      <td>We investigate the results of [1] in the field...</td>\n",
       "      <td>/pdf/a998d0fe66a5e322bb8ff2d4d9f9a6773770aa4a.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ojs.aaai.org/index.php/AAAI/article/vi...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173733</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=vWzZQAahuW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RS_033_MLRC_2022_33</td>\n",
       "      <td>A Reproduction of Automatic Multi-Label Prompt...</td>\n",
       "      <td>['Victor Livernoche', 'Vidya Sujaya']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['prompt engineering', 'label engineering', 'm...</td>\n",
       "      <td>We reproduce the results in the paper Automati...</td>\n",
       "      <td>/pdf/b64f0e09258f2c039d39a1ee7db8a6aea63e4799.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://aclanthology.org/2022.naacl-main.401.pdf</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173735</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=t8ZZ2Y356Ix</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>RS_034_MLRC_2022_34</td>\n",
       "      <td>On the Reproducibility of â€œG-Mixup:Graph Dat...</td>\n",
       "      <td>['Dylan Cordaro', 'Shelby Cox', 'Yiman Ren', '...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Python', 'graphical neural networks', 'mixup...</td>\n",
       "      <td>We attempt to reproduce results on a novel gra...</td>\n",
       "      <td>/pdf/290dee2b4604ec276e9382fdb7e9d0ab378bdb23.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.mlr.press/v162/han22c.html</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173737</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=T54wy0ahGLG&amp;no...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>RS_035_MLRC_2022_35</td>\n",
       "      <td>[Re] Exploring the Role of Grammar and Word Ch...</td>\n",
       "      <td>['Priyanka Bose', 'Chandra Shekhar Pandey', 'F...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Machine Learning', 'Bias', 'BERT', 'Word2Vec...</td>\n",
       "      <td>Reproducibility Summary\\nScope of Reproducibil...</td>\n",
       "      <td>/pdf/b2c19856585f093a3b6f8da25e1cf6c5b87bb812.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3531146.353...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173739</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=MjZVx7a0KX-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>RS_036_MLRC_2022_36</td>\n",
       "      <td>RELIC: Reproducibility and Extension on LIC me...</td>\n",
       "      <td>['Martijn van Raaphorst', 'Egoitz Gonzales', '...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'rescience x', 'machine learni...</td>\n",
       "      <td>Scope of Reproducibility\\nIn this work we repr...</td>\n",
       "      <td>/pdf/7e16d4e632092a5ed4cf3092c119212d8fc0c138.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2203.15395</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173741</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=9_hCoP3LXwy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>RS_037_MLRC_2022_37</td>\n",
       "      <td>[Re] VAE Approximation Error: ELBO and Exponen...</td>\n",
       "      <td>['Volodymyr Kyrylov', 'Navdeep Singh Bedi', 'Q...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Python', 'VAE', 'approximation errors', 'ELB...</td>\n",
       "      <td>Scope of Reproducibility â€” Exponential famil...</td>\n",
       "      <td>/pdf/46c6b4f22344f8ae9d8cf06031c31771b5587457.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openreview.net/forum?id=OIs3SxU5Ynl</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173745</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=ozbAwipuZu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RS_038_MLRC_2022_38</td>\n",
       "      <td>Reproducibility study of the Fairness-enhanced...</td>\n",
       "      <td>['Gijs Joppe Moens', 'Job de Witte', 'Tobias P...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Crosswalk', 'Fairness', 'Graph Networks', 'G...</td>\n",
       "      <td>\"CrossWalk: Fairness-Enhanced Node Representat...</td>\n",
       "      <td>/pdf/194c8f8a2ee0dd7329a023d299003f45684a3cf6.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/pdf/2105.02725.pdf</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173747</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=KNp7Zq3KkT0&amp;no...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>RS_039_MLRC_2022_39</td>\n",
       "      <td>[Re] CrossWalk: Fairness-enhanced Node Represe...</td>\n",
       "      <td>['Luca Pantea', 'Andrei Eusebiu Blahovici']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Fairness in Machine Learning', 'Representati...</td>\n",
       "      <td>Scope of Reproducibility\\nThis work aims to re...</td>\n",
       "      <td>/pdf/eb0d993e25d9bf3503d18c37700bfa51744ee1c8.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2105.02725</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173749</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=tpk45Zll8eh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>RS_040_MLRC_2022_40</td>\n",
       "      <td>[Re] Masked Autoencoders Are Small Scale Visio...</td>\n",
       "      <td>['Athanasios Charisoudis', 'Simon Ekman von Hu...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'python', 'pytorch', 'machine ...</td>\n",
       "      <td>Scope of Reproducibility â€” The Masked Autoen...</td>\n",
       "      <td>/pdf/c9616492444c28600ec773aa1c6db9667f51da99.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openaccess.thecvf.com/content/CVPR2022...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173751</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=KXfjZPL5pqr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>RS_041_MLRC_2022_41</td>\n",
       "      <td>[Re] On Explainability of Graph Neural Network...</td>\n",
       "      <td>['Yannik Mahlau', 'Lukas Berg', 'Leo Kayser']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Rescience c', 'Rescience x', 'Explainable AI...</td>\n",
       "      <td>Yuan et al. claim their proposed method Subgra...</td>\n",
       "      <td>/pdf/cbf6c4b95a73a9b9d62e6fc49248af23160caae8.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.mlr.press/v139/yuan21c</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173753</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=zKBJw4Ht8s</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>RS_042_MLRC_2022_42</td>\n",
       "      <td>[Re] â€œTowards Understanding Grokkingâ€</td>\n",
       "      <td>['Alexander Shabalin', 'Ildus Sadrtdinov', 'Ev...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['grokking', 'neural networks', 'MNIST']</td>\n",
       "      <td>Scope of Reproducibility\\nIn this work, we att...</td>\n",
       "      <td>/pdf/9d5c9e36691ac9fe4b65941ca8908c0f8b3d7519.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://openreview.net/pdf?id=6at6rB3IZm</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173755</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=Vz9VLcJqKS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>RS_043_MLRC_2022_43</td>\n",
       "      <td>[Re] Reproducibility Study of Behavior Transfo...</td>\n",
       "      <td>['Skander Moalla', 'Manuel Madeira', 'Lorenzo ...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['rescience c', 'machine learning', 'reinforce...</td>\n",
       "      <td>Scope of Reproducibility - In this work, we an...</td>\n",
       "      <td>/pdf/184673425e85b10da526e3542760edb71d757856.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.neurips.cc/paper_files/pap...</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173757</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=E0qO5dI5aEn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>RS_044_MLRC_2022_44</td>\n",
       "      <td>A Replication Study of Compositional Generaliz...</td>\n",
       "      <td>['Kaiser Sun', 'Adina Williams', 'Dieuwke Hupk...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['Reproducibility', 'Compositional Generalizat...</td>\n",
       "      <td>Reproducibility Summary\\nScope of Reproducibil...</td>\n",
       "      <td>/pdf/35fb3e86fa46c3cd82126cb00e4b161353283cac.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://aclanthology.org/2021.acl-long.75/</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173759</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=MF9uv95psps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>RS_045_MLRC_2022_45</td>\n",
       "      <td>[Re] Pure Noise to the Rescue of Insufficient ...</td>\n",
       "      <td>['Seungjae Ryan Lee', 'Seungmin Lee']</td>\n",
       "      <td>Replication</td>\n",
       "      <td>['data augmentation', 'computer vision']</td>\n",
       "      <td>Scope of Reproducibility â€” We examine the ma...</td>\n",
       "      <td>/pdf/e83b2fc88fcc0e7d6ce8fc8a73b1877007b716dc.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://proceedings.mlr.press/v162/zada22a</td>\n",
       "      <td>https://www.doi.org/10.5281/zenodo.8173763</td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://openreview.net/forum?id=ErBe4MnsVD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>RS_046_MLRC_2021_01</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>Sinha, Koustuv and Dodge, Jesse and Luccioni, ...</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>rescience c, machine learning, deep learning, ...</td>\n",
       "      <td>DO EXTRACT FROM review_url</td>\n",
       "      <td>https://zenodo.org/record/6574723/files/articl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Editorial - Not Applicable</td>\n",
       "      <td>10.5281/zenodo.6574723</td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ReScience C</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>RS_047_MLRC_2021_02</td>\n",
       "      <td>[Re] Counterfactual Generative Networks</td>\n",
       "      <td>Ankit, Ankit and Ambekar, Sameer and Varadhara...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>rescience c, machine learning, deep learning, ...</td>\n",
       "      <td>DO EXTRACT FROM review_url</td>\n",
       "      <td>https://zenodo.org/record/6574625/files/articl...</td>\n",
       "      <td>https://github.com/ambekarsameer96/FACT_AI/</td>\n",
       "      <td>https://arxiv.org/abs/2101.06046</td>\n",
       "      <td>10.5281/zenodo.6574625</td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>https://openreview.net/forum?id=BSHg22G7n0F</td>\n",
       "      <td>ReScience C</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>RS_048_MLRC_2021_03</td>\n",
       "      <td>[Re] Does Self-Supervision Always Improve Few-...</td>\n",
       "      <td>Ashok, Arjun and Aekula, Haswanth</td>\n",
       "      <td>Replication</td>\n",
       "      <td>rescience c, machine learning, deep learning, ...</td>\n",
       "      <td>DO EXTRACT FROM review_url</td>\n",
       "      <td>https://zenodo.org/record/6574629/files/articl...</td>\n",
       "      <td>https://github.com/ashok-arjun/MLRC-2021-Few-S...</td>\n",
       "      <td>https://openreview.net/forum?id=738x44N7yUE&amp;no...</td>\n",
       "      <td>10.5281/zenodo.6574629</td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>https://openreview.net/forum?id=ScfP3G73CY</td>\n",
       "      <td>ReScience C</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>RS_049_MLRC_2021_04</td>\n",
       "      <td>[Re] Weakly-Supervised Semantic Segmentation v...</td>\n",
       "      <td>Athanasiadis, Ioannis and Moschovis, Georgios ...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>rescience c, machine learning, deep learning, ...</td>\n",
       "      <td>DO EXTRACT FROM review_url</td>\n",
       "      <td>https://zenodo.org/record/6574631/files/articl...</td>\n",
       "      <td>https://github.com/athaioan/ViT_Affinity_Repro...</td>\n",
       "      <td>https://openaccess.thecvf.com/content/CVPR2021...</td>\n",
       "      <td>10.5281/zenodo.6574631</td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>https://openreview.net/forum?id=rcEDhGX3AY</td>\n",
       "      <td>ReScience C</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>RS_050_MLRC_2021_05</td>\n",
       "      <td>[Re] Reproducibility Study of â€œCounterfactua...</td>\n",
       "      <td>Bagad, Piyush and Hilders, Paul and Maas, Jess...</td>\n",
       "      <td>Replication</td>\n",
       "      <td>rescience c, machine learning, python, pytorch...</td>\n",
       "      <td>DO EXTRACT FROM review_url</td>\n",
       "      <td>https://zenodo.org/record/6574635/files/articl...</td>\n",
       "      <td>https://github.com/danilodegoede/fact-team3/</td>\n",
       "      <td>https://openreview.net/pdf?id=BXewfAYMmJw</td>\n",
       "      <td>10.5281/zenodo.6574635</td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>https://openreview.net/forum?id=HNlzT3G720t</td>\n",
       "      <td>ReScience C</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>ML Reproducibility Challenge 2021</td>\n",
       "      <td>article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         key_for_all_RS                                           title_rs  \\\n",
       "0   RS_001_MLRC_2022_01  [Re] $\\mathcal{G}$-Mixup: Graph Data Augmentat...   \n",
       "1   RS_002_MLRC_2022_02  [Re] Exact Feature Distribution Matching for A...   \n",
       "2   RS_003_MLRC_2022_03  [Re] End-to-end Algorithm Synthesis with Recur...   \n",
       "3   RS_004_MLRC_2022_04  [Re] Label-Free Explainability for Unsupervise...   \n",
       "4   RS_005_MLRC_2022_05  [Re] Exploring the Representation of Word Mean...   \n",
       "5   RS_006_MLRC_2022_06   [Re] Intriguing Properties of Contrastive Losses   \n",
       "6   RS_007_MLRC_2022_07  [Re] Bandit Theory and Thompson Sampling-guide...   \n",
       "7   RS_008_MLRC_2022_08  [Re] Hypergraph-Induced Semantic Tuplet Loss f...   \n",
       "8   RS_009_MLRC_2022_09  Easy Bayesian Transfer Learning with Informati...   \n",
       "9   RS_010_MLRC_2022_10            [Re] On the Reproducibility of CartoonX   \n",
       "10  RS_011_MLRC_2022_11  Reproducibility Study of \"Label-Free Explainab...   \n",
       "11  RS_012_MLRC_2022_12  [Re] FOCUS: Flexible Optimizable Counterfactua...   \n",
       "12  RS_013_MLRC_2022_13   [Re] Fairness Guarantees under Demographic Shift   \n",
       "13  RS_014_MLRC_2022_14  [Re] DialSummEval - Evaluation of automatic su...   \n",
       "14  RS_015_MLRC_2022_15  [Re] On the Reproducibility of â€œFairCal: Fai...   \n",
       "15  RS_016_MLRC_2022_16  Reproducibility Study: Label-Free Explainabili...   \n",
       "16  RS_017_MLRC_2022_17  [Re] Numerical influence of ReLU'(0) on backpr...   \n",
       "17  RS_018_MLRC_2022_18  A Reproducibility Case Study of â€œFairness Gu...   \n",
       "18  RS_019_MLRC_2022_19  [Re] Hierarchical Shrinkage: Improving the Acc...   \n",
       "19  RS_020_MLRC_2022_20  Reproducibility study of \"Joint Multisided Exp...   \n",
       "20  RS_021_MLRC_2022_21  Exploring the Explainability of Bias in Image ...   \n",
       "21  RS_022_MLRC_2022_22  Reproducibility study of 'Proto2Proto: Can you...   \n",
       "22  RS_023_MLRC_2022_23  [Re] Reproducibility study of â€Focus On The ...   \n",
       "23  RS_024_MLRC_2022_24  Reproducibility Study of â€Label-Free Explain...   \n",
       "24  RS_025_MLRC_2022_25  [Re] Reproducibility study of â€œExplaining De...   \n",
       "25  RS_026_MLRC_2022_26  Reproducibility Study of â€œQuantifying Societ...   \n",
       "26  RS_027_MLRC_2022_27  [Re] On the reproducibility of \"CrossWalk: Fai...   \n",
       "27  RS_028_MLRC_2022_28  [Re] Reproducing FairCal: Fairness Calibration...   \n",
       "28  RS_029_MLRC_2022_29  Reproducibility Study of â€CartoonX: Cartoon ...   \n",
       "29  RS_030_MLRC_2022_30  Reproducibility Study of â€™Latent Space Smoot...   \n",
       "30  RS_031_MLRC_2022_31          [Re] Variational Neural Cellular Automata   \n",
       "31  RS_032_MLRC_2022_32  [Re] If You Like Shapley Then Youâ€™ll Love th...   \n",
       "32  RS_033_MLRC_2022_33  A Reproduction of Automatic Multi-Label Prompt...   \n",
       "33  RS_034_MLRC_2022_34  On the Reproducibility of â€œG-Mixup:Graph Dat...   \n",
       "34  RS_035_MLRC_2022_35  [Re] Exploring the Role of Grammar and Word Ch...   \n",
       "35  RS_036_MLRC_2022_36  RELIC: Reproducibility and Extension on LIC me...   \n",
       "36  RS_037_MLRC_2022_37  [Re] VAE Approximation Error: ELBO and Exponen...   \n",
       "37  RS_038_MLRC_2022_38  Reproducibility study of the Fairness-enhanced...   \n",
       "38  RS_039_MLRC_2022_39  [Re] CrossWalk: Fairness-enhanced Node Represe...   \n",
       "39  RS_040_MLRC_2022_40  [Re] Masked Autoencoders Are Small Scale Visio...   \n",
       "40  RS_041_MLRC_2022_41  [Re] On Explainability of Graph Neural Network...   \n",
       "41  RS_042_MLRC_2022_42          [Re] â€œTowards Understanding Grokkingâ€   \n",
       "42  RS_043_MLRC_2022_43  [Re] Reproducibility Study of Behavior Transfo...   \n",
       "43  RS_044_MLRC_2022_44  A Replication Study of Compositional Generaliz...   \n",
       "44  RS_045_MLRC_2022_45  [Re] Pure Noise to the Rescue of Insufficient ...   \n",
       "45  RS_046_MLRC_2021_01                  ML Reproducibility Challenge 2021   \n",
       "46  RS_047_MLRC_2021_02            [Re] Counterfactual Generative Networks   \n",
       "47  RS_048_MLRC_2021_03  [Re] Does Self-Supervision Always Improve Few-...   \n",
       "48  RS_049_MLRC_2021_04  [Re] Weakly-Supervised Semantic Segmentation v...   \n",
       "49  RS_050_MLRC_2021_05  [Re] Reproducibility Study of â€œCounterfactua...   \n",
       "\n",
       "                                           authors_rs article_type  \\\n",
       "0               ['Ermin Omeragic', 'Vuk ÄuranoviÄ‡']  Replication   \n",
       "1   ['Mert Erkol', 'Furkan KÄ±nlÄ±', 'BarÄ±ÅŸ Ã–zc...  Replication   \n",
       "2         ['Sean Michael McLeish', 'Long Tran-Thanh']  Replication   \n",
       "3   ['Eric Langezaal', 'Jesse Belleman', 'Joeri No...  Replication   \n",
       "4                 ['Matteo Brivio', 'Cagri Coltekin']  Replication   \n",
       "5   ['Luca Marini', 'Mohamad Nabeel', 'Alexandre L...  Replication   \n",
       "6                                    ['Luka Å½ontar']  Replication   \n",
       "7                   ['Jicheng Yuan', 'Danh Le Phuoc']  Replication   \n",
       "8               ['Martin Å pendl', 'Klementina Pirc']  Replication   \n",
       "9   ['Robin Sasse', 'Aniek Eijpe', 'Jona Ruthardt'...  Replication   \n",
       "10  ['Valentinos Pariza', 'Avik Pal', 'Madhura Paw...  Replication   \n",
       "11                                 ['Kyosuke Morita']  Replication   \n",
       "12  ['Valentin Leonhard Buchner', 'Philip Onno Oli...  Replication   \n",
       "13  ['Patrick Camara', 'Mojca Catharina Kloos', 'V...  Replication   \n",
       "14  ['Marga Don', 'Satchit Chatterji', 'Milena Kap...  Replication   \n",
       "15  ['SÅ‚awomir Garcarz', 'Andreas Giorkatzi', 'An...  Replication   \n",
       "16  ['Tommaso Martorella', 'Hector Manuel Ramirez ...  Replication   \n",
       "17  ['Zjos van de Sande', 'Dennis Agafonov', 'Jelk...  Replication   \n",
       "18               ['Domen MohorÄiÄ', 'David Ocepek']  Replication   \n",
       "19  ['Alessia Hu', 'Oline Ranum', 'Chrysoula Pozri...  Replication   \n",
       "20  ['DaniÃ«l Van Dijk', 'Marten TÃ¼rk', 'L Busser...  Replication   \n",
       "21  ['Gerson de Kleuver', 'David Matthias Bikker',...  Replication   \n",
       "22  ['Walter Simoncini', 'Ioanna Gogou', 'Marta Fr...  Replication   \n",
       "23  ['Julius Wagenbach', 'Gergely Papp', 'Niklas M...  Replication   \n",
       "24  ['Erik Buis', 'Sebastiaan Dijkstra', 'Bram Hei...  Replication   \n",
       "25  ['Farrukh Baratov', 'GÃ¶ksenin YÃ¼ksel', 'Dari...  Replication   \n",
       "26  ['Eric Zila', 'Jonathan Gerbscheid', 'Luc StrÃ...  Replication   \n",
       "27   ['Jip Greven', 'Simon Stallinga', 'Zirk Seljee']  Replication   \n",
       "28  ['Aditya Prakash Patra', 'Sina Taslimi', 'Luke...  Replication   \n",
       "29  ['Didier Merk', 'Tsatsral Mendsuren', 'Denny S...  Replication   \n",
       "30            ['Albert Sund Aillet', 'Simon SondÃ©n']  Replication   \n",
       "31    ['Anes Benmerzoug', 'Miguel de Benito Delgado']  Replication   \n",
       "32              ['Victor Livernoche', 'Vidya Sujaya']  Replication   \n",
       "33  ['Dylan Cordaro', 'Shelby Cox', 'Yiman Ren', '...  Replication   \n",
       "34  ['Priyanka Bose', 'Chandra Shekhar Pandey', 'F...  Replication   \n",
       "35  ['Martijn van Raaphorst', 'Egoitz Gonzales', '...  Replication   \n",
       "36  ['Volodymyr Kyrylov', 'Navdeep Singh Bedi', 'Q...  Replication   \n",
       "37  ['Gijs Joppe Moens', 'Job de Witte', 'Tobias P...  Replication   \n",
       "38        ['Luca Pantea', 'Andrei Eusebiu Blahovici']  Replication   \n",
       "39  ['Athanasios Charisoudis', 'Simon Ekman von Hu...  Replication   \n",
       "40      ['Yannik Mahlau', 'Lukas Berg', 'Leo Kayser']  Replication   \n",
       "41  ['Alexander Shabalin', 'Ildus Sadrtdinov', 'Ev...  Replication   \n",
       "42  ['Skander Moalla', 'Manuel Madeira', 'Lorenzo ...  Replication   \n",
       "43  ['Kaiser Sun', 'Adina Williams', 'Dieuwke Hupk...  Replication   \n",
       "44              ['Seungjae Ryan Lee', 'Seungmin Lee']  Replication   \n",
       "45  Sinha, Koustuv and Dodge, Jesse and Luccioni, ...    Editorial   \n",
       "46  Ankit, Ankit and Ambekar, Sameer and Varadhara...  Replication   \n",
       "47                  Ashok, Arjun and Aekula, Haswanth  Replication   \n",
       "48  Athanasiadis, Ioannis and Moschovis, Georgios ...  Replication   \n",
       "49  Bagad, Piyush and Hilders, Paul and Maas, Jess...  Replication   \n",
       "\n",
       "                                             keywords  \\\n",
       "0   ['rescience c', 'machine learning', 'data augm...   \n",
       "1   ['feature distribution matching', 'style trans...   \n",
       "2   ['rescience c', 'Algorithmic Reasoning', 'Deep...   \n",
       "3   ['Reproducibility', 'label-free', 'unsupervise...   \n",
       "4   ['BERT', 'fastText', 'embeddings', 'language m...   \n",
       "5   ['self-supervised learning', 'contrastive lear...   \n",
       "6   ['Optimization', 'bandit learning', 'Thompson ...   \n",
       "7   ['Hypergraph', 'Deep Metric Learning', 'Semant...   \n",
       "8   ['transfer learning', 'Bayesian inference', 'i...   \n",
       "9   ['Machine Learning', 'Artificial Intelligence'...   \n",
       "10  ['Reproducibility', 'Feature Importance', 'Exa...   \n",
       "11                     ['Counterfactual explanation']   \n",
       "12  ['Fairness and Bias in ML', 'Fair Machine Lear...   \n",
       "13  ['Reproduction', 'Evaluation', 'Dialogue Summa...   \n",
       "14  ['ReScience C', 'ReScience X', 'Machine Learni...   \n",
       "15  ['Label-Free Explainability', 'Explainability'...   \n",
       "16  ['rescience c', 'machine learning', 'deep lear...   \n",
       "17  ['fairness', 'demographic shift', 'classificat...   \n",
       "18  ['rescience c', 'machine learning', 'decision ...   \n",
       "19  ['Reproducibility', 'Information Retrieval', '...   \n",
       "20  ['reproducibility', 'fairness', 'gender', 'rac...   \n",
       "21  ['Reproducibility', 'Interpretability', 'Proto...   \n",
       "22  ['rescience c', 'machine learning', 'deep lear...   \n",
       "23  ['Explainable AI', 'Unsupervised Learning', 'F...   \n",
       "24  ['Reproduce', 'Interpretability', 'Convolution...   \n",
       "25  ['bias', 'gender bias', 'racial bias', 'age bi...   \n",
       "26  ['rescience c', 'deepwalk', 'crosswalk', 'grap...   \n",
       "27  ['fairness', 'bias', 'face verification', 'fai...   \n",
       "28  ['rescience c', 'machine learning', 'Explainab...   \n",
       "29  ['Latent space smoothing', 'LASSI', 'reproduci...   \n",
       "30  ['Neural Cellular Automata', 'Cellular Automat...   \n",
       "31  ['data valuation', 'least core', 'shapley valu...   \n",
       "32  ['prompt engineering', 'label engineering', 'm...   \n",
       "33  ['Python', 'graphical neural networks', 'mixup...   \n",
       "34  ['Machine Learning', 'Bias', 'BERT', 'Word2Vec...   \n",
       "35  ['rescience c', 'rescience x', 'machine learni...   \n",
       "36  ['Python', 'VAE', 'approximation errors', 'ELB...   \n",
       "37  ['Crosswalk', 'Fairness', 'Graph Networks', 'G...   \n",
       "38  ['Fairness in Machine Learning', 'Representati...   \n",
       "39  ['rescience c', 'python', 'pytorch', 'machine ...   \n",
       "40  ['Rescience c', 'Rescience x', 'Explainable AI...   \n",
       "41           ['grokking', 'neural networks', 'MNIST']   \n",
       "42  ['rescience c', 'machine learning', 'reinforce...   \n",
       "43  ['Reproducibility', 'Compositional Generalizat...   \n",
       "44           ['data augmentation', 'computer vision']   \n",
       "45  rescience c, machine learning, deep learning, ...   \n",
       "46  rescience c, machine learning, deep learning, ...   \n",
       "47  rescience c, machine learning, deep learning, ...   \n",
       "48  rescience c, machine learning, deep learning, ...   \n",
       "49  rescience c, machine learning, python, pytorch...   \n",
       "\n",
       "                                             abstract  \\\n",
       "0   Scope of Reproducibility\\nThis paper presents ...   \n",
       "1   Reproducibility Summary:\\n\\nIn this reproducib...   \n",
       "2   Scope of Reproducibility:\\nIn this report, we ...   \n",
       "3   Scope of Reproducibility â€” This study is an ...   \n",
       "4   This report summarizes our efforts to reproduc...   \n",
       "5   Reproducibility Summary\\n\\nScope of Reproducib...   \n",
       "6   The paper presents a novel DE approach using T...   \n",
       "7   Reproducibility Summary\\n\\nScope of Reproducib...   \n",
       "8   REPRODUCIBILITY SUMMARY\\n\\nScope of Reproducib...   \n",
       "9   Scope of Reproducibility â€” CartoonX [1] is a...   \n",
       "10  Scope of Reproducibility\\nIn this work, we eva...   \n",
       "11  Scope of Reproducibility\\n\\nThis study aims to...   \n",
       "12  \\nScope of Reproducibility: The original autho...   \n",
       "13  Scope of Reproducibility â€” In this paper, we...   \n",
       "14  Scope of Reproducibility â€” This paper aims t...   \n",
       "15  Reproducibility Summary\\n\\nScope of Reproducib...   \n",
       "16  Neural networks have become very common in mac...   \n",
       "17  Reproducibility Summary\\n\\nScope of Reproducib...   \n",
       "18  Scope of Reproducibility\\nThe paper presents a...   \n",
       "19  Scope of Reproducibility \\nIn this work, we st...   \n",
       "20  Scope of Reproducibility â€” The main objectiv...   \n",
       "21  Scope of Reproducibility â€” \\n\\nThis paper an...   \n",
       "22  Scope of Reproducibility\\n\\nThis paper attempt...   \n",
       "23  In this work, we present our reproducibility s...   \n",
       "24  Scope of Reproducibility\\nIn this work, we aim...   \n",
       "25  Scope of reproducibility - We study the reprod...   \n",
       "26  Scope of Reproducibility - The original author...   \n",
       "27  Reproducibility Summary\\n===\\n\\nScope of Repro...   \n",
       "28  In this reproducibility study, we verify the c...   \n",
       "29  - Scope of Reproducibility\\nThe aim of this wo...   \n",
       "30  The main claim of the paper being reproduced i...   \n",
       "31  We investigate the results of [1] in the field...   \n",
       "32  We reproduce the results in the paper Automati...   \n",
       "33  We attempt to reproduce results on a novel gra...   \n",
       "34  Reproducibility Summary\\nScope of Reproducibil...   \n",
       "35  Scope of Reproducibility\\nIn this work we repr...   \n",
       "36  Scope of Reproducibility â€” Exponential famil...   \n",
       "37  \"CrossWalk: Fairness-Enhanced Node Representat...   \n",
       "38  Scope of Reproducibility\\nThis work aims to re...   \n",
       "39  Scope of Reproducibility â€” The Masked Autoen...   \n",
       "40  Yuan et al. claim their proposed method Subgra...   \n",
       "41  Scope of Reproducibility\\nIn this work, we att...   \n",
       "42  Scope of Reproducibility - In this work, we an...   \n",
       "43  Reproducibility Summary\\nScope of Reproducibil...   \n",
       "44  Scope of Reproducibility â€” We examine the ma...   \n",
       "45                         DO EXTRACT FROM review_url   \n",
       "46                         DO EXTRACT FROM review_url   \n",
       "47                         DO EXTRACT FROM review_url   \n",
       "48                         DO EXTRACT FROM review_url   \n",
       "49                         DO EXTRACT FROM review_url   \n",
       "\n",
       "                                           pdf_url_rs  \\\n",
       "0   /pdf/8c87cb1e84e1482826c40a3b0c43928eaef747f3.pdf   \n",
       "1   /pdf/7bcb577c2a46db29c48234a5b72368053c7ebed3.pdf   \n",
       "2   /pdf/07d5d68b5873d779bd1fd8c95b9767cb57fe0bc4.pdf   \n",
       "3   /pdf/7fde4f12c675150699fc376cd097d5b9dad0b4d4.pdf   \n",
       "4   /pdf/78430c5af33bc892d852b49a5a6e93abeb314c6f.pdf   \n",
       "5   /pdf/fc591d20ed7c340bbe470789beb95d4cde81e7d5.pdf   \n",
       "6   /pdf/cf9062c0ba6651cb416547df2c1e7e7379d572a9.pdf   \n",
       "7   /pdf/9a228c78b9d481df388578a8e5d78a90412b9891.pdf   \n",
       "8   /pdf/4247c7fbacadbf0bc1d7e5fc08e2c4ce35315657.pdf   \n",
       "9   /pdf/6f7b408230bc01c524943393638a23d4a8f73cc6.pdf   \n",
       "10  /pdf/b037311c182b52dd01853262e9d4fb09103cddae.pdf   \n",
       "11  /pdf/58f0c36da4cbe7eba434f580fb26822b34c6757e.pdf   \n",
       "12  /pdf/a154eb4c4168594c9fec045c44baecf70fac36c0.pdf   \n",
       "13  /pdf/a49585e9e327fcb0e6dcf55d03569a33782b5777.pdf   \n",
       "14  /pdf/e37fde64b0642ea7024c23ed0835ce07292722ff.pdf   \n",
       "15  /pdf/b9d086713ae4efccb1558881e34a3d88acfdb549.pdf   \n",
       "16  /pdf/e8c787801a1d8bad0fb00b5952625b3115ba6fb3.pdf   \n",
       "17  /pdf/6e8499b6188da557805216d83be1cd029ba00bd5.pdf   \n",
       "18  /pdf/8e7ec4f4b707646fa2406a3a1b46e6361ec7caaf.pdf   \n",
       "19  /pdf/8634a0b0fe52eb23f0fa54633413fd4f9176c966.pdf   \n",
       "20  /pdf/57b829f2270fa6cc431997add0162fba06a9bc91.pdf   \n",
       "21  /pdf/0cad70c6cd1e3d75ccf19794d4af2967be60ede5.pdf   \n",
       "22  /pdf/be89dd7facfe263fd65d756feb4b3e75b715b2f2.pdf   \n",
       "23  /pdf/34fdaf29b0edaa22d1258444847ba34b4b9c9fe9.pdf   \n",
       "24  /pdf/d2f5afd089bc7f1472bb80dc9e9933df836b4433.pdf   \n",
       "25  /pdf/764521708b35c1dab2f1507074f5b94b3ff70e06.pdf   \n",
       "26  /pdf/8216b02ff557ce4ec1c022cb7a0707519c499e8d.pdf   \n",
       "27  /pdf/cd9d92da523749654ff721317617b5150b8f929f.pdf   \n",
       "28  /pdf/c95ff07d5f89d48e72218d15f55784039360b4af.pdf   \n",
       "29  /pdf/297b0703140390d112c05d67e700b2dbed1e9278.pdf   \n",
       "30  /pdf/adc8d78a91a593e12451cda54690da8942220f3f.pdf   \n",
       "31  /pdf/a998d0fe66a5e322bb8ff2d4d9f9a6773770aa4a.pdf   \n",
       "32  /pdf/b64f0e09258f2c039d39a1ee7db8a6aea63e4799.pdf   \n",
       "33  /pdf/290dee2b4604ec276e9382fdb7e9d0ab378bdb23.pdf   \n",
       "34  /pdf/b2c19856585f093a3b6f8da25e1cf6c5b87bb812.pdf   \n",
       "35  /pdf/7e16d4e632092a5ed4cf3092c119212d8fc0c138.pdf   \n",
       "36  /pdf/46c6b4f22344f8ae9d8cf06031c31771b5587457.pdf   \n",
       "37  /pdf/194c8f8a2ee0dd7329a023d299003f45684a3cf6.pdf   \n",
       "38  /pdf/eb0d993e25d9bf3503d18c37700bfa51744ee1c8.pdf   \n",
       "39  /pdf/c9616492444c28600ec773aa1c6db9667f51da99.pdf   \n",
       "40  /pdf/cbf6c4b95a73a9b9d62e6fc49248af23160caae8.pdf   \n",
       "41  /pdf/9d5c9e36691ac9fe4b65941ca8908c0f8b3d7519.pdf   \n",
       "42  /pdf/184673425e85b10da526e3542760edb71d757856.pdf   \n",
       "43  /pdf/35fb3e86fa46c3cd82126cb00e4b161353283cac.pdf   \n",
       "44  /pdf/e83b2fc88fcc0e7d6ce8fc8a73b1877007b716dc.pdf   \n",
       "45  https://zenodo.org/record/6574723/files/articl...   \n",
       "46  https://zenodo.org/record/6574625/files/articl...   \n",
       "47  https://zenodo.org/record/6574629/files/articl...   \n",
       "48  https://zenodo.org/record/6574631/files/articl...   \n",
       "49  https://zenodo.org/record/6574635/files/articl...   \n",
       "\n",
       "                                          code_url_rs  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "18                                                NaN   \n",
       "19                                                NaN   \n",
       "20                                                NaN   \n",
       "21                                                NaN   \n",
       "22                                                NaN   \n",
       "23                                                NaN   \n",
       "24                                                NaN   \n",
       "25                                                NaN   \n",
       "26                                                NaN   \n",
       "27                                                NaN   \n",
       "28                                                NaN   \n",
       "29                                                NaN   \n",
       "30                                                NaN   \n",
       "31                                                NaN   \n",
       "32                                                NaN   \n",
       "33                                                NaN   \n",
       "34                                                NaN   \n",
       "35                                                NaN   \n",
       "36                                                NaN   \n",
       "37                                                NaN   \n",
       "38                                                NaN   \n",
       "39                                                NaN   \n",
       "40                                                NaN   \n",
       "41                                                NaN   \n",
       "42                                                NaN   \n",
       "43                                                NaN   \n",
       "44                                                NaN   \n",
       "45                                                NaN   \n",
       "46        https://github.com/ambekarsameer96/FACT_AI/   \n",
       "47  https://github.com/ashok-arjun/MLRC-2021-Few-S...   \n",
       "48  https://github.com/athaioan/ViT_Affinity_Repro...   \n",
       "49       https://github.com/danilodegoede/fact-team3/   \n",
       "\n",
       "                                        org_paper_url  \\\n",
       "0      https://proceedings.mlr.press/v162/han22c.html   \n",
       "1   https://openaccess.thecvf.com/content/CVPR2022...   \n",
       "2                https://arxiv.org/pdf/2202.05826.pdf   \n",
       "3   https://proceedings.mlr.press/v162/crabbe22a.html   \n",
       "4         https://aclanthology.org/2021.acl-long.281/   \n",
       "5   https://proceedings.neurips.cc/paper/2021/hash...   \n",
       "6   https://neurips.cc/Conferences/2022/ScheduleMu...   \n",
       "7   https://openaccess.thecvf.com/content/CVPR2022...   \n",
       "8          https://openreview.net/forum?id=ao30zaT3YL   \n",
       "9        https://doi.org/10.1007/978-3-031-19775-8_26   \n",
       "10  https://proceedings.mlr.press/v162/crabbe22a/c...   \n",
       "11                   https://arxiv.org/abs/1911.12199   \n",
       "12           https://iclr.cc/virtual/2022/poster/6666   \n",
       "13      https://aclanthology.org/2022.naacl-main.418/   \n",
       "14          https://openreview.net/pdf?id=nRj0NcmSuxb   \n",
       "15  https://proceedings.mlr.press/v162/crabbe22a/c...   \n",
       "16  https://proceedings.neurips.cc/paper/2021/file...   \n",
       "17        https://openreview.net/forum?id=wbPObLm6ueA   \n",
       "18          https://icml.cc/virtual/2022/poster/16829   \n",
       "19                   https://arxiv.org/abs/2205.00048   \n",
       "20                   https://arxiv.org/abs/2203.15395   \n",
       "21                   https://arxiv.org/abs/2204.11830   \n",
       "22        https://openreview.net/forum?id=irARV_2VFs4   \n",
       "23                   https://arxiv.org/abs/2203.01928   \n",
       "24       https://ieeexplore.ieee.org/document/9878481   \n",
       "25  https://openaccess.thecvf.com/content/CVPR2022...   \n",
       "26  https://ojs.aaai.org/index.php/AAAI/article/vi...   \n",
       "27                 https://arxiv.org/abs/2106.03761v4   \n",
       "28  https://www.ecva.net/papers/eccv_2022/papers_E...   \n",
       "29                 https://arxiv.org/abs/2111.13650v3   \n",
       "30                   https://arxiv.org/abs/2201.12360   \n",
       "31  https://ojs.aaai.org/index.php/AAAI/article/vi...   \n",
       "32   https://aclanthology.org/2022.naacl-main.401.pdf   \n",
       "33     https://proceedings.mlr.press/v162/han22c.html   \n",
       "34  https://dl.acm.org/doi/pdf/10.1145/3531146.353...   \n",
       "35                   https://arxiv.org/abs/2203.15395   \n",
       "36        https://openreview.net/forum?id=OIs3SxU5Ynl   \n",
       "37               https://arxiv.org/pdf/2105.02725.pdf   \n",
       "38                   https://arxiv.org/abs/2105.02725   \n",
       "39  https://openaccess.thecvf.com/content/CVPR2022...   \n",
       "40         https://proceedings.mlr.press/v139/yuan21c   \n",
       "41           https://openreview.net/pdf?id=6at6rB3IZm   \n",
       "42  https://proceedings.neurips.cc/paper_files/pap...   \n",
       "43         https://aclanthology.org/2021.acl-long.75/   \n",
       "44         https://proceedings.mlr.press/v162/zada22a   \n",
       "45                         Editorial - Not Applicable   \n",
       "46                   https://arxiv.org/abs/2101.06046   \n",
       "47  https://openreview.net/forum?id=738x44N7yUE&no...   \n",
       "48  https://openaccess.thecvf.com/content/CVPR2021...   \n",
       "49          https://openreview.net/pdf?id=BXewfAYMmJw   \n",
       "\n",
       "                                        doi_rs  ... challenge_year_rs  \\\n",
       "0   https://www.doi.org/10.5281/zenodo.8173650  ...              2022   \n",
       "1   https://www.doi.org/10.5281/zenodo.8173652  ...              2022   \n",
       "2   https://www.doi.org/10.5281/zenodo.8173654  ...              2022   \n",
       "3   https://www.doi.org/10.5281/zenodo.8173656  ...              2022   \n",
       "4   https://www.doi.org/10.5281/zenodo.8173658  ...              2022   \n",
       "5   https://www.doi.org/10.5281/zenodo.8173662  ...              2022   \n",
       "6   https://www.doi.org/10.5281/zenodo.8173664  ...              2022   \n",
       "7   https://www.doi.org/10.5281/zenodo.8173666  ...              2022   \n",
       "8   https://www.doi.org/10.5281/zenodo.8173668  ...              2022   \n",
       "9   https://www.doi.org/10.5281/zenodo.8173672  ...              2022   \n",
       "10  https://www.doi.org/10.5281/zenodo.8173674  ...              2022   \n",
       "11  https://www.doi.org/10.5281/zenodo.8173678  ...              2022   \n",
       "12  https://www.doi.org/10.5281/zenodo.8173680  ...              2022   \n",
       "13  https://www.doi.org/10.5281/zenodo.8173682  ...              2022   \n",
       "14  https://www.doi.org/10.5281/zenodo.8173686  ...              2022   \n",
       "15  https://www.doi.org/10.5281/zenodo.8173688  ...              2022   \n",
       "16  https://www.doi.org/10.5281/zenodo.8173692  ...              2022   \n",
       "17  https://www.doi.org/10.5281/zenodo.8206607  ...              2022   \n",
       "18  https://www.doi.org/10.5281/zenodo.8173696  ...              2022   \n",
       "19  https://www.doi.org/10.5281/zenodo.8173698  ...              2022   \n",
       "20  https://www.doi.org/10.5281/zenodo.8173703  ...              2022   \n",
       "21  https://www.doi.org/10.5281/zenodo.8173705  ...              2022   \n",
       "22  https://www.doi.org/10.5281/zenodo.8173707  ...              2022   \n",
       "23  https://www.doi.org/10.5281/zenodo.8173711  ...              2022   \n",
       "24  https://www.doi.org/10.5281/zenodo.8173713  ...              2022   \n",
       "25  https://www.doi.org/10.5281/zenodo.8173715  ...              2022   \n",
       "26  https://www.doi.org/10.5281/zenodo.8173717  ...              2022   \n",
       "27  https://www.doi.org/10.5281/zenodo.8173719  ...              2022   \n",
       "28  https://www.doi.org/10.5281/zenodo.8173721  ...              2022   \n",
       "29  https://www.doi.org/10.5281/zenodo.8173725  ...              2022   \n",
       "30  https://www.doi.org/10.5281/zenodo.8173729  ...              2022   \n",
       "31  https://www.doi.org/10.5281/zenodo.8173733  ...              2022   \n",
       "32  https://www.doi.org/10.5281/zenodo.8173735  ...              2022   \n",
       "33  https://www.doi.org/10.5281/zenodo.8173737  ...              2022   \n",
       "34  https://www.doi.org/10.5281/zenodo.8173739  ...              2022   \n",
       "35  https://www.doi.org/10.5281/zenodo.8173741  ...              2022   \n",
       "36  https://www.doi.org/10.5281/zenodo.8173745  ...              2022   \n",
       "37  https://www.doi.org/10.5281/zenodo.8173747  ...              2022   \n",
       "38  https://www.doi.org/10.5281/zenodo.8173749  ...              2022   \n",
       "39  https://www.doi.org/10.5281/zenodo.8173751  ...              2022   \n",
       "40  https://www.doi.org/10.5281/zenodo.8173753  ...              2022   \n",
       "41  https://www.doi.org/10.5281/zenodo.8173755  ...              2022   \n",
       "42  https://www.doi.org/10.5281/zenodo.8173757  ...              2022   \n",
       "43  https://www.doi.org/10.5281/zenodo.8173759  ...              2022   \n",
       "44  https://www.doi.org/10.5281/zenodo.8173763  ...              2022   \n",
       "45                      10.5281/zenodo.6574723  ...              2021   \n",
       "46                      10.5281/zenodo.6574625  ...              2021   \n",
       "47                      10.5281/zenodo.6574629  ...              2021   \n",
       "48                      10.5281/zenodo.6574631  ...              2021   \n",
       "49                      10.5281/zenodo.6574635  ...              2021   \n",
       "\n",
       "                                        review_url_rs   journal_rs  \\\n",
       "0         https://openreview.net/forum?id=XxUIomN-ndH          NaN   \n",
       "1   https://openreview.net/forum?id=a5_hbZf0NB&not...          NaN   \n",
       "2          https://openreview.net/forum?id=WaZB4pUVTi          NaN   \n",
       "3         https://openreview.net/forum?id=bBVZ3pY4z8p          NaN   \n",
       "4         https://openreview.net/forum?id=Od5dD58libt          NaN   \n",
       "5          https://openreview.net/forum?id=gb71irTNN7          NaN   \n",
       "6          https://openreview.net/forum?id=NE_x1dpz-Q          NaN   \n",
       "7          https://openreview.net/forum?id=JJQbk2hIQ5          NaN   \n",
       "8          https://openreview.net/forum?id=JpaQ8GFOVu          NaN   \n",
       "9         https://openreview.net/forum?id=MK4IQJdLLeo          NaN   \n",
       "10         https://openreview.net/forum?id=qP34dvJpHd          NaN   \n",
       "11  https://openreview.net/forum?id=n1q-iz83S5&not...          NaN   \n",
       "12  https://openreview.net/forum?id=xEfg6h1GFmW&no...          NaN   \n",
       "13  https://openreview.net/forum?id=3jaZ5tKRyiT&no...          NaN   \n",
       "14         https://openreview.net/forum?id=uVHUy7CWCL          NaN   \n",
       "15         https://openreview.net/forum?id=sF_vYZSxSV          NaN   \n",
       "16         https://openreview.net/forum?id=YAWQTQZVoA          NaN   \n",
       "17         https://openreview.net/forum?id=MMuv-v99Hy          NaN   \n",
       "18         https://openreview.net/forum?id=NgPQSqpz-Y          NaN   \n",
       "19        https://openreview.net/forum?id=A0Sjs3IJWb-          NaN   \n",
       "20        https://openreview.net/forum?id=N9Wn91tE7D0          NaN   \n",
       "21         https://openreview.net/forum?id=a_9YF58u61          NaN   \n",
       "22         https://openreview.net/forum?id=ye8PftiQLQ          NaN   \n",
       "23        https://openreview.net/forum?id=n2qXFXiMsAM          NaN   \n",
       "24         https://openreview.net/forum?id=nsrHznwHhl          NaN   \n",
       "25         https://openreview.net/forum?id=eJmQJT0Dtt          NaN   \n",
       "26         https://openreview.net/forum?id=WnaVgRhlyT          NaN   \n",
       "27         https://openreview.net/forum?id=jDBYRwDpeW          NaN   \n",
       "28         https://openreview.net/forum?id=DWKJpl8s06          NaN   \n",
       "29        https://openreview.net/forum?id=J-Lgb7Vc0wX          NaN   \n",
       "30         https://openreview.net/forum?id=d7-ns6SZqp          NaN   \n",
       "31         https://openreview.net/forum?id=vWzZQAahuW          NaN   \n",
       "32        https://openreview.net/forum?id=t8ZZ2Y356Ix          NaN   \n",
       "33  https://openreview.net/forum?id=T54wy0ahGLG&no...          NaN   \n",
       "34        https://openreview.net/forum?id=MjZVx7a0KX-          NaN   \n",
       "35        https://openreview.net/forum?id=9_hCoP3LXwy          NaN   \n",
       "36         https://openreview.net/forum?id=ozbAwipuZu          NaN   \n",
       "37  https://openreview.net/forum?id=KNp7Zq3KkT0&no...          NaN   \n",
       "38        https://openreview.net/forum?id=tpk45Zll8eh          NaN   \n",
       "39        https://openreview.net/forum?id=KXfjZPL5pqr          NaN   \n",
       "40         https://openreview.net/forum?id=zKBJw4Ht8s          NaN   \n",
       "41         https://openreview.net/forum?id=Vz9VLcJqKS          NaN   \n",
       "42        https://openreview.net/forum?id=E0qO5dI5aEn          NaN   \n",
       "43        https://openreview.net/forum?id=MF9uv95psps          NaN   \n",
       "44         https://openreview.net/forum?id=ErBe4MnsVD          NaN   \n",
       "45                                                NaN  ReScience C   \n",
       "46        https://openreview.net/forum?id=BSHg22G7n0F  ReScience C   \n",
       "47         https://openreview.net/forum?id=ScfP3G73CY  ReScience C   \n",
       "48         https://openreview.net/forum?id=rcEDhGX3AY  ReScience C   \n",
       "49        https://openreview.net/forum?id=HNlzT3G720t  ReScience C   \n",
       "\n",
       "    volume_journal_rs issue_journal_rs published_year_journal_rs  \\\n",
       "0                 NaN              NaN                       NaN   \n",
       "1                 NaN              NaN                       NaN   \n",
       "2                 NaN              NaN                       NaN   \n",
       "3                 NaN              NaN                       NaN   \n",
       "4                 NaN              NaN                       NaN   \n",
       "5                 NaN              NaN                       NaN   \n",
       "6                 NaN              NaN                       NaN   \n",
       "7                 NaN              NaN                       NaN   \n",
       "8                 NaN              NaN                       NaN   \n",
       "9                 NaN              NaN                       NaN   \n",
       "10                NaN              NaN                       NaN   \n",
       "11                NaN              NaN                       NaN   \n",
       "12                NaN              NaN                       NaN   \n",
       "13                NaN              NaN                       NaN   \n",
       "14                NaN              NaN                       NaN   \n",
       "15                NaN              NaN                       NaN   \n",
       "16                NaN              NaN                       NaN   \n",
       "17                NaN              NaN                       NaN   \n",
       "18                NaN              NaN                       NaN   \n",
       "19                NaN              NaN                       NaN   \n",
       "20                NaN              NaN                       NaN   \n",
       "21                NaN              NaN                       NaN   \n",
       "22                NaN              NaN                       NaN   \n",
       "23                NaN              NaN                       NaN   \n",
       "24                NaN              NaN                       NaN   \n",
       "25                NaN              NaN                       NaN   \n",
       "26                NaN              NaN                       NaN   \n",
       "27                NaN              NaN                       NaN   \n",
       "28                NaN              NaN                       NaN   \n",
       "29                NaN              NaN                       NaN   \n",
       "30                NaN              NaN                       NaN   \n",
       "31                NaN              NaN                       NaN   \n",
       "32                NaN              NaN                       NaN   \n",
       "33                NaN              NaN                       NaN   \n",
       "34                NaN              NaN                       NaN   \n",
       "35                NaN              NaN                       NaN   \n",
       "36                NaN              NaN                       NaN   \n",
       "37                NaN              NaN                       NaN   \n",
       "38                NaN              NaN                       NaN   \n",
       "39                NaN              NaN                       NaN   \n",
       "40                NaN              NaN                       NaN   \n",
       "41                NaN              NaN                       NaN   \n",
       "42                NaN              NaN                       NaN   \n",
       "43                NaN              NaN                       NaN   \n",
       "44                NaN              NaN                       NaN   \n",
       "45                8.0              2.0                    2022.0   \n",
       "46                8.0              2.0                    2022.0   \n",
       "47                8.0              2.0                    2022.0   \n",
       "48                8.0              2.0                    2022.0   \n",
       "49                8.0              2.0                    2022.0   \n",
       "\n",
       "                               domain  entry_type  rs_score rs_comment  \n",
       "0   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "1   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "2   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "3   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "4   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "5   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "6   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "7   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "8   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "9   ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "10  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "11  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "12  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "13  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "14  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "15  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "16  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "17  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "18  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "19  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "20  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "21  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "22  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "23  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "24  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "25  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "26  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "27  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "28  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "29  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "30  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "31  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "32  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "33  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "34  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "35  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "36  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "37  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "38  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "39  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "40  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "41  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "42  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "43  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "44  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "45  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "46  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "47  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "48  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "49  ML Reproducibility Challenge 2021     article       NaN        NaN  \n",
       "\n",
       "[50 rows x 23 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_url = pd.read_excel('RS_ALL_IN_ONE_metadata.xlsx',sheet_name=\"Sheet1\")\n",
    "\n",
    "df_url.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf29a0b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RS_001_MLRC_2022_01\n",
      "https://proceedings.mlr.press/v162/han22c.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_002_MLRC_2022_02\n",
      "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Exact_Feature_Distribution_Matching_for_Arbitrary_Style_Transfer_and_Domain_CVPR_2022_paper.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_003_MLRC_2022_03\n",
      "https://arxiv.org/pdf/2202.05826.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_004_MLRC_2022_04\n",
      "https://proceedings.mlr.press/v162/crabbe22a.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_005_MLRC_2022_05\n",
      "https://aclanthology.org/2021.acl-long.281/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_006_MLRC_2022_06\n",
      "https://proceedings.neurips.cc/paper/2021/hash/628f16b29939d1b060af49f66ae0f7f8-Abstract.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_007_MLRC_2022_07\n",
      "https://neurips.cc/Conferences/2022/ScheduleMultitrack?event=53784\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_008_MLRC_2022_08\n",
      "https://openaccess.thecvf.com/content/CVPR2022/papers/Lim_Hypergraph-Induced_Semantic_Tuplet_Loss_for_Deep_Metric_Learning_CVPR_2022_paper.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_009_MLRC_2022_09\n",
      "https://openreview.net/forum?id=ao30zaT3YL\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_010_MLRC_2022_10\n",
      "https://doi.org/10.1007/978-3-031-19775-8_26\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_011_MLRC_2022_11\n",
      "https://proceedings.mlr.press/v162/crabbe22a/crabbe22a.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_012_MLRC_2022_12\n",
      "https://arxiv.org/abs/1911.12199\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_013_MLRC_2022_13\n",
      "https://iclr.cc/virtual/2022/poster/6666\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_014_MLRC_2022_14\n",
      "https://aclanthology.org/2022.naacl-main.418/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_015_MLRC_2022_15\n",
      "https://openreview.net/pdf?id=nRj0NcmSuxb\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_016_MLRC_2022_16\n",
      "https://proceedings.mlr.press/v162/crabbe22a/crabbe22a.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_017_MLRC_2022_17\n",
      "https://proceedings.neurips.cc/paper/2021/file/043ab21fc5a1607b381ac3896176dac6-Paper.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_018_MLRC_2022_18\n",
      "https://openreview.net/forum?id=wbPObLm6ueA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_019_MLRC_2022_19\n",
      "https://icml.cc/virtual/2022/poster/16829\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_020_MLRC_2022_20\n",
      "https://arxiv.org/abs/2205.00048\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_021_MLRC_2022_21\n",
      "https://arxiv.org/abs/2203.15395\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_022_MLRC_2022_22\n",
      "https://arxiv.org/abs/2204.11830\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_023_MLRC_2022_23\n",
      "https://openreview.net/forum?id=irARV_2VFs4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_024_MLRC_2022_24\n",
      "https://arxiv.org/abs/2203.01928\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_025_MLRC_2022_25\n",
      "https://ieeexplore.ieee.org/document/9878481\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_026_MLRC_2022_26\n",
      "https://openaccess.thecvf.com/content/CVPR2022/papers/Hirota_Quantifying_Societal_Bias_Amplification_in_Image_Captioning_CVPR_2022_paper.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_027_MLRC_2022_27\n",
      "https://ojs.aaai.org/index.php/AAAI/article/view/21454\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_028_MLRC_2022_28\n",
      "https://arxiv.org/abs/2106.03761v4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_029_MLRC_2022_29\n",
      "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720439.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_030_MLRC_2022_30\n",
      "https://arxiv.org/abs/2111.13650v3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_031_MLRC_2022_31\n",
      "https://arxiv.org/abs/2201.12360\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_032_MLRC_2022_32\n",
      "https://ojs.aaai.org/index.php/AAAI/article/view/16721\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_033_MLRC_2022_33\n",
      "https://aclanthology.org/2022.naacl-main.401.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_034_MLRC_2022_34\n",
      "https://proceedings.mlr.press/v162/han22c.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_035_MLRC_2022_35\n",
      "https://dl.acm.org/doi/pdf/10.1145/3531146.3533144\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_036_MLRC_2022_36\n",
      "https://arxiv.org/abs/2203.15395\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_037_MLRC_2022_37\n",
      "https://openreview.net/forum?id=OIs3SxU5Ynl\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_038_MLRC_2022_38\n",
      "https://arxiv.org/pdf/2105.02725.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_039_MLRC_2022_39\n",
      "https://arxiv.org/abs/2105.02725\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_040_MLRC_2022_40\n",
      "https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_041_MLRC_2022_41\n",
      "https://proceedings.mlr.press/v139/yuan21c\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_042_MLRC_2022_42\n",
      "https://openreview.net/pdf?id=6at6rB3IZm\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_043_MLRC_2022_43\n",
      "https://proceedings.neurips.cc/paper_files/paper/2022/hash/90d17e882adbdda42349db6f50123817-Abstract-Conference.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_044_MLRC_2022_44\n",
      "https://aclanthology.org/2021.acl-long.75/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_045_MLRC_2022_45\n",
      "https://proceedings.mlr.press/v162/zada22a\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_046_MLRC_2021_01\n",
      "Editorial - Not Applicable\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_047_MLRC_2021_02\n",
      "https://arxiv.org/abs/2101.06046\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_048_MLRC_2021_03\n",
      "https://openreview.net/forum?id=738x44N7yUE&noteId=46xr8OHtA-\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_049_MLRC_2021_04\n",
      "https://openaccess.thecvf.com/content/CVPR2021/papers/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_050_MLRC_2021_05\n",
      "https://openreview.net/pdf?id=BXewfAYMmJw\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_051_MLRC_2021_06\n",
      "https://www.ijcai.org/proceedings/2021/51 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_052_MLRC_2021_07\n",
      "https://aclanthology.org/2021.acl-long.556.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_053_MLRC_2021_08\n",
      "https://openaccess.thecvf.com/content/ICCV2021/html/Li_SCOUTER_Slot_Attention-Based_Classifier_for_Explainable_Image_Recognition_ICCV_2021_paper.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_054_MLRC_2021_09\n",
      "https://openreview.net/forum?id=YeSwJDOnTRY&referrer=%5BML%20Reproducibility%20Challenge%202021%20Spring%5D(%2Fgroup%3Fid%3DML_Reproducibility_Challenge%2F2021%2FSpring)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_055_MLRC_2021_10\n",
      "https://openreview.net/forum?id=WCRIASdNpsR&noteId=geUtL2j1uoH\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_056_MLRC_2021_11\n",
      "https://openaccess.thecvf.com/content/CVPR2021/papers/Gao_Privacy-Preserving_Collaborative_Learning_With_Automatic_Transformation_Search_CVPR_2021_paper.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_057_MLRC_2021_12\n",
      "https://aclanthology.org/2021.acl-short.73.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_058_MLRC_2021_13\n",
      "https://aclanthology.org/2020.emnlp-main.656\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_059_MLRC_2021_14\n",
      "https://ojs.aaai.org/index.php/AAAI/article/view/17336\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_060_MLRC_2021_15\n",
      "https://openaccess.thecvf.com/content/ICCV2021/papers/Lang_Explaining_in_Style_Training_a_GAN_To_Explain_a_Classifier_ICCV_2021_paper.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_061_MLRC_2021_16\n",
      "https://arxiv.org/pdf/2010.06121.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_062_MLRC_2021_17\n",
      "http://proceedings.mlr.press/v139/tian21a/tian21a.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_063_MLRC_2021_18\n",
      "https://arxiv.org/pdf/2006.07500.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_064_MLRC_2021_19\n",
      "https://arxiv.org/pdf/2012.08723.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_065_MLRC_2021_20\n",
      "https://arxiv.org/pdf/2103.01826.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_066_MLRC_2021_21\n",
      "https://arxiv.org/abs/2104.13369\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_067_MLRC_2021_22\n",
      "https://ojs.aaai.org/index.php/AAAI/article/view/17080\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_068_MLRC_2021_23\n",
      "https://www.ijcai.org/proceedings/2021/0305.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_069_MLRC_2021_24\n",
      "https://proceedings.mlr.press/v139/correa21a\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_070_MLRC_2021_25\n",
      "https://icml.cc/virtual/2021/spotlight/10160\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_071_MLRC_2021_26\n",
      "http://openaccess.thecvf.com//content/CVPR2021/papers/Oh_Background-Aware_Pooling_and_Noise-Aware_Loss_for_Weakly-Supervised_Semantic_Segmentation_CVPR_2021_paper.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_072_MLRC_2021_27\n",
      "https://openreview.net/forum?id=YrKu2s0HaG6Q&noteId=SxQ0jqm5O0w\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_073_MLRC_2021_28\n",
      "https://ojs.aaai.org/index.php/AAAI/article/view/17080\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_074_MLRC_2021_29\n",
      "https://www.ijcai.org/proceedings/2021/51\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_075_MLRC_2021_30\n",
      "https://openreview.net/forum?id=FGqiDsBUKL0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_076_MLRC_2021_31\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=9548\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_077_MLRC_2021_32\n",
      "https://proceedings.mlr.press/v139/correa21a.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_078_MLRC_2021_33\n",
      "http://proceedings.mlr.press/v139/lee21b.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_079_MLRC_2021_34\n",
      "http://proceedings.mlr.press/v139/chaplot21a.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_080_MLRC_2021_35\n",
      "https://openreview.net/forum?id=aGfr4iF_Li&noteId=dOsxraq3Vto\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_081_MLRC_2021_36\n",
      "https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Social_NCE_Contrastive_Learning_of_Socially-Aware_Motion_Representations_ICCV_2021_paper.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_082_MLRC_2021_37\n",
      "https://arxiv.org/abs/2012.01526\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_083_MLRC_2021_38\n",
      "https://openreview.net/forum?id=dlEJsyHGeaL\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_084_MLRC_2021_39\n",
      "https://openaccess.thecvf.com/content/CVPR2021/html/Ranjan_Learning_To_Count_Everything_CVPR_2021_paper.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_085_MLRC_2021_40\n",
      "https://aclanthology.org/2021.acl-long.148/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_086_MLRC_2021_41\n",
      "https://openaccess.thecvf.com/content/ICCV2021/papers/Fan_Transparent_Object_Tracking_Benchmark_ICCV_2021_paper.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_087_MLRC_2021_42\n",
      "https://openaccess.thecvf.com/content/ICCV2021/papers/Lang_Explaining_in_Style_Training_a_GAN_To_Explain_a_Classifier_ICCV_2021_paper.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_088_MLRC_2021_43\n",
      "https://openreview.net/forum?id=XN1M27T6uux\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_089_MLRC_2021_44\n",
      "https://openaccess.thecvf.com/content/CVPR2021/html/Gao_Privacy-Preserving_Collaborative_Learning_With_Automatic_Transformation_Search_CVPR_2021_paper.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_090_MLRC_2021_45\n",
      "https://openreview.net/forum?id=wGmOLwb8ClT\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_091_MLRC_2021_46\n",
      "https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_Lifting_2D_StyleGAN_for_3D-Aware_Face_Generation_CVPR_2021_paper.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_092_MLRC_2021_47\n",
      "https://arxiv.org/pdf/2105.06709.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_093_MLRC_2021_48\n",
      "https://paperswithcode.com/paper/nondeterminism-and-instability-in-neural\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_094_MLRC_2020_01\n",
      "https://openreview.net/forum?id=MFj70_2-eY1&noteId=qf1uS0EEyLy&referrer=%5BML%20Reproducibility%20Challenge%202020%5D(%2Fgroup%3Fid%3DML_Reproducibility_Challenge%2F2020)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_095_MLRC_2020_02\n",
      "https://openreview.net/forum?id=fgUXyn4Qrk0&noteId=n4yKe2VVmN3&referrer=%5BML%20Reproducibility%20Challenge%202020%5D(%2Fgroup%3Fid%3DML_Reproducibility_Challenge%2F2020)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_096_MLRC_2020_03\n",
      "https://openreview.net/forum?id=r_qY49Av7wj&noteId=hEN70lsEC4&referrer=%5BML%20Reproducibility%20Challenge%202020%5D(%2Fgroup%3Fid%3DML_Reproducibility_Challenge%2F2020)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_097_MLRC_2020_04\n",
      "https://openreview.net/forum?id=MhMYW2PqGSH\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_098_MLRC_2020_05\n",
      "https://openreview.net/forum?id=jsrGMN98wJ&noteId=D7XO0agO1i5&referrer=%5BML%20Reproducibility%20Challenge%202020%5D(%2Fgroup%3Fid%3DML_Reproducibility_Challenge%2F2020)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_099_MLRC_2020_06\n",
      "https://openreview.net/forum?id=ZVxchkVPa8S\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_100_MLRC_2020_07\n",
      "https://openreview.net/forum?id=WsphwsV5hV&referrer=%5BML%20Reproducibility%20Challenge%202020%5D(%2Fgroup%3Fid%3DML_Reproducibility_Challenge%2F2020)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_101_MLRC_2020_08\n",
      "https://openreview.net/forum?id=kqKeIP46EF8&noteId=epbmiUjgl51&referrer=%5BML%20Reproducibility%20Challenge%202020%5D(%2Fgroup%3Fid%3DML_Reproducibility_Challenge%2F2020)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_102_MLRC_2020_09\n",
      "https://openreview.net/forum?id=bleIdqV_-JY\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_103_MLRC_2020_10\n",
      "https://openreview.net/forum?id=BygSP6Vtvr\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_104_MLRC_2020_11\n",
      "https://openreview.net/forum?id=Emc_HGRBOHH\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_105_MLRC_2020_12\n",
      "https://openreview.net/forum?id=8CJIjwUrbls&noteId=r7GUWB4G6oT\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_106_MLRC_2020_13\n",
      "https://openreview.net/forum?id=_yBqU7P7_cn&noteId=TezNrEptzwO\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_107_MLRC_2020_14\n",
      "https://openreview.net/forum?id=zP0rh1RL8y-\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_108_MLRC_2020_15\n",
      "https://openreview.net/forum?id=PUaAgFaMIlaZ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_109_MLRC_2020_16\n",
      "https://openreview.net/forum?id=MFj70_2-eY1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_110_MLRC_2020_17\n",
      "https://openreview.net/forum?id=QxBBSsd_j9b&referrer=%5BML%20Reproducibility%20Challenge%202020%5D(%2Fgroup%3Fid%3DML_Reproducibility_Challenge%2F2020)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_111_MLRC_2020_18\n",
      "https://openreview.net/forum?id=w23q3ttruo&noteId=XenPH-P3an\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_112_MLRC_2020_19\n",
      "https://ieeexplore.ieee.org/document/9157055\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_113_MLRC_2020_20\n",
      "https://openreview.net/forum?id=S1xFl64tDr\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_114_MLRC_2020_21\n",
      "https://openreview.net/forum?id=NqEF-9d38i6&referrer=%5BML%20Reproducibility%20Challenge%202020%5D(%2Fgroup%3Fid%3DML_Reproducibility_Challenge%2F2020)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_115_MLRC_2020_22\n",
      "https://openreview.net/forum?id=ykG2B9bWiPXe&referrer=%5BML%20Reproducibility%20Challenge%202020%5D(%2Fgroup%3Fid%3DML_Reproducibility_Challenge%2F2020)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_116_MLRC_2020_23\n",
      "https://openreview.net/forum?id=tdG6Fa3Y6hq&referrer=%5BML%20Reproducibility%20Challenge%202020%5D(%2Fgroup%3Fid%3DML_Reproducibility_Challenge%2F2020)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_117_MLRC_2020_24\n",
      "Editorial - Not Applicable\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_118_NeurIPS_2019_01\n",
      "Editorial - Not Applicable\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_119_NeurIPS_2019_02\n",
      "https://arxiv.org/abs/1906.02107\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_120_NeurIPS_2019_03\n",
      "https://arxiv.org/abs/1907.05600\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_121_NeurIPS_2019_04\n",
      "https://arxiv.org/abs/1901.10738\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_122_NeurIPS_2019_05\n",
      "https://arxiv.org/abs/1806.08593\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_123_NeurIPS_2019_06\n",
      "https://arxiv.org/abs/1906.01563\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_124_NeurIPS_2019_07\n",
      "https://arxiv.org/abs/1905.09768\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_125_NeurIPS_2019_08\n",
      "https://arxiv.org/abs/1906.08253\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_126_NeurIPS_2019_09\n",
      "https://arxiv.org/abs/1906.08226\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_127_NeurIPS_2019_10\n",
      "https://arxiv.org/abs/1906.02773\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_128_NeurIPS_2019_11\n",
      "https://arxiv.org/abs/1905.11001\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_129_ICLR_2019_01\n",
      "Editorial - Not Applicable\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_130_ICLR_2019_02\n",
      "https://arxiv.org/abs/1805.08136\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_131_ICLR_2019_03\n",
      "https://proceedings.mlr.press/v115/tonolini20a.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_132_ICLR_2019_04\n",
      "https://arxiv.org/abs/1906.01200\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_133_ICLR_2019_05\n",
      "https://arxiv.org/abs/1810.03023\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_134_ICDAR_2018_01\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_135_ICDAR_2018_02\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_136_ICDAR_2018_03\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_137_ICDAR_2018_04\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_138_ICDAR_2018_05\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_139_ICDAR_2018_06\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_140_ICDAR_2018_07\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_141_ICDAR_2018_08\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_142_ICDAR_2018_09\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_143_ICDAR_2018_10\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_144_ICDAR_2018_11\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_145_ICDAR_2018_12\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_146_ICDAR_2018_13\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_147_ICDAR_2018_14\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_148_ICDAR_2018_15\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_149_ICDAR_2018_16\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_url.iterrows():\n",
    "    \n",
    "    print(row['key_for_all_RS'])\n",
    "    print(row['org_paper_url'])\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9109207",
   "metadata": {},
   "source": [
    "## ``parse pdfs for rs_comment``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3436265b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RS_ALL_148\\\\RS_001_MLRC_2022_01.pdf',\n",
       " 'RS_ALL_148\\\\RS_002_MLRC_2022_02.pdf',\n",
       " 'RS_ALL_148\\\\RS_003_MLRC_2022_03.pdf',\n",
       " 'RS_ALL_148\\\\RS_004_MLRC_2022_04.pdf',\n",
       " 'RS_ALL_148\\\\RS_005_MLRC_2022_05.pdf',\n",
       " 'RS_ALL_148\\\\RS_006_MLRC_2022_06.pdf',\n",
       " 'RS_ALL_148\\\\RS_007_MLRC_2022_07.pdf',\n",
       " 'RS_ALL_148\\\\RS_008_MLRC_2022_08.pdf',\n",
       " 'RS_ALL_148\\\\RS_009_MLRC_2022_09.pdf',\n",
       " 'RS_ALL_148\\\\RS_010_MLRC_2022_10.pdf',\n",
       " 'RS_ALL_148\\\\RS_011_MLRC_2022_11.pdf',\n",
       " 'RS_ALL_148\\\\RS_012_MLRC_2022_12.pdf',\n",
       " 'RS_ALL_148\\\\RS_013_MLRC_2022_13.pdf',\n",
       " 'RS_ALL_148\\\\RS_014_MLRC_2022_14.pdf',\n",
       " 'RS_ALL_148\\\\RS_015_MLRC_2022_15.pdf',\n",
       " 'RS_ALL_148\\\\RS_016_MLRC_2022_16.pdf',\n",
       " 'RS_ALL_148\\\\RS_017_MLRC_2022_17.pdf',\n",
       " 'RS_ALL_148\\\\RS_018_MLRC_2022_18.pdf',\n",
       " 'RS_ALL_148\\\\RS_019_MLRC_2022_19.pdf',\n",
       " 'RS_ALL_148\\\\RS_020_MLRC_2022_20.pdf',\n",
       " 'RS_ALL_148\\\\RS_021_MLRC_2022_21.pdf',\n",
       " 'RS_ALL_148\\\\RS_022_MLRC_2022_22.pdf',\n",
       " 'RS_ALL_148\\\\RS_023_MLRC_2022_23.pdf',\n",
       " 'RS_ALL_148\\\\RS_024_MLRC_2022_24.pdf',\n",
       " 'RS_ALL_148\\\\RS_025_MLRC_2022_25.pdf',\n",
       " 'RS_ALL_148\\\\RS_026_MLRC_2022_26.pdf',\n",
       " 'RS_ALL_148\\\\RS_027_MLRC_2022_27.pdf',\n",
       " 'RS_ALL_148\\\\RS_028_MLRC_2022_28.pdf',\n",
       " 'RS_ALL_148\\\\RS_029_MLRC_2022_29.pdf',\n",
       " 'RS_ALL_148\\\\RS_030_MLRC_2022_30.pdf',\n",
       " 'RS_ALL_148\\\\RS_031_MLRC_2022_31.pdf',\n",
       " 'RS_ALL_148\\\\RS_032_MLRC_2022_32.pdf',\n",
       " 'RS_ALL_148\\\\RS_033_MLRC_2022_33.pdf',\n",
       " 'RS_ALL_148\\\\RS_034_MLRC_2022_34.pdf',\n",
       " 'RS_ALL_148\\\\RS_035_MLRC_2022_35.pdf',\n",
       " 'RS_ALL_148\\\\RS_036_MLRC_2022_36.pdf',\n",
       " 'RS_ALL_148\\\\RS_037_MLRC_2022_37.pdf',\n",
       " 'RS_ALL_148\\\\RS_038_MLRC_2022_38.pdf',\n",
       " 'RS_ALL_148\\\\RS_039_MLRC_2022_39.pdf',\n",
       " 'RS_ALL_148\\\\RS_040_MLRC_2022_40.pdf',\n",
       " 'RS_ALL_148\\\\RS_041_MLRC_2022_41.pdf',\n",
       " 'RS_ALL_148\\\\RS_042_MLRC_2022_42.pdf',\n",
       " 'RS_ALL_148\\\\RS_043_MLRC_2022_43.pdf',\n",
       " 'RS_ALL_148\\\\RS_044_MLRC_2022_44.pdf',\n",
       " 'RS_ALL_148\\\\RS_045_MLRC_2022_45.pdf',\n",
       " 'RS_ALL_148\\\\RS_046_MLRC_2021_01.pdf',\n",
       " 'RS_ALL_148\\\\RS_047_MLRC_2021_02.pdf',\n",
       " 'RS_ALL_148\\\\RS_048_MLRC_2021_03.pdf',\n",
       " 'RS_ALL_148\\\\RS_049_MLRC_2021_04.pdf',\n",
       " 'RS_ALL_148\\\\RS_050_MLRC_2021_05.pdf',\n",
       " 'RS_ALL_148\\\\RS_051_MLRC_2021_06.pdf',\n",
       " 'RS_ALL_148\\\\RS_052_MLRC_2021_07.pdf',\n",
       " 'RS_ALL_148\\\\RS_053_MLRC_2021_08.pdf',\n",
       " 'RS_ALL_148\\\\RS_054_MLRC_2021_09.pdf',\n",
       " 'RS_ALL_148\\\\RS_055_MLRC_2021_10.pdf',\n",
       " 'RS_ALL_148\\\\RS_056_MLRC_2021_11.pdf',\n",
       " 'RS_ALL_148\\\\RS_057_MLRC_2021_12.pdf',\n",
       " 'RS_ALL_148\\\\RS_058_MLRC_2021_13.pdf',\n",
       " 'RS_ALL_148\\\\RS_059_MLRC_2021_14.pdf',\n",
       " 'RS_ALL_148\\\\RS_060_MLRC_2021_15.pdf',\n",
       " 'RS_ALL_148\\\\RS_061_MLRC_2021_16.pdf',\n",
       " 'RS_ALL_148\\\\RS_062_MLRC_2021_17.pdf',\n",
       " 'RS_ALL_148\\\\RS_063_MLRC_2021_18.pdf',\n",
       " 'RS_ALL_148\\\\RS_064_MLRC_2021_19.pdf',\n",
       " 'RS_ALL_148\\\\RS_065_MLRC_2021_20.pdf',\n",
       " 'RS_ALL_148\\\\RS_066_MLRC_2021_21.pdf',\n",
       " 'RS_ALL_148\\\\RS_067_MLRC_2021_22.pdf',\n",
       " 'RS_ALL_148\\\\RS_068_MLRC_2021_23.pdf',\n",
       " 'RS_ALL_148\\\\RS_069_MLRC_2021_24.pdf',\n",
       " 'RS_ALL_148\\\\RS_070_MLRC_2021_25.pdf',\n",
       " 'RS_ALL_148\\\\RS_071_MLRC_2021_26.pdf',\n",
       " 'RS_ALL_148\\\\RS_072_MLRC_2021_27.pdf',\n",
       " 'RS_ALL_148\\\\RS_073_MLRC_2021_28.pdf',\n",
       " 'RS_ALL_148\\\\RS_074_MLRC_2021_29.pdf',\n",
       " 'RS_ALL_148\\\\RS_075_MLRC_2021_30.pdf',\n",
       " 'RS_ALL_148\\\\RS_076_MLRC_2021_31.pdf',\n",
       " 'RS_ALL_148\\\\RS_077_MLRC_2021_32.pdf',\n",
       " 'RS_ALL_148\\\\RS_078_MLRC_2021_33.pdf',\n",
       " 'RS_ALL_148\\\\RS_079_MLRC_2021_34.pdf',\n",
       " 'RS_ALL_148\\\\RS_080_MLRC_2021_35.pdf',\n",
       " 'RS_ALL_148\\\\RS_081_MLRC_2021_36.pdf',\n",
       " 'RS_ALL_148\\\\RS_082_MLRC_2021_37.pdf',\n",
       " 'RS_ALL_148\\\\RS_083_MLRC_2021_38.pdf',\n",
       " 'RS_ALL_148\\\\RS_084_MLRC_2021_39.pdf',\n",
       " 'RS_ALL_148\\\\RS_085_MLRC_2021_40.pdf',\n",
       " 'RS_ALL_148\\\\RS_086_MLRC_2021_41.pdf',\n",
       " 'RS_ALL_148\\\\RS_087_MLRC_2021_42.pdf',\n",
       " 'RS_ALL_148\\\\RS_088_MLRC_2021_43.pdf',\n",
       " 'RS_ALL_148\\\\RS_089_MLRC_2021_44.pdf',\n",
       " 'RS_ALL_148\\\\RS_090_MLRC_2021_45.pdf',\n",
       " 'RS_ALL_148\\\\RS_091_MLRC_2021_46.pdf',\n",
       " 'RS_ALL_148\\\\RS_092_MLRC_2021_47.pdf',\n",
       " 'RS_ALL_148\\\\RS_093_MLRC_2021_48.pdf',\n",
       " 'RS_ALL_148\\\\RS_094_MLRC_2020_01.pdf',\n",
       " 'RS_ALL_148\\\\RS_095_MLRC_2020_02.pdf',\n",
       " 'RS_ALL_148\\\\RS_096_MLRC_2020_03.pdf',\n",
       " 'RS_ALL_148\\\\RS_097_MLRC_2020_04.pdf',\n",
       " 'RS_ALL_148\\\\RS_098_MLRC_2020_05.pdf',\n",
       " 'RS_ALL_148\\\\RS_099_MLRC_2020_06.pdf',\n",
       " 'RS_ALL_148\\\\RS_100_MLRC_2020_07.pdf',\n",
       " 'RS_ALL_148\\\\RS_101_MLRC_2020_08.pdf',\n",
       " 'RS_ALL_148\\\\RS_102_MLRC_2020_09.pdf',\n",
       " 'RS_ALL_148\\\\RS_103_MLRC_2020_10.pdf',\n",
       " 'RS_ALL_148\\\\RS_104_MLRC_2020_11.pdf',\n",
       " 'RS_ALL_148\\\\RS_105_MLRC_2020_12.pdf',\n",
       " 'RS_ALL_148\\\\RS_106_MLRC_2020_13.pdf',\n",
       " 'RS_ALL_148\\\\RS_107_MLRC_2020_14.pdf',\n",
       " 'RS_ALL_148\\\\RS_108_MLRC_2020_15.pdf',\n",
       " 'RS_ALL_148\\\\RS_109_MLRC_2020_16.pdf',\n",
       " 'RS_ALL_148\\\\RS_110_MLRC_2020_17.pdf',\n",
       " 'RS_ALL_148\\\\RS_111_MLRC_2020_18.pdf',\n",
       " 'RS_ALL_148\\\\RS_112_MLRC_2020_19.pdf',\n",
       " 'RS_ALL_148\\\\RS_113_MLRC_2020_20.pdf',\n",
       " 'RS_ALL_148\\\\RS_114_MLRC_2020_21.pdf',\n",
       " 'RS_ALL_148\\\\RS_115_MLRC_2020_22.pdf',\n",
       " 'RS_ALL_148\\\\RS_116_MLRC_2020_23.pdf',\n",
       " 'RS_ALL_148\\\\RS_117_MLRC_2020_24.pdf',\n",
       " 'RS_ALL_148\\\\RS_118_NeurIPS_2019_01.pdf',\n",
       " 'RS_ALL_148\\\\RS_119_NeurIPS_2019_02.pdf',\n",
       " 'RS_ALL_148\\\\RS_120_NeurIPS_2019_03.pdf',\n",
       " 'RS_ALL_148\\\\RS_121_NeurIPS_2019_04.pdf',\n",
       " 'RS_ALL_148\\\\RS_122_NeurIPS_2019_05.pdf',\n",
       " 'RS_ALL_148\\\\RS_123_NeurIPS_2019_06.pdf',\n",
       " 'RS_ALL_148\\\\RS_124_NeurIPS_2019_07.pdf',\n",
       " 'RS_ALL_148\\\\RS_125_NeurIPS_2019_08.pdf',\n",
       " 'RS_ALL_148\\\\RS_126_NeurIPS_2019_09.pdf',\n",
       " 'RS_ALL_148\\\\RS_127_NeurIPS_2019_10.pdf',\n",
       " 'RS_ALL_148\\\\RS_128_NeurIPS_2019_11.pdf',\n",
       " 'RS_ALL_148\\\\RS_129_ICLR_2019_01.pdf',\n",
       " 'RS_ALL_148\\\\RS_130_ICLR_2019_02.pdf',\n",
       " 'RS_ALL_148\\\\RS_131_ICLR_2019_03.pdf',\n",
       " 'RS_ALL_148\\\\RS_132_ICLR_2019_04.pdf',\n",
       " 'RS_ALL_148\\\\RS_133_ICLR_2019_05.pdf',\n",
       " 'RS_ALL_148\\\\RS_134_ICDAR_2018_01.pdf',\n",
       " 'RS_ALL_148\\\\RS_135_ICDAR_2018_02.pdf',\n",
       " 'RS_ALL_148\\\\RS_136_ICDAR_2018_03.pdf',\n",
       " 'RS_ALL_148\\\\RS_137_ICDAR_2018_04.pdf',\n",
       " 'RS_ALL_148\\\\RS_138_ICDAR_2018_05.pdf',\n",
       " 'RS_ALL_148\\\\RS_139_ICDAR_2018_06.pdf',\n",
       " 'RS_ALL_148\\\\RS_140_ICDAR_2018_07.pdf',\n",
       " 'RS_ALL_148\\\\RS_141_ICDAR_2018_08.pdf',\n",
       " 'RS_ALL_148\\\\RS_142_ICDAR_2018_09.pdf',\n",
       " 'RS_ALL_148\\\\RS_143_ICDAR_2018_10.pdf',\n",
       " 'RS_ALL_148\\\\RS_144_ICDAR_2018_11.pdf',\n",
       " 'RS_ALL_148\\\\RS_145_ICDAR_2018_12.pdf',\n",
       " 'RS_ALL_148\\\\RS_146_ICDAR_2018_13.pdf',\n",
       " 'RS_ALL_148\\\\RS_147_ICDAR_2018_14.pdf',\n",
       " 'RS_ALL_148\\\\RS_148_ICDAR_2018_15.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names = glob.glob(\"RS_ALL_148/*.pdf\")\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d8ce70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 11.683,725.909,94.604,734.876 'R E S C I E N C E C\\n'>\n",
      "<LTTextBoxHorizontal(1) 127.559,693.286,486.414,744.746 'Replication / ML Reproducibility Challenge 2022\\n[Re] Exact Feature Distribution Matching for Arbitrary\\nStyle Transfer and Domain Generalization\\n'>\n",
      "<LTTextBoxHorizontal(2) 127.410,650.243,362.549,672.531 'Mert Erkol1, ID , Furkan Kınlı1, ID , Barış Özcan1, ID , and Furkan Kıraç1, ID\\n1Ozyegin University, Vision and Graphics Laboratory, Istanbul, Turkey\\n'>\n",
      "<LTTextBoxHorizontal(3) 54.396,577.323,106.961,613.686 'Edited by\\nKoustuv Sinha,\\nMaurits Bleeker,\\nSamarth Bhargav\\n'>\n",
      "<LTTextBoxHorizontal(4) 53.719,548.929,106.307,566.364 'Received\\n04 February 2023\\n'>\n",
      "<LTTextBoxHorizontal(5) 67.869,520.536,106.312,537.970 'Published\\n20 July 2023\\n'>\n",
      "<LTTextBoxHorizontal(6) 29.583,492.142,106.327,509.577 'DOI\\n10.5281/zenodo.8173652\\n'>\n",
      "<LTTextBoxHorizontal(7) 127.559,579.150,268.023,593.496 'Reproducibility Summary\\n'>\n",
      "<LTTextBoxHorizontal(8) 127.559,471.980,511.787,565.629 'In this reproducibility study, we present our results and experience during replicating\\nthe paper, titled Exact Feature Distribution Matching for Arbitrary Style Transfer and\\nDomain Generalization [1]. In real‐world scenarios, the feature distributions are mostly\\nmuch more complicated than Gaussian, so only mean and standard deviation may not\\nbe fully representative to match them. This paper introduces a novel strategy to exactly\\nmatch the histograms of image features via the Sort‐Matching algorithm in a computa‐\\ntionally feasible way. We were able to reproduce most of the results presented in the\\noriginal paper both qualitatively and quantitatively.\\n'>\n",
      "<LTTextBoxHorizontal(9) 127.559,408.779,511.786,454.608 'Scope of Reproducibility — In the scope of this study, we aim to reproduce all the quali‐\\ntative and quantitative results on two tasks, namely Arbitrary Style Transfer (AST) and\\nDomain Generalization (DG). Moreover, we investigate the capability of forming better\\nstyle representations by EFDM in another recent study [2].\\n'>\n",
      "<LTTextBoxHorizontal(10) 127.559,333.624,511.787,391.407 'Methodology — We have conducted all experiments in the original work by using the of‐\\nficial repository, which is implemented by PyTorch [3]. For additional experiments, we\\nhave implemented the modular version of EFDM as a layer to replace it with the normal‐\\nization modules. We have used 2 NVIDIA RTX 2080Ti GPUs for both training and testing,\\nand it took roughly 1 day to complete a single training.\\n'>\n",
      "<LTTextBoxHorizontal(11) 127.559,270.423,511.787,316.252 'Results — We have reproduced the experiments done on two selected tasks, and com‐\\npared their results with the reported results. Although our experimental results are not\\nidentical to the reported ones, we can validate the claims made by the original study\\naccording to these results.\\n'>\n",
      "<LTTextBoxHorizontal(12) 127.180,231.133,510.234,253.051 'What was easy — The paper is well‐written and easy to follow. The original repository is\\nwell‐organized to run all tests with the data presented in the paper.\\n'>\n",
      "<LTTextBoxHorizontal(13) 127.559,191.843,510.233,213.761 'What was difficult — The requirements in the repository were not updated, and we had to\\nmanage different versions of Python packages to be able to conduct the experiments.\\n'>\n",
      "<LTTextBoxHorizontal(14) 127.385,121.149,402.013,167.973 'Copyright © 2023 M. Erkol et al., released under a Creative Commons Attribution 4.0 International license.\\nCorrespondence should be addressed to Mert Erkol (mert.erkol@ozu.edu.tr)\\nThe authors have declared that no competing interests exist.\\nCode\\nat\\nswh:1:dir:b76a5bf3f3f540d17ef0f4a22ecc0b4e2c27d680.\\nOpen peer review is available at https://openreview.net/forum?id=a5_hbZf0NB&noteId=Rsj9NvSj2Ft.\\n'>\n",
      "<LTTextBoxHorizontal(15) 218.467,137.089,334.867,144.063 'https://github.com/birdortyedi/efdm-pytorch\\n'>\n",
      "<LTTextBoxHorizontal(16) 167.714,137.089,190.749,144.063 'available\\n'>\n",
      "<LTTextBoxHorizontal(17) 361.474,137.089,370.812,144.063 'DOI\\n'>\n",
      "<LTTextBoxHorizontal(18) 152.093,137.089,156.458,144.063 'is\\n'>\n",
      "<LTTextBoxHorizontal(19) 346.160,137.089,350.219,144.063 '–\\n'>\n",
      "<LTTextBoxHorizontal(20) 382.059,137.089,450.685,144.063 '10.5281/zenodo.7895753.\\n'>\n",
      "<LTTextBoxHorizontal(21) 481.996,137.089,486.055,144.063 '–\\n'>\n",
      "<LTTextBoxHorizontal(22) 497.310,137.089,510.240,144.063 'SWH\\n'>\n",
      "<LTTextBoxHorizontal(23) 127.559,42.726,231.896,49.700 'ReScience C 9.2 (#2) – Erkol et al. 2023\\n'>\n",
      "<LTTextBoxHorizontal(24) 506.954,42.726,510.239,49.700 '1\\n'>\n",
      "<LTRect 0.198,716.966,106.498,745.313>\n",
      "<LTCurve 168.121,666.706,175.045,673.357>\n",
      "<LTCurve 168.519,667.105,174.647,672.958>\n",
      "<LTCurve 225.426,666.706,232.350,673.357>\n",
      "<LTCurve 225.825,667.105,231.952,672.958>\n",
      "<LTCurve 282.412,666.706,289.336,673.357>\n",
      "<LTCurve 282.811,667.105,288.938,672.958>\n",
      "<LTCurve 357.120,666.706,364.044,673.357>\n",
      "<LTCurve 357.519,667.105,363.646,672.958>\n",
      "<LTLine 127.559,171.888,510.236,171.888>\n",
      "<LTTextBoxHorizontal(0) 127.559,782.258,375.715,789.232 '[Re] Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization\\n'>\n",
      "<LTTextBoxHorizontal(1) 127.559,732.243,510.454,754.161 'Communication with original authors — We were in contact with the authors, and asked for\\nthe original results as JPEG files to prepare the figures in this report.\\n'>\n",
      "<LTTextBoxHorizontal(2) 113.633,696.549,195.790,710.895 '1 Introduction\\n'>\n",
      "<LTTextBoxHorizontal(3) 127.260,314.310,512.335,682.928 'Feature distribution matching is one of the most challenging learning tasks for visual\\ninputs. Arbitrary Style Transfer (AST) and Domain Generalization (DG) are the common\\ntasks in the literature where feature distribution matching can be considered as the so‐\\nlution. For example, in AST, the style information of input and target images can be\\ninterpreted as feature distributions and style can be transferred by cross‐distribution\\nfeature matching [4, 5, 6, 7, 8, 9, 10]. The first drawback in the previous studies is to\\nuse only the mean and standard deviation to match the feature distributions, which\\nmainly relies on the assumption of that the feature distributions follow Gaussian. In\\nreal‐world scenarios, the feature distributions are often much more complicated than\\nGaussian, thus mean and standard deviation may not be fully representative to exactly\\nmatch them. Secondly, although Exact Feature Distribution Matching (EFDM) can be\\nachieved by directly matching the higher‐order statistics of the image features, it is not\\npractical for the current application areas due to the intensive computational overhead.\\nThis paper [1] proposes to perform EFDM in a more effective way by exactly matching the\\nempirical Cumulative Distribution Functions (eCDFs) of image features. As mentioned\\nin the paper, Glivenko–Cantelli theorem [11] states that the empirical Cumulative Distri‐\\nbution Function (eCDF) asymptotically converges to the Cumulative Distribution Func‐\\ntion (CDF) when the number of samples approaches infinity. Relying on this theorem,\\nthis study demonstrates that the feature distributions (i.e., mean, standard deviation,\\nand higher‐order statistics) can be exactly matched by using eCDFs. The authors claim\\nthat this can be achieved by employing a custom Exact Histogram Matching algorithm\\nthat implements Sort‐Matching [12].\\nIn this reproducibility report, we studied EFDM via the Sort‐Matching algorithm on two\\ntasks related to feature distribution matching. In this study, we aimed to reproduce the\\nexperiments provided in the original paper on AST and DG, and reported the details and\\nissues we encountered during this process. We have compared the results obtained in\\nour experiments with the ones reported in the original paper. We have also extended the\\nexperiments to observe how much the performance changes when some hyperparame‐\\nters of EFDM or EFDMix are modified. In addition to the experiments in the paper, we\\nhave investigated the EFDM have the capability of forming better style representations\\nin the cases of modeling the subjects as style.\\n'>\n",
      "<LTTextBoxHorizontal(4) 113.633,278.616,259.731,292.962 '2 Scope of reproducibility\\n'>\n",
      "<LTTextBoxHorizontal(5) 127.260,171.346,511.786,264.995 'The main idea of the paper is to introduce a novel strategy that achieves to exactly match\\nthe feature distributions by using eCDFs of the input and target image features. This\\nstrategy is tested on two tasks related to feature distribution matching, namely Arbitrary\\nStyle Transfer (AST) and Domain Generalization (DG).\\nThe proposed EFDM strategy claims that it shows superior performance to the exist‐\\ning state‐of‐the‐art methods of AST and DG in terms of visual quality and quantitative\\nmeasures. To validate these claims and further analyze the proposed strategy, we try to\\ninvestigate the following questions:\\n'>\n",
      "<LTTextBoxHorizontal(6) 144.476,149.428,510.243,159.391 '• Does EFDM work stably on AST and also more challenging photo‐realistic style\\n'>\n",
      "<LTTextBoxHorizontal(7) 152.466,137.473,235.604,147.436 'transfer scenarios?\\n'>\n",
      "<LTTextBoxHorizontal(8) 144.476,117.548,510.233,127.511 '• How can the style information of multiple images, which is extracted by EFDM, be\\n'>\n",
      "<LTTextBoxHorizontal(9) 152.466,105.593,299.713,115.556 'interpolated in the feature space?\\n'>\n",
      "<LTTextBoxHorizontal(10) 127.559,42.726,231.896,49.700 'ReScience C 9.2 (#2) – Erkol et al. 2023\\n'>\n",
      "<LTTextBoxHorizontal(11) 506.954,42.726,510.239,49.700 '2\\n'>\n",
      "<LTTextBoxHorizontal(0) 127.559,782.258,375.715,789.232 '[Re] Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization\\n'>\n",
      "<LTTextBoxHorizontal(1) 127.559,585.241,510.242,605.166 'Figure 1. Overall training scheme of the proposed strategy in AST, including the EFDM of the image\\nfeatures. Obtained from the presentation of [1].\\n'>\n",
      "<LTTextBoxHorizontal(2) 144.476,527.886,510.237,561.759 '• Does the proposed feature augmentation method via EFDM (i.e., EFDMix) improve\\nthe generalization capability on category classification and cross‐domain instance\\nretrieval?\\n'>\n",
      "<LTTextBoxHorizontal(3) 144.476,484.050,510.243,517.923 '• Does the performance of the proposed strategy change by modifying the weight\\nof the style loss term used in the training of EFDM and the instance‐wise mixing\\nweight used for EFDMix?\\n'>\n",
      "<LTTextBoxHorizontal(4) 144.476,464.125,511.789,474.088 '• Does EFDM have the capability of forming better style representations (e.g., mod‐\\n'>\n",
      "<LTTextBoxHorizontal(5) 152.466,452.170,282.189,462.133 'eling the lighting as style [2])?\\n'>\n",
      "<LTTextBoxHorizontal(6) 113.633,416.476,200.179,430.822 '3 Methodology\\n'>\n",
      "<LTTextBoxHorizontal(7) 127.081,201.609,511.787,402.855 'The paper proposes a novel feature distribution matching strategy, namely EFDM via\\nthe Sort‐Matching algorithm. We mainly focused on implementing the functionality of\\nEFDM within the details described in the paper. For reproducing the results presented\\nin the original paper, we have used the functional version of EFDM and the training\\npipelines of both AST and DG, as given in the official GitHub repository. The overall\\ntraining scheme of AST and the usage of EFDM instead of the common normalization\\nmethods (e.g., AdaIN [4]) can be seen in Figure 1. For our additional experiments on\\nforming style representations by EFDM, we have implemented the modular version of\\nEFDM as a layer to replace it with the common normalization modules.\\nWe found that the paper is well‐written and easy to follow. With the details given as sup‐\\nplementary material, the paper contains the important details required to reproduce all\\nqualitative and quantitative results. However, the scripts provided in the official reposi‐\\ntory for t‐SNE visualizations of higher‐order statistics in the feature space does not work\\nproperly, and we could not achieve to fix it.\\nIn this section, we introduce the implementation details of EFDM and further proposed\\nfeature augmentation strategy, namely EFDMix. We present the important points for the\\nreproduction of this study, the hyperparameters we used, and our experimental setup.\\n'>\n",
      "<LTTextBoxHorizontal(8) 107.758,171.732,213.613,183.687 '3.1 Proposed Strategy\\n'>\n",
      "<LTTextBoxHorizontal(9) 127.180,105.407,510.455,163.191 'This study proposes to apply EFDM to tasks of AST and DG by exactly matching eCDFs\\nwith exact histogram matching via the Sort‐Matching algorithm in feature space. Given\\nthe input vector X ∈ RB×C×HW and the style vector Y ∈ RB×C×HW , EFDM can be\\napplied by exact histogram matching in a channel‐wise manner where B, C, H, W refer\\nto the batch size, number of channels, height, and width, respectively. First, the values\\n'>\n",
      "<LTTextBoxHorizontal(10) 127.559,42.726,231.896,49.700 'ReScience C 9.2 (#2) – Erkol et al. 2023\\n'>\n",
      "<LTTextBoxHorizontal(11) 506.954,42.726,510.239,49.700 '3\\n'>\n",
      "<LTFigure(Im0) 127.559,616.254,510.237,756.853 matrix=[382.68,0.00,0.00,140.60, (127.56,616.25)]>\n",
      "<LTTextBoxHorizontal(0) 127.559,782.258,375.715,789.232 '[Re] Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization\\n'>\n",
      "<LTTextBoxHorizontal(1) 127.051,681.991,377.614,753.812 'Algorithm 1 PyTorch‐like pseudo‐code for EFDM.\\nX: input vector, Y: target vector\\n_, IndexX = torch.sort(X)\\nSortedY, _ = torch.sort(Y)\\nInverseIndex = IndexX.argsort(−1)\\nreturn X+ SortedY.gather(−1, InverseIndex) −X.detach()\\n'>\n",
      "<LTTextBoxHorizontal(2) 127.260,480.350,512.329,657.685 'in X and Y are sorted in ascending order. To obtain the required output, the sorted\\nvalues of X are replaced with the values of sorted Y in corresponding positions, then\\nit returns the unsorted values of X whose elements are replaced with the values of Y.\\nIn this way, the output will share the identical feature distribution to Y. Note that it\\nrequires applying the stop‐gradient operation [13] to the style features, as practiced in\\nthe previous studies [4, 9], to ensure the flow of the gradients during back‐propagation\\nin deep models. The steps applied in practice are presented in Algorithm 1.\\nThe proposed strategy does not introduce any additional parameters and can be used in\\na plug‐and‐play manner with few lines of code and minimal cost. It is important to note\\nthat the Sort‐Matching algorithm assumes that two vectors (i.e., X and Y) should have\\nthe same number of dimensions in order to be directly applicable to this algorithm.\\nTo extend this strategy for feature augmentation in DG, the authors introduce a style\\nmixing method in feature space by interpolating the sorted vectors used in EFDM. This\\nmethod is named as Exact Feature Distribution Mixing (EFDMix) in the paper. The main\\ndifference between EFDM and EFDMix is described in the following equation.\\n'>\n",
      "<LTTextBoxHorizontal(3) 249.174,456.282,388.117,469.093 'O = Xu + (1 − λ)Ys − (1 − λ)Xd\\n'>\n",
      "<LTTextBoxHorizontal(4) 499.546,456.440,510.873,466.403 '(1)\\n'>\n",
      "<LTTextBoxHorizontal(5) 127.180,414.920,510.245,448.793 'where O stands for the required output, Xu is an unsorted input vector, Ys is a sorted style\\nvector, Xd refers to the gradient‐stop operation applied to Xu, and λ is the instance‐wise\\nmixing coefficient, which is sampled from Beta(α, α) where α ∈ (0, ∞).\\n'>\n",
      "<LTTextBoxHorizontal(6) 107.758,385.144,218.909,397.099 '3.2 Architecture Design\\n'>\n",
      "<LTTextBoxHorizontal(7) 127.230,270.999,511.787,376.604 'A lightweight encoder‐decoder architecture is employed for AST task where the encoder\\nf is composed of the first 4 blocks of a pre‐trained VGG‐19 [14]. The decoder part is\\ndesigned as a custom convolutional network that contains 4 convolutional blocks fol‐\\nlowing ReLU activations [15]. Given the content images Ic and the style images Is, both\\nimages are encoded into the feature space by using f . Note that the weights of f are\\nfixed, and not trained during the experiments. EFDM is applied to these features in or‐\\nder to extract a new feature vector of the content images whose distribution is matched\\nto the distribution of the style images. To summarize, the content features from the\\ndistribution of the style features S can be extracted by Equation 2.\\n'>\n",
      "<LTTextBoxHorizontal(8) 263.092,246.932,374.686,257.809 'S = EF DM (f (Ic), f (Is))\\n'>\n",
      "<LTTextBoxHorizontal(9) 499.546,247.089,510.873,257.052 '(2)\\n'>\n",
      "<LTTextBoxHorizontal(10) 127.559,217.367,510.241,240.200 'Decoder network g is responsible for projecting new stylized features S into the image\\nspace. The final output Io can be generated by Equation 3.\\n'>\n",
      "<LTTextBoxHorizontal(11) 298.080,193.457,339.706,204.334 'Io = g(S)\\n'>\n",
      "<LTTextBoxHorizontal(12) 499.546,193.614,510.873,203.577 '(3)\\n'>\n",
      "<LTTextBoxHorizontal(13) 127.180,152.094,510.234,186.724 'During the optimization of the weights of g, as a common practice in AST literature, the\\nweighted combination of the content loss Lc and the style loss Ls is used, as shown in\\nEquation 4.\\n'>\n",
      "<LTTextBoxHorizontal(14) 127.081,98.462,510.236,121.294 'Where ω is the balancing term for two components. The content loss refers to a simple\\nEuclidean distance between the content images Ic and the final output Io. The style loss\\n'>\n",
      "<LTTextBoxHorizontal(15) 288.423,128.026,348.873,140.837 'L = Lc + ωLs\\n'>\n",
      "<LTTextBoxHorizontal(16) 499.546,128.184,510.873,138.147 '(4)\\n'>\n",
      "<LTTextBoxHorizontal(17) 127.559,42.726,231.896,49.700 'ReScience C 9.2 (#2) – Erkol et al. 2023\\n'>\n",
      "<LTTextBoxHorizontal(18) 506.954,42.726,510.239,49.700 '4\\n'>\n",
      "<LTLine 127.559,756.452,510.236,756.452>\n",
      "<LTLine 127.559,741.827,510.236,741.827>\n",
      "<LTLine 127.559,680.069,510.236,680.069>\n",
      "<LTTextBoxHorizontal(0) 127.559,782.258,375.715,789.232 '[Re] Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization\\n'>\n",
      "<LTTextBoxHorizontal(1) 127.559,528.528,510.244,548.453 'Figure 2. Visual comparison of the reported and our produced results on standard [4] (the first two\\nrows) and photo‐realistic [17] (the last two rows) style transfer.\\n'>\n",
      "<LTTextBoxHorizontal(2) 127.559,399.442,511.787,505.803 'calculates the distribution divergence between VGG features of the style images Is and\\nthe final output Io.\\nFor DG task, the original paper follows the prior work [9], and the only difference is to\\nchange the feature augmentation method used during training (i.e., using EFDMix, in‐\\nstead of MixStyle [9]). ResNet‐18 and ResNet‐50 [16] are picked as a backbone network,\\nand started to train these networks with pre‐trained weights. There are two different\\nsettings in DG. The first is the leave‐one‐domain‐out setting that trains the model on\\nthree domains and tests on the remaining one, and the latter is the single source gener‐\\nalization training on a single domain and testing on the remaining three domains.\\n'>\n",
      "<LTTextBoxHorizontal(3) 107.758,369.564,168.601,381.519 '3.3 Datasets\\n'>\n",
      "<LTTextBoxHorizontal(4) 127.559,243.464,511.787,361.024 'During our reproduction study, we used the same datasets and the same settings as\\nmentioned in the original paper. The AST task is trained with the training set of MS‐\\nCOCO [18] for the content images and WikiArt [19] for the style images. The training set\\nof MS‐COCO dataset contains 118K unique images, while WikiArt contains 42K images\\nfor training and 10K images for testing, collected from the artworks of 195 artists. For\\nDG task, PACS dataset [20] is employed for domain generalization performance on im‐\\nage classification. This dataset contains images from four different domains (i.e., Art\\nPainting with 1.670 samples, Cartoon with 2.048 samples, Sketch with 2.344 samples,\\nand Photo with 3.929 samples) with 7 shared categories. Moreover, Market1501 [21] and\\nGRID [22] datasets are used for domain generalization on instance retrieval.\\n'>\n",
      "<LTTextBoxHorizontal(5) 107.758,213.587,207.946,225.542 '3.4 Hyperparameters\\n'>\n",
      "<LTTextBoxHorizontal(6) 127.559,135.307,510.238,205.046 'In our reproduction study, we used Adam optimizer [23] during training with an initial\\nlearning rate of 1e−4, decay of 5e−5, and the batch size of 8. The details for optimization\\nare not available in the paper, and we have decided to use the default values as given in\\nthe official repository. In AST training, ω is set to 10 to adjust the content and style\\ntrade‐off in the objective function. For DG task, α is the parameter for Beta distribution\\nsampling, and is set to 0.1 during the experiments.\\n'>\n",
      "<LTTextBoxHorizontal(7) 127.559,42.726,231.896,49.700 'ReScience C 9.2 (#2) – Erkol et al. 2023\\n'>\n",
      "<LTTextBoxHorizontal(8) 506.954,42.726,510.239,49.700 '5\\n'>\n",
      "<LTFigure(Im1) 127.559,559.542,510.237,756.851 matrix=[382.68,0.00,0.00,197.31, (127.56,559.54)]>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 127.559,782.258,375.715,789.232 '[Re] Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization\\n'>\n",
      "<LTTextBoxHorizontal(1) 129.161,556.793,508.644,565.768 'Figure 3. Illustration of style interpolation between a single content image and four style images.\\n'>\n",
      "<LTTextBoxHorizontal(2) 127.335,511.971,510.471,542.854 'Table 1. Average running time of prior methods and proposed strategy used in AST on a 512px\\nimage. Note that the compared methods run on a single Tesla V100, while our measurement has\\nbeen done on a single RTX 2080Ti.\\n'>\n",
      "<LTTextBoxHorizontal(3) 133.594,477.330,504.210,499.647 'Method Gatys et al. [24] CMD [10] HM AdaIN [4] EFDM [1] EFDM (ours)\\n0.33\\nTime (s)\\n'>\n",
      "<LTTextBoxHorizontal(4) 462.517,477.330,490.064,487.293 '0.0043\\n'>\n",
      "<LTTextBoxHorizontal(5) 348.968,477.330,376.514,487.293 '0.0038\\n'>\n",
      "<LTTextBoxHorizontal(6) 402.158,477.330,429.705,487.293 '0.0039\\n'>\n",
      "<LTTextBoxHorizontal(7) 268.699,477.330,291.334,487.293 '19.84\\n'>\n",
      "<LTTextBoxHorizontal(8) 203.291,477.330,225.926,487.293 '25.61\\n'>\n",
      "<LTTextBoxHorizontal(9) 107.758,442.907,261.780,454.862 '3.5 Experimental setup and code\\n'>\n",
      "<LTTextBoxHorizontal(10) 127.081,316.807,512.335,434.367 'We have followed the same protocol described in the original paper for both AST and DG.\\nFor any missing information in the paper, we abided by the default values given in the\\nofficial repository. We present a qualitative comparison to evaluate the performance of\\nEFDM on AST. Following the original paper for DG task, the classification accuracy of the\\nproposed strategy is reported in two specified settings (i.e., leave‐one‐out generalization\\nand single source generalization) and also the retrieval accuracy in the cross‐dataset\\nsetting. For the additional experiments on modeling the lighting as the style, which is\\nextracted by EFDM, we have followed the same training pipeline as introduced in [2], just\\nreplacing AdaIN layers with EFDM layers. Our implementation and the trained weights\\nare available at the link1.\\n'>\n",
      "<LTTextBoxHorizontal(11) 107.758,286.930,259.700,298.885 '3.6 Computational requirements\\n'>\n",
      "<LTTextBoxHorizontal(12) 127.260,208.651,510.242,278.389 'The experiments have been conducted on a single NVIDIA RTX 2080Ti GPU. A single\\ntraining for AST task took approximately 12 hours, while all experiments for DG task has\\nbeen completed in a single day. These experiments do not require any other significant\\nresources, but GPU memory (i.e., ∼6GB for training of both tasks with the batch size of\\n8). The average running time of different methods used in AST to process a 512px image\\nis shown in Table 1.\\n'>\n",
      "<LTTextBoxHorizontal(13) 113.633,172.957,169.306,187.303 '4 Results\\n'>\n",
      "<LTTextBoxHorizontal(14) 127.081,125.462,512.335,159.336 'We have conducted all experiments by following the descriptions given in the paper.\\nIn general, we were mostly able to reproduce the qualitative results on AST and photo‐\\nrealistic style transfer, and quantitative results on DG. Reproduced results for both tasks\\n'>\n",
      "<LTTextBoxHorizontal(15) 138.460,107.747,310.337,117.084 '1https://anonymous.4open.science/r/efdm-pytorch-767F\\n'>\n",
      "<LTTextBoxHorizontal(16) 127.559,42.726,231.896,49.700 'ReScience C 9.2 (#2) – Erkol et al. 2023\\n'>\n",
      "<LTTextBoxHorizontal(17) 506.954,42.726,510.239,49.700 '6\\n'>\n",
      "<LTFigure(Im2) 175.394,576.857,462.403,756.851 matrix=[287.01,0.00,0.00,179.99, (175.39,576.86)]>\n",
      "<LTLine 127.617,500.942,510.179,500.942>\n",
      "<LTLine 175.328,488.788,175.328,500.743>\n",
      "<LTLine 127.617,488.588,510.179,488.588>\n",
      "<LTLine 175.328,476.434,175.328,488.389>\n",
      "<LTLine 127.617,476.235,510.179,476.235>\n",
      "<LTLine 127.559,119.455,280.628,119.455>\n",
      "<LTTextBoxHorizontal(0) 127.559,782.258,375.715,789.232 '[Re] Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization\\n'>\n",
      "<LTTextBoxHorizontal(1) 147.849,631.857,489.946,641.514 'Figure 4. Illustration of the content‐style trade‐off with different λ values in Equation 1.\\n'>\n",
      "<LTTextBoxHorizontal(2) 127.559,480.862,510.244,500.787 'Figure 5. Comparison of reproduced results of different feature distribution matching strategies\\napplied in the original paper.\\n'>\n",
      "<LTTextBoxHorizontal(3) 127.559,411.552,512.335,457.380 'support the claims made in the original paper. We can state that the overall performance\\nseems robust to the changes in different hyper‐parameters used for EFDM and EFDMix.\\nLastly, EFDM has the capacity to better represent the style information in the cases of\\nmodeling the lighting as style.\\n'>\n",
      "<LTTextBoxHorizontal(4) 107.758,381.674,286.778,393.629 '4.1 Results reproducing original paper\\n'>\n",
      "<LTTextBoxHorizontal(5) 127.240,301.801,510.538,371.539 'Qualitative comparison on AST — As shown in Figure 2, we were able to reproduce the AST\\n(the first two rows) and photo‐realistic style transfer (the last two rows) results of AdaIN\\nand EFDM reported in the original study. Although there could be minor differences\\nin the corresponding outputs, depending on the optimization process, our reproduced\\nmodels have similar behaviors on the same stylistic changes. Therefore, we can validate\\nour first claim, EFDM works stably on AST and photo‐realistic style transfer scenarios.\\n'>\n",
      "<LTTextBoxHorizontal(6) 127.559,250.555,511.737,284.429 'Mixing multiple styles — Figure 3 demonstrates the validation of our second claim. It is\\npossible to blend more than one style information, instead of matching to a single one,\\nto obtain novel styles by linearly combining their feature distributions.\\n'>\n",
      "<LTTextBoxHorizontal(7) 127.559,187.355,511.779,233.183 'Partial utilization of style information — The paper points out that the formula of EFDMix,\\ngiven in Equation 1, enables adjusting the amount of style information utilized during\\nstyle transfer. Figure 4 illustrates that we were able to reproduce the content‐style trade‐\\noff experiment conducted in the original study.\\n'>\n",
      "<LTTextBoxHorizontal(8) 127.559,112.199,511.787,169.983 'EFDM versus different order of statistics — Following the ablation on AST in the original\\nstudy, we present our reproduced results in Figure 5, where different feature distribu‐\\ntion matching strategies are employed during AST training. AdaMean matches the dom‐\\ninant color scheme, while AdaStd tends to preserve the global structure more, instead of\\nthe stylistic details. AdaIN, by definition, can combine the behaviors of AdaMean and\\n'>\n",
      "<LTTextBoxHorizontal(9) 127.559,42.726,231.896,49.700 'ReScience C 9.2 (#2) – Erkol et al. 2023\\n'>\n",
      "<LTTextBoxHorizontal(10) 506.954,42.726,510.239,49.700 '7\\n'>\n",
      "<LTFigure(Im3) 165.826,651.921,471.970,756.850 matrix=[306.14,0.00,0.00,104.93, (165.83,651.92)]>\n",
      "<LTFigure(Im4) 165.826,511.875,471.970,619.042 matrix=[306.14,0.00,0.00,107.17, (165.83,511.88)]>\n",
      "<LTTextBoxHorizontal(0) 127.559,782.258,375.715,789.232 '[Re] Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization\\n'>\n",
      "<LTTextBoxHorizontal(1) 141.893,746.749,495.688,755.724 'Table 2. DG results of category classification on PACS. (R) refers to our reproduced results.\\n'>\n",
      "<LTTextBoxHorizontal(2) 152.102,725.552,183.093,734.575 'Method\\n'>\n",
      "<LTTextBoxHorizontal(3) 152.102,605.375,260.681,712.200 'R‐18 w/ MixStyle [9]\\nR‐18 w/ EFDMix [1]\\nR‐18 w/ EFDMix (R)\\nR‐18 w/ EFDMix (R) α = 0.5\\nR‐18 w/ EFDMix (R) α = 1.0\\nR‐50 w/ MixStyle [9]\\nR‐50 w/ EFDMix [1]\\nR‐50 w/ EFDMix (R)\\nR‐50 w/ EFDMix (R) α = 0.5\\nR‐50 w/ EFDMix (R) α = 1.0\\n'>\n",
      "<LTTextBoxHorizontal(4) 282.749,725.552,295.660,734.575 'Art\\n'>\n",
      "<LTTextBoxHorizontal(5) 318.433,725.552,350.967,734.575 'Cartoon\\n'>\n",
      "<LTTextBoxHorizontal(6) 243.492,605.375,397.536,734.575 'Photo\\nLeave‐one‐domain‐out generalization\\n95.9±0.4\\n96.8±0.4\\n94.1±0.9\\n94.2±1.3\\n94.1±1.3\\n97.7±0.4\\n98.1±0.2\\n94.3±2.2\\n94.5±1.7\\n94.7±1.6\\n'>\n",
      "<LTTextBoxHorizontal(7) 317.358,605.375,352.038,714.636 '78.6±0.9\\n79.4±0.7\\n78.1±0.6\\n78.2±1.0\\n78.1±0.9\\n82.3±0.7\\n82.5±0.7\\n81.8±1.6\\n81.1±1.3\\n81.6±1.4\\n'>\n",
      "<LTTextBoxHorizontal(8) 271.869,605.375,306.541,714.636 '83.1±0.8\\n83.9±0.4\\n80.6±1.5\\n80.7±1.8\\n80.9±1.4\\n90.3±0.3\\n90.6±0.3\\n87.4±1.6\\n87.6±1.7\\n87.4±2.1\\n'>\n",
      "<LTTextBoxHorizontal(9) 152.102,485.196,260.681,592.021 'R‐18 w/ MixStyle [9]\\nR‐18 w/ EFDMix [1]\\nR‐18 w/ EFDMix (R)\\nR‐18 w/ EFDMix (R) α = 0.5\\nR‐18 w/ EFDMix (R) α = 1.0\\nR‐50 w/ MixStyle [9]\\nR‐50 w/ EFDMix [1]\\nR‐50 w/ EFDMix (R)\\nR‐50 w/ EFDMix (R) α = 0.5\\nR‐50 w/ EFDMix (R) α = 1.0\\n'>\n",
      "<LTTextBoxHorizontal(10) 262.241,594.187,375.561,603.209 'Single source generalization\\n'>\n",
      "<LTTextBoxHorizontal(11) 271.869,485.196,306.541,594.457 '61.9±2.2\\n63.2±2.3\\n63.5±3.4\\n63.8±2.4\\n63.7±3.4\\n73.2±1.1\\n75.3±0.9\\n73.0±2.2\\n73.8±1.6\\n73.7±1.2\\n'>\n",
      "<LTTextBoxHorizontal(12) 317.358,485.196,352.038,594.457 '71.5±0.8\\n73.9±0.7\\n72.9±1.2\\n73.2±0.9\\n73.2±0.9\\n74.8±1.1\\n77.4±0.8\\n77.2±0.9\\n77.6±1.3\\n77.8±0.4\\n'>\n",
      "<LTTextBoxHorizontal(13) 362.865,485.196,397.536,594.457 '41.2±1.8\\n42.5±1.8\\n41.9±1.4\\n42.5±1.6\\n41.9±1.8\\n46.0±2.0\\n48.0±0.9\\n48.3±1.2\\n47.9±1.2\\n47.9±0.7\\n'>\n",
      "<LTTextBoxHorizontal(14) 412.418,725.552,438.971,734.575 'Sketch\\n'>\n",
      "<LTTextBoxHorizontal(15) 453.858,725.552,485.697,734.575 'Average\\n'>\n",
      "<LTTextBoxHorizontal(16) 408.362,605.375,443.034,714.636 '74.2±2.7\\n75.0±0.7\\n72.3±1.2\\n71.4±1.9\\n71.4±2.1\\n74.7±0.7\\n76.4±1.2\\n73.7±1.7\\n73.9±1.5\\n74.3±1.6\\n'>\n",
      "<LTTextBoxHorizontal(17) 408.362,485.196,443.034,594.457 '32.2±4.1\\n38.1±3.7\\n36.3±3.1\\n37.1±3.0\\n36.3±2.4\\n40.6±2.0\\n44.2±2.4\\n47.7±2.7\\n46.7±3.2\\n46.0±4.2\\n'>\n",
      "<LTTextBoxHorizontal(18) 461.746,605.375,477.797,712.200 '82.9\\n83.9\\n81.3\\n81.3\\n81.1\\n86.2\\n86.9\\n84.3\\n84.3\\n84.5\\n'>\n",
      "<LTTextBoxHorizontal(19) 461.746,485.196,477.797,592.021 '51.7\\n54.4\\n53.7\\n54.2\\n53.7\\n58.6\\n61.2\\n61.6\\n61.5\\n61.4\\n'>\n",
      "<LTTextBoxHorizontal(20) 167.973,462.328,469.601,471.303 'Table 3. DG results on person re‐ID task. (R) refers to our reproduced results.\\n'>\n",
      "<LTTextBoxHorizontal(21) 132.105,438.553,161.243,446.129 'Methods\\n'>\n",
      "<LTTextBoxHorizontal(22) 132.105,406.429,199.435,432.189 'OSNet + MixStyle\\nOSNet + EFDMix\\nOSNet + EFDMix (R)\\n'>\n",
      "<LTTextBoxHorizontal(23) 247.943,443.099,317.205,452.721 'MarKet1501 → GRID\\n'>\n",
      "<LTTextBoxHorizontal(24) 401.068,443.099,470.323,452.721 'GRID → MarKet1501\\n'>\n",
      "<LTTextBoxHorizontal(25) 208.837,406.429,237.952,441.583 'mAP\\n33.8±0.9\\n35.5±1.8\\n35.0±2.6\\n'>\n",
      "<LTTextBoxHorizontal(26) 247.036,406.429,279.892,441.583 'R1\\n24.89±1.6\\n26.7±3.3\\n25.1±2.3\\n'>\n",
      "<LTTextBoxHorizontal(27) 288.976,406.429,318.098,441.583 'R5\\n43.7±2.0\\n44.4±0.8\\n45.6±4.1\\n'>\n",
      "<LTTextBoxHorizontal(28) 327.182,406.429,356.303,441.583 'R10\\n53.1±1.6\\n53.6±2.0\\n52.0±2.9\\n'>\n",
      "<LTTextBoxHorizontal(29) 365.698,406.429,391.077,441.583 'mAP\\n4.9±0.2\\n6.4±0.2\\n6.2±0.7\\n'>\n",
      "<LTTextBoxHorizontal(30) 400.161,406.429,429.282,441.583 'R1\\n15.4±1.2\\n19.9±0.6\\n18.8±1.8\\n'>\n",
      "<LTTextBoxHorizontal(31) 438.374,406.429,467.487,441.583 'R5\\n28.4±1.3\\n34.4±1.0\\n33.6±2.7\\n'>\n",
      "<LTTextBoxHorizontal(32) 476.579,406.429,505.693,441.583 'R10\\n35.7±0.9\\n42.2±0.8\\n41.4±2.9\\n'>\n",
      "<LTTextBoxHorizontal(33) 127.230,361.214,510.451,383.132 'AdaStd. EFDM can effectively preserve the content details with the help of higher‐order\\nfeature statistics.\\n'>\n",
      "<LTTextBoxHorizontal(34) 127.081,250.193,512.329,343.842 'Feature augmentation method via EFDM on DG — We present the domain generalization re‐\\nsults of category classification in Table 2 and cross‐domain instance retrieval in Table 3.\\nWe only report the results of the latest state‐of‐the‐art [9], the original study [1], and our\\nreproduction. We were able to reproduce the reported results of single source general‐\\nization experiments, while we could partially achieve to reproduce the reported results\\non the leave‐one‐domain‐out generalization. Moreover, the original study claims that\\nEFDMix outperforms the latest feature augmentation strategy for DG on cross‐domain\\nperson re‐identification, and our reproduced results can validate this claim.\\n'>\n",
      "<LTTextBoxHorizontal(35) 107.758,220.315,264.972,232.270 '4.2 Results beyond original paper\\n'>\n",
      "<LTTextBoxHorizontal(36) 127.230,164.352,510.452,210.181 'Trade-off between content and style loss terms — We investigate how much EFDM is robust to\\nthe weighting of two components in the objective function. As previously induced for\\nAdaIN [4], the model inevitably starts to vanish the content details when the weight of\\nstyle loss term is increased.\\n'>\n",
      "<LTTextBoxHorizontal(37) 127.559,113.107,511.783,146.980 'Modifying the instance-wise mixing coefficient — As shown in Table 2, the range parameters\\nα of the distribution of the mixing coefficient in EFDMix does not have significant im‐\\npact on DG results of category classification. This was expected since the method still\\n'>\n",
      "<LTTextBoxHorizontal(38) 127.559,42.726,231.896,49.700 'ReScience C 9.2 (#2) – Erkol et al. 2023\\n'>\n",
      "<LTTextBoxHorizontal(39) 506.954,42.726,510.239,49.700 '8\\n'>\n",
      "<LTLine 146.689,735.748,491.105,735.748>\n",
      "<LTLine 266.275,724.740,266.275,735.567>\n",
      "<LTLine 146.689,724.560,491.105,724.560>\n",
      "<LTLine 146.689,713.372,491.105,713.372>\n",
      "<LTLine 266.275,702.365,266.275,713.192>\n",
      "<LTLine 266.275,691.538,266.275,702.365>\n",
      "<LTLine 266.275,680.711,266.275,691.538>\n",
      "<LTLine 266.275,669.884,266.275,680.711>\n",
      "<LTLine 266.275,659.057,266.275,669.884>\n",
      "<LTLine 146.689,658.877,491.105,658.877>\n",
      "<LTLine 266.275,647.870,266.275,658.697>\n",
      "<LTLine 266.275,637.042,266.275,647.870>\n",
      "<LTLine 266.275,626.216,266.275,637.042>\n",
      "<LTLine 266.275,615.389,266.275,626.216>\n",
      "<LTLine 266.275,604.562,266.275,615.389>\n",
      "<LTLine 146.689,604.382,491.105,604.382>\n",
      "<LTLine 146.689,593.194,491.105,593.194>\n",
      "<LTLine 266.275,582.187,266.275,593.014>\n",
      "<LTLine 266.275,571.360,266.275,582.187>\n",
      "<LTLine 266.275,560.533,266.275,571.360>\n",
      "<LTLine 266.275,549.706,266.275,560.533>\n",
      "<LTLine 266.275,538.880,266.275,549.706>\n",
      "<LTLine 146.689,538.699,491.105,538.699>\n",
      "<LTLine 266.275,527.692,266.275,538.518>\n",
      "<LTLine 266.275,516.865,266.275,527.692>\n",
      "<LTLine 266.275,506.038,266.275,516.865>\n",
      "<LTLine 266.275,495.212,266.275,506.038>\n",
      "<LTLine 266.275,484.385,266.275,495.212>\n",
      "<LTLine 204.140,442.417,204.140,451.508>\n",
      "<LTLine 361.000,442.417,361.000,451.508>\n",
      "<LTLine 204.140,433.325,204.140,442.417>\n",
      "<LTLine 361.000,433.325,361.000,442.417>\n",
      "<LTLine 127.559,433.174,510.239,433.174>\n",
      "<LTLine 204.140,423.930,204.140,433.022>\n",
      "<LTLine 361.000,423.930,361.000,433.022>\n",
      "<LTLine 204.140,414.839,204.140,423.930>\n",
      "<LTLine 361.000,414.839,361.000,423.930>\n",
      "<LTLine 204.140,405.747,204.140,414.839>\n",
      "<LTLine 361.000,405.747,361.000,414.839>\n",
      "<LTTextBoxHorizontal(0) 127.559,782.258,375.715,789.232 '[Re] Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization\\n'>\n",
      "<LTTextBoxHorizontal(1) 177.813,682.563,459.992,691.538 'Figure 6. Ablation on the trade‐off between content and style loss terms.\\n'>\n",
      "<LTTextBoxHorizontal(2) 127.335,648.699,510.231,668.624 'Table 4. White‐balance correction results of the recent methods [2] and its variant with EFDM on\\nmixed‐illuminant evaluation set [25].\\n'>\n",
      "<LTTextBoxHorizontal(3) 152.317,621.827,184.544,631.209 'Method\\n'>\n",
      "<LTTextBoxHorizontal(4) 152.317,582.047,236.520,625.579 'Mean\\n822.77\\nStyleWB [2]\\nSR + AdaIN 818.99\\nSR + EFDM 761.05\\n'>\n",
      "<LTTextBoxHorizontal(5) 268.765,627.455,294.556,639.370 'MSE ↓\\n'>\n",
      "<LTTextBoxHorizontal(6) 247.778,582.047,273.719,625.579 'Q1\\n572.52\\n527.34\\n513.96\\n'>\n",
      "<LTTextBoxHorizontal(7) 284.977,582.047,310.918,625.579 'Q2\\n840.67\\n875.56\\n818.39\\n'>\n",
      "<LTTextBoxHorizontal(8) 322.176,582.047,352.742,625.579 'Q3\\n1025.26\\n1049.03\\n969.33\\n'>\n",
      "<LTTextBoxHorizontal(9) 364.377,582.047,387.757,625.579 'Mean\\n11.65\\n11.01\\n10.16\\n'>\n",
      "<LTTextBoxHorizontal(10) 399.006,582.047,452.895,639.370 '∆E 2000↓\\nQ2\\nQ1\\n11.86\\n10.63\\n11.41\\n8.64\\n9.81\\n8.75\\n'>\n",
      "<LTTextBoxHorizontal(11) 464.153,582.047,485.469,625.579 'Q3\\n13.02\\n12.31\\n11.69\\n'>\n",
      "<LTTextBoxHorizontal(12) 127.559,536.670,510.452,558.588 'mixes the distributions, and intuitively modifying its coefficient just makes it another\\ndistribution to be matched.\\n'>\n",
      "<LTTextBoxHorizontal(13) 127.230,413.694,511.787,519.298 'Forming better style representation — We further investigate the impact of using EFDM in‐\\nstead of AdaIN on a different domain. The approach [2] proposes to model the lighting as\\nstyle to provide white‐balance correction. This approach assumes that the illuminations\\nin the scene basically stands for the additional style information injected to the scene,\\nand tries to normalize this information in adaptive manner. In practice, we replaced\\nAdaIN layers in this method with EFDM layers, which are implemented by us, and re‐\\npeated the same experiment on mixed‐illuminant evaluation set [25], as described in\\n[2]. Table 4 demonstrates that EFDM forms better style representations to be utilized by\\nproposed style removal model to remove the illumination.\\n'>\n",
      "<LTTextBoxHorizontal(14) 113.633,378.000,188.215,392.346 '5 Discussion\\n'>\n",
      "<LTTextBoxHorizontal(15) 127.081,270.829,510.491,364.479 'We can clearly say that the paper was well‐written. Although there are some parts that\\nwe struggled in the official repository, we were able to run all necessary experiments\\nrequiring to reproduce this study. Overall, the reproduced results are similar to the\\nreported results in the paper. As an exception, we could partially achieve to obtain\\ncomparable results on DG for category classification. In addition to this, we present the\\nperformance of the proposed method when some essential hyperparameters are slightly\\nmodified. Lastly, we extend the experiments to a different task in order to observe the\\nimpact of EFDM on forming style representations.\\n'>\n",
      "<LTTextBoxHorizontal(16) 107.758,240.952,196.684,252.907 '5.1 What was easy\\n'>\n",
      "<LTTextBoxHorizontal(17) 127.260,186.583,510.242,232.411 'The given code in the original repository was easy to follow, and it was well‐written in\\ngeneral. The authors designed the documentation and the source code in a way that\\nanyone who has fundamental knowledge of Python could run the experiments, or even\\ngenerate their own stylized image from any content.\\n'>\n",
      "<LTTextBoxHorizontal(18) 107.758,156.705,210.958,168.660 '5.2 What was difficult\\n'>\n",
      "<LTTextBoxHorizontal(19) 127.081,102.336,510.242,148.165 'We would like to add the reproduced outputs by Histogram Matching (HM) along with\\nthe others, however the training of HM was based on CPU and the estimated time to\\ncomplete a single training was around 15 days in our setup. Consequently, we could not\\ninclude the reproduced outputs by HM to this report. Moreover, it could not be possible\\n'>\n",
      "<LTTextBoxHorizontal(20) 127.559,42.726,231.896,49.700 'ReScience C 9.2 (#2) – Erkol et al. 2023\\n'>\n",
      "<LTTextBoxHorizontal(21) 506.954,42.726,510.239,49.700 '9\\n'>\n",
      "<LTFigure(Im5) 127.559,702.627,510.237,756.850 matrix=[382.68,0.00,0.00,54.22, (127.56,702.63)]>\n",
      "<LTLine 204.762,626.612,204.762,637.870>\n",
      "<LTLine 358.560,626.612,358.560,637.870>\n",
      "<LTLine 204.762,615.353,204.762,626.612>\n",
      "<LTLine 358.560,615.353,358.560,626.612>\n",
      "<LTLine 146.688,615.165,491.108,615.165>\n",
      "<LTLine 204.762,603.720,204.762,614.978>\n",
      "<LTLine 358.560,603.720,358.560,614.978>\n",
      "<LTLine 204.762,592.461,204.762,603.720>\n",
      "<LTLine 358.560,592.461,358.560,603.720>\n",
      "<LTLine 204.762,581.203,204.762,592.461>\n",
      "<LTLine 358.560,581.203,358.560,592.461>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(0) 127.559,782.258,375.715,789.232 '[Re] Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization\\n'>\n",
      "<LTTextBoxHorizontal(1) 127.559,732.243,510.232,754.161 'to add t‐SNE visualizations to this report, as in the original paper, due to the lack of\\nclarity in the documentation of its script.\\n'>\n",
      "<LTTextBoxHorizontal(2) 107.758,702.365,298.757,714.320 '5.3 Communication with original authors\\n'>\n",
      "<LTTextBoxHorizontal(3) 127.081,671.907,510.233,693.825 'We were in contact with the authors, and asked for the original results as JPEG files to\\nprepare the figures in this report.\\n'>\n",
      "<LTTextBoxHorizontal(4) 127.559,636.213,189.922,650.559 'References\\n'>\n",
      "<LTTextBoxHorizontal(5) 131.841,591.571,137.992,600.538 '2.\\n'>\n",
      "<LTTextBoxHorizontal(6) 131.841,558.694,137.992,567.661 '3.\\n'>\n",
      "<LTTextBoxHorizontal(7) 131.841,613.489,137.992,622.456 '1.\\n'>\n",
      "<LTTextBoxHorizontal(8) 131.841,471.023,511.564,622.456 'Y. Zhang, M. Li, R. Li, K. Jia, and L. Zhang. “Exact Feature Distribution Matching for Arbitrary Style Transfer and\\nDomain Generalization.” In: CVPR. 2022.\\nF. Kınlı, D. Yılmaz, B. Özcan, and F. Kıraç. “Modeling the Lighting in Scenes as Style for Auto White-Balance\\nCorrection.” In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023,\\npp. 4903–4913.\\nA. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer.\\n“Automatic differentiation in PyTorch.” In: (2017).\\nX. Huang and S. Belongie. “Arbitrary style transfer in real-time with adaptive instance normalization.” In: Pro-\\nceedings of the IEEE international conference on computer vision. 2017, pp. 1501–1510.\\nY. Li, N. Wang, J. Liu, and X. Hou. “Demystifying neural style transfer.” In: Proceedings of the 26th International\\nJoint Conference on Artificial Intelligence. 2017, pp. 2230–2236.\\nP. Li, L. Zhao, D. Xu, and D. Lu. “Optimal transport of deep feature for image style transfer.” In: Proceedings of\\nthe 2019 4th International Conference on Multimedia Systems and Signal Processing. 2019, pp. 167–171.\\n7. M. Lu, H. Zhao, A. Yao, Y. Chen, F. Xu, and L. Zhang. “A closed-form solution to universal style transfer.” In:\\n'>\n",
      "<LTTextBoxHorizontal(9) 131.841,536.776,137.992,545.743 '4.\\n'>\n",
      "<LTTextBoxHorizontal(10) 131.841,514.859,137.992,523.826 '5.\\n'>\n",
      "<LTTextBoxHorizontal(11) 131.841,492.941,137.992,501.908 '6.\\n'>\n",
      "<LTTextBoxHorizontal(12) 131.841,449.105,137.992,458.072 '8.\\n'>\n",
      "<LTTextBoxHorizontal(13) 131.841,427.187,137.992,436.154 '9.\\n'>\n",
      "<LTTextBoxHorizontal(14) 147.950,416.229,511.583,469.031 'Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019, pp. 5952–5961.\\nY. Mroueh. “Wasserstein Style Transfer.” In: International Conference on Artificial Intelligence and Statistics.\\nPMLR. 2020, pp. 842–852.\\nK. Zhou, Y. Yang, Y. Qiao, and T. Xiang. “Domain Generalization with MixStyle.” In: International Conference on\\nLearning Representations. 2020.\\n'>\n",
      "<LTTextBoxHorizontal(15) 127.559,405.270,510.157,414.237 '10. N. Kalischek, J. D. Wegner, and K. Schindler. “In the light of feature distributions: moment matching for Neural\\n'>\n",
      "<LTTextBoxHorizontal(16) 127.620,262.804,137.994,271.771 '17.\\n'>\n",
      "<LTTextBoxHorizontal(17) 127.620,284.722,137.994,293.689 '16.\\n'>\n",
      "<LTTextBoxHorizontal(18) 127.620,306.639,137.994,315.606 '15.\\n'>\n",
      "<LTTextBoxHorizontal(19) 127.620,328.557,137.994,337.524 '14.\\n'>\n",
      "<LTTextBoxHorizontal(20) 127.620,350.475,137.994,359.442 '13.\\n'>\n",
      "<LTTextBoxHorizontal(21) 127.620,372.393,137.994,392.319 '11.\\n12.\\n'>\n",
      "<LTTextBoxHorizontal(22) 127.620,109.379,511.555,403.466 'Style Transfer.” In: (2021). arXiv:2103.07208 [cs.CV].\\nA. W. Van der Vaart. Asymptotic statistics. Vol. 3. Cambridge university press, 2000.\\nJ. P. Rolland, V. Vo, B. Bloss, and C. K. Abbey. “Fast algorithms for histogram matching: Application to texture\\nsynthesis.” In: Journal of Electronic Imaging 9.1 (2000), pp. 39–45.\\nX. Chen and K. He. “Exploring simple siamese representation learning.” In: Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition. 2021, pp. 15750–15758.\\nK. Simonyan and A. Zisserman. “Very deep convolutional networks for large-scale image recognition.” In: arXiv\\npreprint arXiv:1409.1556 (2014).\\nA. L. Maas, A. Y. Hannun, A. Y. Ng, et al. “Rectifier nonlinearities improve neural network acoustic models.” In:\\nProc. icml. Vol. 30. 1. Atlanta, Georgia, USA. 2013, p. 3.\\nK. He, X. Zhang, S. Ren, and J. Sun. “Deep residual learning for image recognition.” In: Proceedings of the IEEE\\nconference on computer vision and pattern recognition. 2016, pp. 770–778.\\nF. Luan, S. Paris, E. Shechtman, and K. Bala. “Deep photo style transfer.” In: Proceedings of the IEEE conference\\non computer vision and pattern recognition. 2017, pp. 4990–4998.\\nT.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. “Microsoft coco:\\nCommon objects in context.” In: European conference on computer vision. Springer. 2014, pp. 740–755.\\n19. W. R. Tan, C. S. Chan, H. Aguirre, and K. Tanaka. “Improved ArtGAN for Conditional Synthesis of Nat-\\nural Image and Artwork.” In: IEEE Transactions on Image Processing 28.1 (2019), pp. 394–409. DOI:\\n10.1109/TIP.2018.2866698. URL: https://doi.org/10.1109/TIP.2018.2866698.\\nD. Li, Y. Yang, Y.-Z. Song, and T. M. Hospedales. “Deeper, broader and artier domain generalization.” In: Pro-\\nceedings of the IEEE international conference on computer vision. 2017, pp. 5542–5550.\\nL. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian. “Scalable person re-identification: A benchmark.” In:\\nProceedings of the IEEE international conference on computer vision. 2015, pp. 1116–1124.\\nC. C. Loy, T. Xiang, and S. Gong. “Multi-camera activity correlation analysis.” In: 2009 IEEE Conference on\\nComputer Vision and Pattern Recognition. IEEE. 2009, pp. 1988–1995.\\nD. P. Kingma and J. Ba. “Adam: A method for stochastic optimization.” In: arXiv preprint arXiv:1412.6980\\n(2014).\\n'>\n",
      "<LTTextBoxHorizontal(23) 127.620,240.886,137.994,249.853 '18.\\n'>\n",
      "<LTTextBoxHorizontal(24) 127.620,164.174,137.994,173.141 '21.\\n'>\n",
      "<LTTextBoxHorizontal(25) 127.620,142.256,137.994,151.223 '22.\\n'>\n",
      "<LTTextBoxHorizontal(26) 127.620,120.338,137.994,129.305 '23.\\n'>\n",
      "<LTTextBoxHorizontal(27) 127.559,186.092,137.996,195.059 '20.\\n'>\n",
      "<LTTextBoxHorizontal(28) 127.559,42.726,231.896,49.700 'ReScience C 9.2 (#2) – Erkol et al. 2023\\n'>\n",
      "<LTTextBoxHorizontal(29) 503.623,42.726,510.241,49.700 '10\\n'>\n",
      "<LTTextBoxHorizontal(0) 127.559,782.258,375.715,789.232 '[Re] Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization\\n'>\n",
      "<LTTextBoxHorizontal(1) 127.620,744.458,137.994,753.425 '24.\\n'>\n",
      "<LTTextBoxHorizontal(2) 147.950,733.499,511.557,753.425 'L. A. Gatys, A. S. Ecker, and M. Bethge. “Image style transfer using convolutional neural networks.” In: Proceed-\\nings of the IEEE conference on computer vision and pattern recognition. 2016, pp. 2414–2423.\\n'>\n",
      "<LTTextBoxHorizontal(3) 127.620,700.622,511.053,731.507 '25. M. Afifi, M. A. Brubaker, and M. S. Brown. “Auto White-Balance Correction for Mixed-Illuminant Scenes.” In:\\nProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). Jan. 2022,\\npp. 1210–1219.\\n'>\n",
      "<LTTextBoxHorizontal(4) 127.559,42.726,231.896,49.700 'ReScience C 9.2 (#2) – Erkol et al. 2023\\n'>\n",
      "<LTTextBoxHorizontal(5) 503.671,42.726,510.240,49.700 '11\\n'>\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "\n",
    "for page_layout in extract_pages(\"RS_ALL_148\\\\RS_002_MLRC_2022_02.pdf\"):\n",
    "    for element in page_layout:\n",
    "        print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f8877fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We verify the authors’ claims by replicating the experiments presented in [1].\n",
      "All of our experiments show identical results to the ones presented in [1], apart from per‐\n",
      "turbation testing for which we provide an additional in depth analysis. We also provide\n",
      "an analysis of the new alpha hyperparameter and a hyperparameter search.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer\n",
    "\n",
    "import glob\n",
    "\n",
    "# https://github.com/pdfminer/pdfminer.six/blob/22f90521b823ac5a22785d1439a64c7bdf2c2c6d/pdfminer/high_level.py#L126\n",
    "for page_layout in extract_pages(\"RS_ALL_148\\\\RS_003_MLRC_2022_03.pdf\",maxpages=1):\n",
    "    for element in page_layout:\n",
    "        if isinstance(element, LTTextContainer):\n",
    "            el_text = element.get_text()\n",
    "            \n",
    "            if el_text.split(' ')[0].lower() == 'results':\n",
    "                print(el_text)\n",
    "                break\n",
    "            else:\n",
    "                print(\"NOT FOUND\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "293a5c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(13) 107.758,346.972,162.349,358.927 '1.3 Results\\n'>\n",
      "Overall we find the original results to be reproducible; transformation policies found\n",
      "using Gao et al.1’s method can defend against gradient reconstruction attacks, and these\n",
      "transformations have negligible impact on training efficiency and model accuracy. How‐\n",
      "ever we do not observe the reported correlation between the proposed privacy‐score Spri\n",
      "and reconstruction PSNR. We also find that the degree of protection differs greatly from\n",
      "image to image, with poor protection in the worst case.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for 2021 and 2020\n",
    "\n",
    "flag = 0\n",
    "for page_layout in extract_pages(\"RS_ALL_148\\\\RS_056_MLRC_2021_11.pdf\",maxpages=1):\n",
    "    \n",
    "    for element in page_layout:\n",
    "        if isinstance(element, LTTextContainer):\n",
    "            if flag == 1 : \n",
    "                print(element.get_text())\n",
    "                break\n",
    "\n",
    "            el_text = element.get_text()\n",
    "            \n",
    "            if 'results' in el_text.lower():\n",
    "                print(element)\n",
    "                flag = 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f014e2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(8) 301.332,608.970,336.464,617.937 'Abstract\\n'>\n",
      "We reproduce the work in Zero-shot Knowledge Transfer via Adversarial Belief\n",
      "Matching, which describes a novel approach for knowledge transfer. A teacher net-\n",
      "work trained on real samples distills knowledge to a student network that is trained\n",
      "solely on pseudo data extracted from a generator network, with the student trying\n",
      "to mimic the teacher’s outputs on these datapoints. To this end, we additionally\n",
      "re-implement Wide Residual Networks which are used as the main framework for\n",
      "both teacher and student networks and train them from scratch on CIFAR10 and\n",
      "SVHN. We compare the results of the proposed method with a few-shot knowledge\n",
      "distillation attention transfer setting implemented and trained from scratch. We\n",
      "suggest an approach for further exploitation of the learnt mechanics of the gener-\n",
      "ator network in the zero-shot setting, which operates on top of the main method,\n",
      "and brieﬂy discuss the beneﬁts and drawbacks of this approach. Our code can be\n",
      "found publicly available in https://github.com/AlexandrosFerles/NIPS_2019_Reproducibilty_\n",
      "Challenge_Zero-shot_Knowledge_Transfer_via_Adversarial_Belief_Matching.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flag = 0\n",
    "for page_layout in extract_pages(\"RS_ALL_148\\\\RS_124_NeurIPS_2019_07.pdf\",maxpages=1):\n",
    "    \n",
    "    for element in page_layout:\n",
    "        if isinstance(element, LTTextContainer):\n",
    "            if flag == 1 : \n",
    "                print(element.get_text())\n",
    "                break\n",
    "\n",
    "            el_text = element.get_text()\n",
    "            \n",
    "            if 'abstract' in el_text.lower():\n",
    "                print(element)\n",
    "                flag = 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "50f282b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0  ---  RS_001_MLRC_2022_01\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — Due to many missing implementation details, we were not able to reproduce all\n",
      "of the original results. Some claims can be supported by our results, but most results are\n",
      "very vague. Even though the new method outperforms the baselines in certain scenarios,\n",
      "we find that the superiority of the method is not as strong as presented in the original\n",
      "paper.\n",
      "\n",
      "1\n",
      "1  ---  RS_002_MLRC_2022_02\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We have reproduced the experiments done on two selected tasks, and com‐\n",
      "pared their results with the reported results. Although our experimental results are not\n",
      "identical to the reported ones, we can validate the claims made by the original study\n",
      "according to these results.\n",
      "\n",
      "2\n",
      "2  ---  RS_003_MLRC_2022_03\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We verify the authors’ claims by replicating the experiments presented in [1].\n",
      "All of our experiments show identical results to the ones presented in [1], apart from per‐\n",
      "turbation testing for which we provide an additional in depth analysis. We also provide\n",
      "an analysis of the new alpha hyperparameter and a hyperparameter search.\n",
      "\n",
      "3\n",
      "3  ---  RS_004_MLRC_2022_04\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We find that the three main claims of the paper hold true as we were able to\n",
      "successfully reproduce each corresponding experiment. Our results differ in some mi‐\n",
      "nor aspects, but they do support the validity of the three main claims made in the paper.\n",
      "Furthermore, we also demonstrated that the framework proposed is expandable in new\n",
      "contexts, thus providing further support for its utility and applicability.\n",
      "\n",
      "4\n",
      "4  ---  RS_005_MLRC_2022_05\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We only manage to partially reproduce the original scores. Nonetheless, the\n",
      "hypothesis formulated in the original paper are still corroborated.\n",
      "\n",
      "5\n",
      "5  ---  RS_006_MLRC_2022_06\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — The obtained linear evaluation accuracies differ in a range between 0.19% and\n",
      "2.05%, while the ones in the original paper differ from 0.20% to 0.80%. Nonetheless, we\n",
      "believe that our results support that the differences in top‐1 accuracy among different\n",
      "batch sizes are minimal because of different choices of the dataset, base encoder, and\n",
      "batch sizes, and also because the range substantially increases when the projection head\n",
      "is not deep. All the other experiments support the original and the newly tested claims.\n",
      "\n",
      "6\n",
      "6  ---  RS_007_MLRC_2022_07\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We managed to reproduce all the experiments supporting the main claims of\n",
      "the original article. Additionally, we add uncertainty quantification to the results as\n",
      "we believe this is a crucial part to confirm any of the claims. Finally, we present the\n",
      "exploration‐exploitation trade‐off experiment in a more robust manner leveraging the\n",
      "nucleotide diversity metric to gain additional insight into how the proposed algorithm\n",
      "works.\n",
      "\n",
      "7\n",
      "7  ---  RS_008_MLRC_2022_08\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — Our study confirms three (iii, iv and v) claims from [1]. However, it cannot fully\n",
      "support the other claims (i and ii). Using the configurations given in [1], we achieved\n",
      "comparable performances on the CARS196 dataset, but large deviances were observed\n",
      "on other datasets, which dropped by 1.5% and 1% R@1 using ResNet50. By implement‐\n",
      "ing Bayesian optimization, the performances are improved by 0.7% and 0.4% R@1 on\n",
      "CARS196 and CUB‐200‐2011. We also close the performance gap for SOP dataset from\n",
      "‐1% to ‐0.6% compared to the proposed results in [1] using ResNet50.\n",
      "\n",
      "8\n",
      "8  ---  RS_009_MLRC_2022_09\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — Our results confirm most of the claims tested, although we could not achieve\n",
      "the exact same accuracy due to missing hyper‐parameters. We reproduced the trend in\n",
      "how scaling the prior impacts the performance and how a learned prior outperforms a\n",
      "non‐learned prior. On contrary, we could not reproduce the effect of rank in low‐rank\n",
      "covariance approximation on model performance, as well as the beneficial boost in per‐\n",
      "formance of Bayesian learning compared to the standard SGD.\n",
      "\n",
      "9\n",
      "9  ---  RS_010_MLRC_2022_10\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — Our results support the claims made by the original authors. (i) We observe\n",
      "that CartoonX produces piece‐wise smooth explanations. Most of the explanations give\n",
      "valuable insights. (ii) Most experiments, that show how CartoonX achieves lower dis‐\n",
      "tortion outputs compared to other methods, have been reproduced. In the cases where\n",
      "exact reproducibility has not been achieved, claim (ii) of the author still holds. (iii) The\n",
      "model‐agnosticism claim still holds as the overall quality of the ViT‐based explanations\n",
      "almost matches that of the CNN‐based explanations. Finally, simple heuristical initial‐\n",
      "izations did not improve the runtime.\n",
      "\n",
      "10\n",
      "10  ---  RS_011_MLRC_2022_11\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We reproduced the original paper’s work through our experiments. We find\n",
      "that the main claims of the paper largely hold. We assess the robustness and general‐\n",
      "izability of some of the claims, through our additional experiments. In that case, we\n",
      "find that one claim is not generalizable and another is not reproducible for the graph\n",
      "dataset.\n",
      "\n",
      "11\n",
      "11  ---  RS_012_MLRC_2022_12\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — This study was able to replicate the results of the original paper in terms of find‐\n",
      "ing counterfactual explanations for all instances in datasets. Additional experiments\n",
      "were conducted to validate the robustness and generality of the conclusion. While there\n",
      "were slight deviations in terms of generating smaller mean distances, half of the models\n",
      "still outperformed the results of the existing method.\n",
      "\n",
      "12\n",
      "12  ---  RS_013_MLRC_2022_13\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — Our results approached the results reported in the original paper. They sup‐\n",
      "ported the claim that Shifty reliably guarantees fairness under demographic shift, but\n",
      "could not verify that Shifty performs at no loss of accuracy.\n",
      "\n",
      "13\n",
      "13  ---  RS_014_MLRC_2022_14\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — The original paper’s main claims were reproduced. While not all original au‐\n",
      "thors’ arguments were replicated (e.g. ROUGE scoring higher for relevance), the correla‐\n",
      "tion between metrics and human judgments showed similar tendencies as in [1]. The\n",
      "annotations correlated with the original at a Pearson score of 0.6, sufficient for repro‐\n",
      "ducing main claims.\n",
      "\n",
      "14\n",
      "14  ---  RS_015_MLRC_2022_15\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — In most cases, we were able to reproduce results from the original paper to\n",
      "within 1 standard deviation, and observe similar trends. However, due to missing infor‐\n",
      "mation about image pre‐processing, we were unable to reproduce the results exactly.\n",
      "\n",
      "15\n",
      "15  ---  RS_016_MLRC_2022_16\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — The plots supporting the label‐free feature and example importance match the\n",
      "ones from the paper, except for the label‐free feature importance experiment for CIFAR‐\n",
      "10. Similarly, the Pearson correlation results were successfully reproduced. Due to the\n",
      "nature of the autoencoders used for evaluation, we could not obtain the exact numerical\n",
      "results. However, we visually and numerically compare the trends, and in most cases,\n",
      "we observe that our results are similar to the ones in the paper.\n",
      "\n",
      "16\n",
      "16  ---  RS_017_MLRC_2022_17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — The results that we got were very coherent with the ones exposed in the pa‐\n",
      "per. Although we were not able to run many experiments with large models, we got a\n",
      "big insight into the topic. Something that is somehow negligible in theory, combined\n",
      "with finite‐precision arithmetics and the bad election of the subgradient, might lead to\n",
      "chaotic behaviour. This gives big support to the default values and gives a solid answer\n",
      "to the question: What would it be the best subgradient election? The theory says: it\n",
      "doesn’t matter, yet it does.\n",
      "\n",
      "17\n",
      "17  ---  RS_018_MLRC_2022_18\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — The claim that Shifty guarantees fairness with high confidence is strongly\n",
      "confirmed by the reproduction results of this study. It was also found in this repro‐\n",
      "ducibility study that Shifty achieves accuracy scores comparable to those of other\n",
      "fairness algorithms.\n",
      "\n",
      "18\n",
      "18  ---  RS_019_MLRC_2022_19\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We managed to reproduce most of the results the authors get. The method\n",
      "works well and most of the claims are supported. The method does increase the pre‐\n",
      "dictive performance of tree‐based models most of the time, but not always. When com‐\n",
      "pared to other regularization techniques the Hierarchical Shrinkage outperforms them\n",
      "when used on decision trees, but not when used on random forests. Since the method\n",
      "is applied after learning, it is extremely fast. And it does simplify decision boundaries\n",
      "for random forests, making them easier to interpret.\n",
      "\n",
      "19\n",
      "19  ---  RS_020_MLRC_2022_20\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We successfully reproduce the major trends of the core results, although some\n",
      "numerical deviations occur. We are able of providing support to two out of three claims.\n",
      "However, due to insufficient documentation and resources, we were unable to verify\n",
      "the paper’s third claim. We conclude that in order to determine the fairness of a rec‐\n",
      "ommender system, considering different fairness dimensions with a multi‐stakeholder\n",
      "perspective is essential.\n",
      "\n",
      "20\n",
      "20  ---  RS_021_MLRC_2022_21\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — The results we obtained showed the same patterns as in the original authors’\n",
      "work. All our results were in the range of ±1 LIC score units compared to the original\n",
      "work, which supports the claims on the gender and racial bias amplification, robustness\n",
      "against encoders, and amplification by NIC+Equalizer beyond baseline. As for our con‐\n",
      "tributions, we show that the attribution scores obtained by using integrated gradients\n",
      "follow similar patterns in terms of gender amplification for all evaluated language mod‐\n",
      "els, providing additional support for the proposed LIC metric.\n",
      "During data set analysis we observed a leakage in the original data split being used, re‐\n",
      "sulting in identical captions occurring multiple times in both the training and test set.\n",
      "The removal of already seen captions during training from the test set reduced its size\n",
      "by 62.4% on average and caused a decline in LICM scores of approximately 5 units.\n",
      "\n",
      "21\n",
      "21  ---  RS_022_MLRC_2022_22\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — The results we were able to produce support claim 1, albeit weakly. Further\n",
      "results are in line with the paper, but we found them to go against claim 3. In addition,\n",
      "we carried out a theoretical analysis which provides support for claim 4. Finally, we\n",
      "were unable to carry out our intended experiment to verify claim 2.\n",
      "\n",
      "22\n",
      "22  ---  RS_023_MLRC_2022_23\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — The results from our experiments were overall in line with the paper. We show\n",
      "that CGD outperforms Group-DRO on synthetic datasets with induced spurious correla‐\n",
      "tions, but the benefits of CGD are not clear in a real‐world setting. Beyond the results\n",
      "of the original paper, our attempt to empirically verify the mathematical proof of the\n",
      "authors that CGD monotonically decreases the loss was not conclusive.\n",
      "\n",
      "23\n",
      "23  ---  RS_024_MLRC_2022_24\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We found that all of the main claims of the paper were reproducible. However,\n",
      "when we repeated the same experiments on two new datasets, we found that there was\n",
      "a much higher correlation in example scores across different tasks (point iv above).\n",
      "\n",
      "24\n",
      "24  ---  RS_025_MLRC_2022_25\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — Our recall scores and qualitative experiments validate all claims of the authors:\n",
      "the framework creates an accurate mapping between the visual and semantic space, can\n",
      "analyze any trained CNN regardless of original training data availability, and is able to\n",
      "generate novel out‐of‐dataset descriptions for filters.\n",
      "\n",
      "25\n",
      "25  ---  RS_026_MLRC_2022_26\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — The reproduced results are close to the original results and support all four\n",
      "main claims. Furthermore, our additional results show that only a subset of the models\n",
      "amplifies age bias, while they strengthen the claim that LIC is robust against encoders.\n",
      "However, we acknowledge that our extension to age bias has its limitations.\n",
      "\n",
      "26\n",
      "26  ---  RS_027_MLRC_2022_27\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — Our results provide solid evidence in favor of the performance‐conserving prop‐\n",
      "erty of CrossWalk. However, we find inconclusive evidence of the fairness‐enhancing\n",
      "property of CrossWalk, mostly due to large variation in the reproduced disparity values.\n",
      "On the other hand, we find additional evidence in its favor by performing an experiment\n",
      "portraying the influence of the hyperparameters of CrossWalk.\n",
      "\n",
      "27\n",
      "27  ---  RS_028_MLRC_2022_28\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — Compared to the data reported in the original paper, the reproduced results\n",
      "vary across embedding models and evaluation metrics, where some combinations per‐\n",
      "form very similarly to the original paper while other combinations deviate significantly.\n",
      "Despite this, the claims of the original paper have been confirmed, which include no\n",
      "loss of accuracy, fairly calibrated subgroups and predictive equality.\n",
      "\n",
      "28\n",
      "28  ---  RS_029_MLRC_2022_29\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We verified the main claims of the paper and offered extensions. Our quali‐\n",
      "tative CartoonX and PixelRDE visualization results were similar to the original paper’s.\n",
      "From visualizing them, we saw that CartoonX could reveal piece‐wise information in the\n",
      "image relevant to the classifier. Our quantitative distortion plots followed trends similar\n",
      "to the original plots, allowing us to verify their claims, but only after adjustments to the\n",
      "unclear ’λ’ and ’number of steps’ hyperparameters the paper provides.\n",
      "\n",
      "29\n",
      "29  ---  RS_030_MLRC_2022_30\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We are able to reproduce and verify the three main claims of the original paper,\n",
      "by reproducing the results within 5% of the reported values. The additional experiments\n",
      "were successful and strengthen the claims that LASSI increases certified individual fair‐\n",
      "ness compared to the baseline models. Outliers of the experiments are studied and\n",
      "found to be caused by biased and inaccurate input data.\n",
      "\n",
      "30\n",
      "30  ---  RS_031_MLRC_2022_31\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — All but one of the figures and results from the original study were possible\n",
      "to reproduce. The obtained Evidence Lower Bound (ELBO) of the doubling VNCA was\n",
      "within 0.3% of the stated and for the non‐doubling VNCA the ELBO was within 1.8% and\n",
      "the observed damage recovery was similar. We were however not able to reproduce the\n",
      "t‐SNE reduction experiment for the baseline and were therefore not able to show the\n",
      "VNCA decoder having a cleaner t‐SNE separation than the baseline.\n",
      "\n",
      "31\n",
      "31  ---  RS_032_MLRC_2022_32\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We were able to reproduce the results on Least Core approximation. For the\n",
      "task of low‐value point identification we observed similar performance for least core\n",
      "and (Truncated Monte Carlo) Shapley values, whereas for high‐value identification, the\n",
      "least core outperformed other methods. In two experiments, we must depart from the\n",
      "original paper and arrive at different conclusions. Overall, we find that the Least Core\n",
      "offers similar results to other game‐theoretic approaches to data valuation, but it does\n",
      "not overcome the main drawbacks of computational complexity and sensitivity to ran‐\n",
      "domness that such techniques have.\n",
      "\n",
      "32\n",
      "32  ---  RS_033_MLRC_2022_33\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We validate the original paper’s claims by reproducing its metrics within the\n",
      "reported standard deviation, proving the method’s competitiveness. Our extended trials\n",
      "highlight the method’s potential applicability to real‐world data and reveal new consid‐\n",
      "erations about prompt template, language model, and seed for optimal performance.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "33  ---  RS_034_MLRC_2022_34\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We verify their theoretical claims that G‐Mixup indeed leads to a mixture of\n",
      "“key graph topologies.” We reproduce their experimental results on classification accu‐\n",
      "racy for REDDIT‐B to within about 6% of the reported value, and for IMDB‐B to within\n",
      "about 2% of the reported value. However, our experimental results do not provide sta‐\n",
      "tistically significant evidence to support the paper’s experimental claims that G‐Mixup\n",
      "improves the performance of GNNs or performs better than other graph data augmen‐\n",
      "tation methods.\n",
      "\n",
      "34\n",
      "34  ---  RS_035_MLRC_2022_35\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — Our results are consistent with the claim that the censored text (without swear\n",
      "words) is less often classified as hate speech, offensive, or abusive than the same text\n",
      "with swear words. However, we find the classification is very sensitive to the word re‐\n",
      "placement dictionary being used.\n",
      "\n",
      "35\n",
      "35  ---  RS_036_MLRC_2022_36\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — All claims made by the paper [1] seem to hold, as the results we obtained fol‐\n",
      "low the same trends as those presented in the original paper even if they do not match\n",
      "exactly. However, the same cannot always be said of the additional experiments.\n",
      "\n",
      "36\n",
      "36  ---  RS_037_MLRC_2022_37\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We observe expected qualitative behavior predicted by the theory on all exper‐\n",
      "iments. We improve the best semantic hashing model’s test performance by 5 nats by\n",
      "using a modern method for gradient estimation of discrete random variables.\n",
      "\n",
      "37\n",
      "37  ---  RS_038_MLRC_2022_38\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "38\n",
      "38  ---  RS_039_MLRC_2022_39\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — Our work suggests that the first claim of the paper, which states that Crosswalk\n",
      "minimizes disparity and thus enhances fairness is partially reproducible, and only for\n",
      "the tasks of Node classification and Influence maximization as the parameters specified\n",
      "in the paper do not always yield similar results. Then, the second claim of the paper\n",
      "which states that Crosswalk attains the necessary structural properties of the graph is\n",
      "fully reproducible through our experiments.\n",
      "\n",
      "39\n",
      "39  ---  RS_040_MLRC_2022_40\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — This paper successfully reproduces the claim that the MAE poses a nontrivial\n",
      "and meaningful self‐supervisory task. We show that models trained with this frame‐\n",
      "work generalize well to new datasets and conclude that the MAE is reproducible with\n",
      "exception for some hyperparameter choices. We also demonstrate that MAE performs\n",
      "well with smaller backbones and datasets. Finally, our results suggest that the SMAE\n",
      "extension improves the downstream classification accuracy of the MAE on CUB (+5 pp)\n",
      "when coupled with an appropriate masking strategy.\n",
      "\n",
      "40\n",
      "40  ---  RS_041_MLRC_2022_41\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We were able to reproduce the main claims on the MUTAG dataset, where Sub‐\n",
      "graphX has a better performance than GNNExplainer. Furthermore, SubgraphX has a\n",
      "reasonable runtime of about seven times longer than GNNExplainer. We successfully\n",
      "employed SubgraphX on the Karate Club dataset, where it outperforms GNNExplainer as\n",
      "well. The hyperparameter study revealed that the number of Monte‐Carlo Tree search it‐\n",
      "erations and Monte‐Carlo sampling steps are the most important hyperparameters and\n",
      "directly trade performance for runtime. Lastly, we show that our proposed improve‐\n",
      "ments to SubgraphX significantly enhance fidelity and runtime.\n",
      "\n",
      "41\n",
      "41  ---  RS_042_MLRC_2022_42\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We succeeded in reproducing phase diagrams for the toy example. For the\n",
      "MNIST dataset, we observed a behavior similar to the one from the paper. We used\n",
      "a wider range of hyperparameters leading us to an extra area with the memorization\n",
      "phase. We also argue that the original memorization phase is even more delayed\n",
      "grokking. Therefore, the authors’ findings about the MNIST phases are incomplete.\n",
      "\n",
      "42\n",
      "42  ---  RS_043_MLRC_2022_43\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — Running the code released by the authors does not produce an evaluation of\n",
      "BeT according to the metrics reported in the paper. After extending the implementation\n",
      "with the proper evaluation metrics, we obtain results that support the main claims of\n",
      "the paper in a significant subset of the experiments but that also diverge in many of the\n",
      "actual values obtained. Therefore, we conclude that the paper is largely replicable but\n",
      "not readily reproducible.\n",
      "\n",
      "43\n",
      "43  ---  RS_044_MLRC_2022_44\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — Claim 1 is verified: the model with the highest performance on SCAN does\n",
      "not maintain its high performance on other datasets (Section 4.1). Claims 2 and 3 are\n",
      "verified, with a comparison of performance between NQG‐T5 and the selected baseline\n",
      "models in [1] and [2]. Claim 4 is also verified by computing the coverage and precision\n",
      "of NQG in Section 4.4. Overall, accuracy for most experiments reaches within 2% of that\n",
      "reported in the original paper, with a deviation that our T5 achieves higher performance\n",
      "on some splits and slightly lower performance on one split than reported previously.\n",
      "\n",
      "44\n",
      "44  ---  RS_045_MLRC_2022_45\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "NOT FOUND\n",
      "Results — We reproduced the main claims that (i) oversampling with pure noise improves\n",
      "generalization by improving the minority‐class accuracy, (ii) the proposed batch nor‐\n",
      "malization (BN) method outperforms baselines, (iii) and this improvement is robust\n",
      "across data augmentations. Our results also support that (iv) adding pure noise images\n",
      "can improve classification on balanced training data. However, additional experiments\n",
      "suggest that the performance improvement from OPeN may be more orthogonal to the\n",
      "improvement caused by a bigger network or more complex data augmentation.\n",
      "\n",
      "45\n",
      "45  ---  RS_046_MLRC_2021_01\n",
      "46\n",
      "46  ---  RS_047_MLRC_2021_02\n",
      "<LTTextBoxHorizontal(13) 127.559,371.158,162.349,383.113 'Results\\n'>\n",
      "We validated each of the author’s claim through the experiments given in the original\n",
      "paper and few additional experiments of our own. Overall, we found many experiments\n",
      "yielding identical results while some deviations were observed with both the Counter‐\n",
      "factual Generative Network and the subsequent classification task. We were able to ex‐\n",
      "plain most of these deviations through our additional experiments while some couldn’t\n",
      "be validated due to computational limitations.\n",
      "\n",
      "47\n",
      "47  ---  RS_048_MLRC_2021_03\n",
      "<LTTextBoxHorizontal(13) 127.559,240.820,162.349,252.775 'Results\\n'>\n",
      "On the ResNet‐18 architecture and a high input resolution that the paper uses through‐\n",
      "out, our results on 6 datasets overall verify the claim that SSL regularizes few‐shot learn‐\n",
      "ers and provides higher gains with difficult tasks. Further, our results also verify that\n",
      "out‐of‐distribution images for SSL hurt the accuracy, and the domain selection algo‐\n",
      "rithm that we implement from scratch also verifies the paper’s claim that the algorithm\n",
      "\n",
      "48\n",
      "48  ---  RS_049_MLRC_2021_04\n",
      "<LTTextBoxHorizontal(13) 107.758,276.685,162.349,288.640 '1.3 Results\\n'>\n",
      "Overall, we reproduced the experiments related to the vision task as conducted at [1].\n",
      "Our results are up to first decimal place identical to those reported in [1] thus support‐\n",
      "ing the authors’ claim of having implemented a relatively sufficient ViT interpretabil‐\n",
      "ity method. When it comes to the AffinityNet [2], the method has been adapted in the\n",
      "context of Hybrid‐ViT architectures with our experiments indicating that the weakly‐\n",
      "supervised semantic segmentation performance of Hybrid‐ViT architectures are inferior\n",
      "to the CNN‐based ones.\n",
      "\n",
      "49\n",
      "49  ---  RS_050_MLRC_2021_05\n",
      "<LTTextBoxHorizontal(13) 127.559,344.151,162.349,356.106 'Results\\n'>\n",
      "We find that the main claims of the paper of (i) generating high‐quality counterfactuals,\n",
      "(ii) utilizing appropriate inductive biases, and (iii) using them to instil invariance in clas‐\n",
      "sifiers, do largely hold. However, we found certain experiments that were not directly\n",
      "reproducible due to either inconsistency between the paper and code, or incomplete\n",
      "specification of the necessary hyperparameters. Further, we were unable to reproduce\n",
      "a subset of experiments on a large‐scale dataset due to resource constraints, for which\n",
      "we compensate by performing those on a smaller version of the same dataset with our\n",
      "results supporting the general performance trend.\n",
      "\n",
      "50\n",
      "50  ---  RS_051_MLRC_2021_06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(13) 127.559,301.592,162.349,313.547 'Results\\n'>\n",
      "The results in the original paper regarding different objective functions were repro‐\n",
      "duced within a margin of error. Also, income redistribution is able to reduce wage\n",
      "inequality, albeit to a lesser degree. The objective functions appear to be sensitive to\n",
      "the neighbourhood selection mechanism. While the results of the rider‐fairness objec‐\n",
      "tive functions are maintained, performance of the driver‐fairness objective functions\n",
      "declines. There appear to be only small differences in service rates between ethnicities,\n",
      "while rider‐side fairness seems to mitigate inequalities the most. However, this is only\n",
      "achieved by worsening the service for well‐served neighbourhoods instead of improving\n",
      "it for underserved ones.\n",
      "\n",
      "51\n",
      "51  ---  RS_052_MLRC_2021_07\n",
      "<LTTextBoxHorizontal(14) 107.758,237.720,162.349,249.675 '1.3 Results\\n'>\n",
      "Our findings diverge substantially from the results reported in the original paper. In\n",
      "particular, in our reproduction experiments, including sentiment features appears to\n",
      "hurt the performance of the model in the hate speech detection task (approximately 0.5\n",
      "to 2.0 F1‐score) in the setting we could reproduce based on the description in the original\n",
      "paper and published source code (and limited contact with the authors, see Section 1.6).\n",
      "\n",
      "52\n",
      "52  ---  RS_053_MLRC_2021_08\n",
      "<LTTextBoxHorizontal(13) 127.559,302.588,162.349,314.543 'Results\\n'>\n",
      "We were only able to reproduce 22.2% of the explanation evaluation metrics and could\n",
      "thus not find conclusive support for claim 1. We could only verify claim 2 for one of the\n",
      "datasets and in total could reproduce 55.5% of the original scores. We could reproduce\n",
      "all scores regarding claim 3, but the claim is still not justified, as the scores between the\n",
      "fully connected and SCOUTER models lie very close to one another.\n",
      "\n",
      "53\n",
      "53  ---  RS_054_MLRC_2021_09\n",
      "<LTTextBoxHorizontal(13) 127.559,391.972,162.349,403.927 'Results\\n'>\n",
      "The image classification experiments on CIFAR‐10, CIFAR‐100 and ImageNet are repro‐\n",
      "duced to within 0.29%, 0.18% and 0.25% of reported values respectively. The language\n",
      "modeling experiments produce an average deviation of 0.22%, while the generative mod‐\n",
      "eling experiments on WGAN, WGAN‐GP and SN‐GAN are replicated to within 2.2%, 1.8%\n",
      "and 0.33% of value reported in the original paper.\n",
      "We perform ablation studies for change of dataset in language modeling and for effect\n",
      "of weight decay on ImageNet. We also perform analysis of generalization ability of op‐\n",
      "timizers and of training stability of GANs. All of the results largely support the claims\n",
      "made in the paper [1].\n",
      "\n",
      "54\n",
      "54  ---  RS_055_MLRC_2021_10\n",
      "<LTTextBoxHorizontal(13) 127.559,370.334,162.349,382.289 'Results\\n'>\n",
      "We were able to reproduce the results and verify the claims made by the authors for the\n",
      "StyleGAN and StyleGAN2 models by recreating the modified images, given the seed and\n",
      "other configuration parameters. Additionally, we also perform our own experiments to\n",
      "identify new edits and extend the truncation trick to images generated using StyleGAN.\n",
      "\n",
      "55\n",
      "55  ---  RS_056_MLRC_2021_11\n",
      "<LTTextBoxHorizontal(13) 107.758,346.972,162.349,358.927 '1.3 Results\\n'>\n",
      "Overall we find the original results to be reproducible; transformation policies found\n",
      "using Gao et al.1’s method can defend against gradient reconstruction attacks, and these\n",
      "transformations have negligible impact on training efficiency and model accuracy. How‐\n",
      "ever we do not observe the reported correlation between the proposed privacy‐score Spri\n",
      "and reconstruction PSNR. We also find that the degree of protection differs greatly from\n",
      "image to image, with poor protection in the worst case.\n",
      "\n",
      "56\n",
      "56  ---  RS_057_MLRC_2021_12\n",
      "<LTTextBoxHorizontal(13) 127.559,324.506,162.349,336.461 'Results\\n'>\n",
      "We found that the cluster‐based method does indeed consistently noticeably increase\n",
      "the isotropy of a set of CWRs over the global method. However, when it comes to se‐\n",
      "mantic tasks, we found that the cluster‐based method performs better than the global\n",
      "method in some and worse in other tasks, or that the improvements are within margin\n",
      "of error. Additionally, the results of one side experiment, which analyzes the structural\n",
      "information of CWRs, also contradict the authors’ findings for the GPT‐2 model.\n",
      "\n",
      "57\n",
      "57  ---  RS_058_MLRC_2021_13\n",
      "<LTTextBoxHorizontal(13) 127.559,358.900,162.349,370.855 'Results\\n'>\n",
      "Overall, our results support the main claims of the original paper1. Although the percent\n",
      "gendered words and male bias in our results are not exactly the same as those in the\n",
      "original paper1, the main trends are the same. The main difference is lower male bias\n",
      "for the baseline model in our results. However, our findings and the trend similarities\n",
      "between our results and those obtained by Dinan et al.1 demonstrate that bias controlled\n",
      "training or combining all three bias mitigation techniques can effectively control the\n",
      "amount of gender bias present in the model generated responses, supporting Dinan et\n",
      "al.’s claims1.\n",
      "\n",
      "58\n",
      "58  ---  RS_059_MLRC_2021_14\n",
      "<LTTextBoxHorizontal(13) 127.559,346.424,162.349,358.379 'Results\\n'>\n",
      "We found that that three of the four claims made by Ziko et al. are supported, and that\n",
      "one claim is partially supported. VFC is mostly on par with SOTA clustering objectives,\n",
      "if the trade‐off parameter and Lipschitz constant are tuned. Additionally, we verified\n",
      "that VFC is scalable on large‐scale datasets and found that the trade‐off control works\n",
      "as stated by the authors. Moreover, we conclude that VFC is capable of handling both\n",
      "prototype‐based and graph‐based datasets. Regarding the replicability of VFC, the ex‐\n",
      "periment on the alternative dataset did not indicate that VFC is worse than SOTA base‐\n",
      "lines. The proposed kernel‐based VFC performs on par with the original framework.\n",
      "\n",
      "59\n",
      "59  ---  RS_060_MLRC_2021_15\n",
      "<LTTextBoxHorizontal(12) 127.559,324.506,162.349,336.461 'Results\\n'>\n",
      "We reproduce the StylEx model in a different framework and test the AttFind algorithm,\n",
      "verifying the original paper’s results for the perceived age classifier. However, we could\n",
      "not reproduce the results for the other classifiers used, due to time limitations in training\n",
      "and the absence of their pre‐trained models. In addition, we verify the paper’s claim\n",
      "of providing human‐interpretable explanations, by reproducing the two user studies\n",
      "outlined in the original paper.\n",
      "\n",
      "60\n",
      "60  ---  RS_061_MLRC_2021_16\n",
      "<LTTextBoxHorizontal(13) 127.559,370.334,162.349,382.289 'Results\\n'>\n",
      "This work finds that (1) adversarial training does in fact lead to classwise performance\n",
      "discrepancies not only in standard error (accuracy) but also in attack robustness, (2)\n",
      "these discrepancies exacerbate existing biases in the model, (3) upweighting the stan‐\n",
      "dard and robust errors of poorly performing classes during training decreased this dis‐\n",
      "crepancy for both both the standard error and robustness and (4) increasing the attack\n",
      "margin for poorly performing classes during training also decreased these discrepan‐\n",
      "cies, at the cost of some performance. (1) (2) and (3) match the conclusions of the origi‐\n",
      "nal paper, while (4) deviated in that it was unsuccessful in helping increasing the robust‐\n",
      "ness the most poorly performing classes. Because the model and datasets used were\n",
      "totally different from the original paper’s, it is hard to to quantify the exact similarity of\n",
      "our results. Conceptually however, I find very similar conclusions.\n",
      "\n",
      "61\n",
      "61  ---  RS_062_MLRC_2021_17\n",
      "<LTTextBoxHorizontal(13) 127.559,380.017,162.349,391.972 'Results\\n'>\n",
      "We show that the theoretical assumption regarding eigenspace alignment and symmetry\n",
      "hold also for a different dataset other than the one used in the original paper. In addition,\n",
      "we reproduce ablations regarding learning rate, weight decay and Exponential Moving\n",
      "Average.\n",
      "Since we used CIFAR‐10 in all experiments we can not directly compare accuracies.\n",
      "However, we show the same relative behaviour of different networks given hyperparam‐\n",
      "eter changes. We can directly compare performance for one of the experiments (Table 8.\n",
      "in [1] bottom left part). Our models, namely SGD Baseline, DirectPred (with and without\n",
      "frequency=5), achieve comparable accuracy which differ by at most 1%. We also con‐\n",
      "firm the claim that DirectPred outperforms its one‐layer SGD alternative. Our code can be\n",
      "accessed under the following link: https://anonymous.4open.science/r/SelfSupervisedLearning-FD0F.\n",
      "\n",
      "62\n",
      "62  ---  RS_063_MLRC_2021_18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(12) 127.559,272.077,162.349,284.032 'Results\\n'>\n",
      "Most of our results closely match the reported results in the original paper for the Rotated‐\n",
      "MNIST [1], Fashion‐MNIST [2], PACS [3, 4], and Chest‐Xray [5] datasets. However, in\n",
      "some cases, as described later, we obtained better results quantitatively than the ones\n",
      "reported in the paper. By investigating the root cause of such mismatches, we provide\n",
      "a possible reason to avoid such a gap. We performed additional experiments by making\n",
      "necessary modifications for the Rotated‐MNIST and Rotated Fashion‐MNIST dataset. In\n",
      "\n",
      "63\n",
      "63  ---  RS_064_MLRC_2021_19\n",
      "<LTTextBoxHorizontal(17) 127.559,320.521,162.349,332.476 'Results\\n'>\n",
      "Our results followed similar patterns as those of the authors, which backs up their\n",
      "claims regarding the attacks. However, our results did slightly deviate from their re‐\n",
      "sults, meaning the original paper has some reproducibility issues in the context of our\n",
      "experimental setup.\n",
      "\n",
      "64\n",
      "64  ---  RS_065_MLRC_2021_20\n",
      "<LTTextBoxHorizontal(17) 107.758,282.663,162.349,294.618 '1.3 Results\\n'>\n",
      "The reproduction of the original paper as well as the extended implementation were suc‐\n",
      "cessful. We were able to reproduce the original results and examine the performance\n",
      "of the proposed model in an environment where strategic and non‐strategic users both\n",
      "present. Linear models seem to struggle with different proportions of strategic users,\n",
      "while the non‐linear model (RNN) achieves good performance regardless of the propor‐\n",
      "tion of strategic users.\n",
      "\n",
      "65\n",
      "65  ---  RS_066_MLRC_2021_21\n",
      "<LTTextBoxHorizontal(12) 127.559,348.416,162.349,360.371 'Results\\n'>\n",
      "A model is created for both dog‐cat and age classification. The models performed worse\n",
      "than stated than the pretrained models, most likely due to some issues with the StylEx\n",
      "style space, as AttFind performed well on the StyleGAN2 model. Due to the limitations\n",
      "in the adaptation it is not possible to definitively state whether the claims are true or\n",
      "false.\n",
      "\n",
      "66\n",
      "66  ---  RS_067_MLRC_2021_22\n",
      "<LTTextBoxHorizontal(14) 127.559,221.523,162.349,233.478 'Results\\n'>\n",
      "Due to many missing implementation details, it is not possible to reproduce the original\n",
      "results using the paper alone. However, in a specific setting motivated by the authors’\n",
      "code (more details in section 3), we managed to obtain results that support 3 out of 5\n",
      "claims. Even though the IAF and anchoring attacks outperform the baselines in certain\n",
      "\n",
      "67\n",
      "67  ---  RS_068_MLRC_2021_23\n",
      "<LTTextBoxHorizontal(13) 127.559,346.423,162.349,358.378 'Results\\n'>\n",
      "There are no exact numbers in the original paper to reproduce, rather the main claims\n",
      "are supported with some visualisations. With this in mind, our reproduction confirms\n",
      "the advantage provided by clustering over the assumption of independent arms, as well\n",
      "as the newly proposed algorithms outperforming the referenced benchmarks. We re‐\n",
      "peat all the experiments with multiple seeds to obtain robust estimates of the algorithms’\n",
      "performance and reduce the risk of drawing any conclusions out of results obtained by\n",
      "chance.\n",
      "\n",
      "68\n",
      "68  ---  RS_069_MLRC_2021_24\n",
      "<LTTextBoxHorizontal(13) 127.559,320.241,162.349,332.196 'Results\\n'>\n",
      "The reproduced results support all claims made in [1]. However, in the case of the unfair\n",
      "secretary algorithm (SA), some irregular results arise in the experiments due to random‐\n",
      "ness. This irregularity is also existent in the original code.\n",
      "\n",
      "69\n",
      "69  ---  RS_070_MLRC_2021_25\n",
      "<LTTextBoxHorizontal(13) 127.559,324.436,169.306,338.782 'Results\\n'>\n",
      "We successfuly reproduced the task‐agnostic experiments of the original paper, finding\n",
      "our results to strongly match with the original results. We also carried out a comparison\n",
      "with FrequentDirections but found the evaluation metrics of the original paper to\n",
      "be ill‐suited to compare ‐ setting up for further work on developing fair comparisons.\n",
      "\n",
      "70\n",
      "70  ---  RS_071_MLRC_2021_26\n",
      "<LTTextBoxHorizontal(13) 127.559,350.008,162.349,361.963 'Results\\n'>\n",
      "We reproduced and verified all the central claims made by the authors in the paper,\n",
      "confirming the intuition behind the novel methodologies introduced in the paper. Our\n",
      "results differ using the parameters given in the paper for the segmentation experiments\n",
      "but still support the claim of NAL being superior to its counterpart losses.\n",
      "\n",
      "71\n",
      "71  ---  RS_072_MLRC_2021_27\n",
      "<LTTextBoxHorizontal(12) 127.559,240.820,162.349,252.775 'Results\\n'>\n",
      "We were able to reproduce the exact results reported by the authors in all originally re‐\n",
      "ported scenarios. However, extended results on larger Wide Residual Networks have\n",
      "demonstrated the limitations of the newly proposed learning rate rewinding – we ob‐\n",
      "served a previously unreported accuracy degradation in low sparsity ranges. Neverthe‐\n",
      "less, the general conclusion of the paper still holds and was indeed reproduced.\n",
      "\n",
      "72\n",
      "72  ---  RS_073_MLRC_2021_28\n",
      "<LTTextBoxHorizontal(12) 127.559,297.327,162.349,309.282 'Results\\n'>\n",
      "Our results slightly deviate from the ones reported by the authors. This could be at‐\n",
      "tributed to the design choices we had to make, due to ambiguities present in the orig‐\n",
      "inal paper. After inspecting the provided codebase along with relevant literature, we\n",
      "were able to replicate the experimental setup. In our experiments, we observe similar\n",
      "trends and hence we can verify most of the paper’s claims, albeit not getting identical\n",
      "experimental results.\n",
      "\n",
      "73\n",
      "73  ---  RS_074_MLRC_2021_29\n",
      "<LTTextBoxHorizontal(13) 127.559,312.551,162.349,324.506 'Results\\n'>\n",
      "We reproduced the first claim since we observed the same monotonic increase of the\n",
      "success rate in the worst‐served neighborhood with respect to the overall success rate.\n",
      "The second claim we did not reproduce, since we found that the driver‐side fairness ob‐\n",
      "jective function obtains a higher income for the lowest‐earning drivers than the request‐\n",
      "maximizing objective function. We reproduced the third claim, since the driver‐side\n",
      "objective function performs best in terms of overall success rate and success rate in\n",
      "the worst‐served neighborhood, and also reduces the spread of income. Changes of the\n",
      "value estimator, preprocessing method and even dataset all led to consistent results re‐\n",
      "garding these claims.\n",
      "\n",
      "74\n",
      "74  ---  RS_075_MLRC_2021_30\n",
      "<LTTextBoxHorizontal(13) 127.559,370.334,162.349,382.289 'Results\\n'>\n",
      "We replicate the results of Pan et al. [1] on a subset of the LSUN Cat, LSUN Car [2] and\n",
      "CelebA [3] datasets and observe varying degrees of success. We perform several exper‐\n",
      "iments and illustrate the successes and shortcomings of the method. Our novel shape\n",
      "priors improve the 3D shape recovery in certain cases where the original shape prior\n",
      "was unsuitable. Our generalized training approach shows initial promise, but has to be\n",
      "confirmed with increased computational resources.\n",
      "\n",
      "75\n",
      "75  ---  RS_076_MLRC_2021_31\n",
      "<LTTextBoxHorizontal(13) 127.559,334.468,162.349,346.423 'Results\\n'>\n",
      "The techniques proposed by authors in [1] can successfully address the value alignment\n",
      "verification problem in different settings. We empirically demonstrate the effectiveness\n",
      "of their proposals by performing exhaustive experiments with several variations to their\n",
      "original claims. We show high accuracy and low false positive and false negative rates in\n",
      "the value alignment verification task with a minimum number of questions for different\n",
      "algorithms and heuristics.\n",
      "\n",
      "76\n",
      "76  ---  RS_077_MLRC_2021_32\n",
      "<LTTextBoxHorizontal(13) 127.559,359.734,162.349,371.689 'Results\\n'>\n",
      "For the Multi‐Color Secretary problem, we were able to recreate the outcomes, as well\n",
      "as the performance of the proposed algorithm (with a margin of 3‐4%). However, one\n",
      "baseline within the second experiment returned different results, due to inconsisten‐\n",
      "cies in the original implementation. In the context of the Multi‐Color Prophet problem,\n",
      "we were not able to exactly reproduce the original results, as the authors ran their exper‐\n",
      "iments with twice as many runs as reported. After correcting this, the original outcomes\n",
      "are reproduced.\n",
      "A drawback of the proposed prophet algorithms is that they only select a candidate in 50‐\n",
      "70% of cases. None results are often undesirable, so we extend the paper by proposing\n",
      "adjusted algorithms that pick a candidate (almost) every time. Furthermore, we show\n",
      "empirically that these algorithms maintain similar levels of fairness.\n",
      "\n",
      "77\n",
      "77  ---  RS_078_MLRC_2021_33\n",
      "<LTTextBoxHorizontal(12) 127.559,336.461,162.349,348.416 'Results\\n'>\n",
      "We were not able to fully reproduce the results of the original paper in this setting. The\n",
      "numbers (accuracies, precisions and margin distributions) obtained in our experiments\n",
      "differ significantly from those reported in the original paper. Though differences be‐\n",
      "tween the baseline model and the sufficiency model are not as significant as in the\n",
      "original paper, our results do support the main claims about sufficiency being able to\n",
      "increase the worst‐group precision and thus causing disparities between groups to de‐\n",
      "crease.\n",
      "\n",
      "78\n",
      "78  ---  RS_079_MLRC_2021_34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(13) 127.559,322.513,162.349,334.468 'Results\\n'>\n",
      "We reproduced the accuracy for the SPT planner module to within 14.7% of reported\n",
      "value, which, while outperforming the baselines [2] [3] in select cases, fails to support\n",
      "the paper’s conclusion that it outperforms the baselines. However, we achieve a similar\n",
      "drop‐off in accuracy in percentage points over different model settings. We suspect that\n",
      "the vagueness in the accuracy metric leads to the absolute difference of 14.7% despite\n",
      "the paper being reproducible. We further improve the reproduced figures by increasing\n",
      "model complexity. The Mapper module’s accuracy could not be tested.\n",
      "\n",
      "79\n",
      "79  ---  RS_080_MLRC_2021_35\n",
      "<LTTextBoxHorizontal(13) 127.559,310.558,162.349,322.513 'Results\\n'>\n",
      "In general, we are able to reproduce the results of Hyder et al. [1]. Because of the hy‐\n",
      "perparameter search, we are certain that the results are not cherry‐picked and mostly\n",
      "reproducible using the authors’ implementation of the algorithm. With our additional\n",
      "experiments, we further strengthen the validity of the proposed method and help future\n",
      "researchers and practitioners by providing additional information on the learning rates\n",
      "in the training and retrieval process.\n",
      "\n",
      "80\n",
      "80  ---  RS_081_MLRC_2021_36\n",
      "<LTTextBoxHorizontal(14) 127.559,325.315,162.349,337.270 'Results\\n'>\n",
      "We were able to reproduce majority of the results claimed in the paper except the Social‐\n",
      "LSTM and Directional‐LSTM models due to lack of time, and got a maximum of 2% de‐\n",
      "viation from that of the original paper.\n",
      "\n",
      "81\n",
      "81  ---  RS_082_MLRC_2021_37\n",
      "<LTTextBoxHorizontal(13) 127.559,314.543,162.349,326.498 'Results\\n'>\n",
      "Through our testing, we were able to come within 2% of the proposed metrics on certain\n",
      "datasets like Stanford Drone Dataset (SDD) and ETH/UCY, implying the author’s claims\n",
      "are sanguine and reproducible on varied hardware. However, certain results such as\n",
      "long‐term predictions on the Intersection Drone (InD) dataset were quite different; the\n",
      "probable reasons for which have been discussed.\n",
      "\n",
      "82\n",
      "82  ---  RS_083_MLRC_2021_38\n",
      "<LTTextBoxHorizontal(12) 127.559,359.799,162.349,371.754 'Results\\n'>\n",
      "We were able to reproduce GEN’s out‐performance of a chosen baseline and its perfect\n",
      "scores on synthetic data sets. We also confirm the author’s claims of the sub‐quadratic\n",
      "scaling of GEN’s forward passes and deduce that they reported the scaling of back‐passes\n",
      "too favourably. We conclude our work with scepticism of the chosen experiments’ suit‐\n",
      "ability to evaluate the model’s performance and discuss our findings.\n",
      "\n",
      "83\n",
      "83  ---  RS_084_MLRC_2021_39\n",
      "<LTTextBoxHorizontal(13) 127.559,358.379,162.349,370.334 'Results\\n'>\n",
      "We could not reproduce the density maps, but we produced similar density maps by\n",
      "modifying some of the parameters. We exactly reproduced the results on the paper’s\n",
      "data set. We did not get the same results on the CARPK data set and in experiments\n",
      "where implementation details were not provided. However, the differences are within\n",
      "standard error and our results support the claim that the model outperforms the base‐\n",
      "lines.\n",
      "\n",
      "84\n",
      "84  ---  RS_085_MLRC_2021_40\n",
      "<LTTextBoxHorizontal(13) 127.559,344.151,162.349,356.106 'Results\\n'>\n",
      "We reproduce most of the results supporting the original authors’ general claim: seed\n",
      "sets often suffer from biases that affect their performance as a baseline for bias metrics.\n",
      "Generally, our results mirror the original paper’s. They are slightly different on select\n",
      "occasions, but not in ways that undermine the paper’s general intent to show the fragility\n",
      "of seed sets.\n",
      "\n",
      "85\n",
      "85  ---  RS_086_MLRC_2021_41\n",
      "<LTTextBoxHorizontal(13) 127.559,322.796,162.349,334.751 'Results\\n'>\n",
      "The tracking performance was reproduced in terms of success, precision, and normal‐\n",
      "ized precision, and the reported value is in the 95 percent confidence interval, which\n",
      "supports the paper’s conclusion that TransATOM significantly outperforms other state‐\n",
      "of‐the‐art algorithms on TOTB database. Also, it supports a claim that including a trans‐\n",
      "parency feature in the tracker improves performance when tracking transparent objects.\n",
      "However, we refuted the claim that TransATOM well handles all challenges for robust\n",
      "target localization.\n",
      "\n",
      "86\n",
      "86  ---  RS_087_MLRC_2021_42\n",
      "<LTTextBoxHorizontal(12) 127.559,324.506,162.349,336.461 'Results\\n'>\n",
      "We verified that the publicly available pre‐trained model has a ’sufficiency’ measure\n",
      "within 1% of the value reported in the paper. Additionally, we evaluate the Fréchet in-\n",
      "ception distance (FID) scores of images generated by the released model. We show that\n",
      "the FID score increases with the number of attributes used to generate a counterfac‐\n",
      "tual explanation. Custom models were trained on three datasets, with a reduced image\n",
      "dimensionality (642px). Additionally, a user study was conducted to evaluate the distinc‐\n",
      "tiveness and coherence of the images. We report a significantly lower accuracy in the\n",
      "identification of the extracted attributes and ’sufficiency’ scores on our model.\n",
      "\n",
      "87\n",
      "87  ---  RS_088_MLRC_2021_43\n",
      "<LTTextBoxHorizontal(12) 107.758,370.241,162.349,382.196 '1.3 Results\\n'>\n",
      "We reproduced most of the data utility results reported in the first experiment for the\n",
      "Adult dataset. However, while the fairness metrics generally match the original paper\n",
      "they are numerically not comparable in absolute or relative terms. For the second exper‐\n",
      "iment, we were unsuccessful in reproducing results found by the authors. We note how‐\n",
      "ever that we made considerable changes to the experimental setup, which may make it\n",
      "difficult to perform a direct comparison of the results.\n",
      "\n",
      "88\n",
      "88  ---  RS_089_MLRC_2021_44\n",
      "<LTTextBoxHorizontal(13) 127.559,380.017,162.349,391.972 'Results\\n'>\n",
      "It was possible to verify the results from the original paper within a reasonable margin\n",
      "of error. However, the reproduced results show that the claimed protection does not\n",
      "generalize to an attacker that has knowledge over the augmentations used. Addition‐\n",
      "ally, the results show that the optimal augmentations are often predictable since the\n",
      "policies found by the proposed search algorithm mostly consist of the augmentations\n",
      "that perform best individually.\n",
      "\n",
      "89\n",
      "89  ---  RS_090_MLRC_2021_45\n",
      "<LTTextBoxHorizontal(13) 127.559,345.147,162.349,357.102 'Results\\n'>\n",
      "For the validation of the original paper, we compare the pre‐trained model and the re‐\n",
      "trained model to the results reported in the original paper. The retrained RCExplainer\n",
      "outperformed the other methods on fidelity and robustness, which corresponds with\n",
      "the results of the original authors. The measured efficiency of the method also corre‐\n",
      "sponds to the original result. To extend the paper, this comparison is also performed\n",
      "using a train‐test split, which showed no significant difference. The implementation of\n",
      "the metric is investigated and concerns are raised. Finally, the method generalises well\n",
      "to MNISTSuperpixels in terms of fidelity, but lacks in robustness.\n",
      "\n",
      "90\n",
      "90  ---  RS_091_MLRC_2021_46\n",
      "<LTTextBoxHorizontal(14) 127.559,259.562,162.349,271.517 'Results\\n'>\n",
      "We have achieved to reproduce the results qualitatively and quantitatively on a large\n",
      "scale. We also validated the generalization ability of the model by training and testing it\n",
      "on CelebA dataset. Although our experimental results are not identical with the original\n",
      "paper, they are consistent and validates the claims made by the original work.\n",
      "\n",
      "91\n",
      "91  ---  RS_092_MLRC_2021_47\n",
      "<LTTextBoxHorizontal(14) 127.559,333.462,162.349,345.417 'Results\\n'>\n",
      "We reproduced the papers results within standard deviations of our repeated experi‐\n",
      "ments. But in some cases, this still means there is a big difference between the perfor‐\n",
      "mances, which is coming from different train‐test splits of the newly proposed split‐\n",
      "ting schemes. Even with these discrepancies we still managed to (at least partially)\n",
      "confirm all authors claims. The proposed model GNN‐PPI performed better than PIPR\n",
      "overall and for inter‐novel‐protein interactions, evaluation on their proposed schemes\n",
      "predicted the generalization performance better, and their model is also robust for pre‐\n",
      "dictions for newly discovered proteins – here our results were surprising, they were even\n",
      "better when the network was built knowing fewer proteins.\n",
      "\n",
      "92\n",
      "92  ---  RS_093_MLRC_2021_48\n",
      "<LTTextBoxHorizontal(13) 127.559,251.508,162.349,263.463 'Results\\n'>\n",
      "With our tests and the obtained results, we confirm that all individual and combined\n",
      "sources of nondeterminism have similar effects on model variability and that instabil‐\n",
      "ity in neural network optimization is the main reason for this phenomenon. However,\n",
      "our results show some discrepancies in the reduction of variability by test‐time data aug‐\n",
      "mentation (TTA) and accelerated ensembling (claim 3 above). Like the original study, we\n",
      "show that these approaches successfully reduce variability, but the degree of reduction\n",
      "\n",
      "93\n",
      "93  ---  RS_094_MLRC_2020_01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LTTextBoxHorizontal(13) 127.559,336.637,162.349,348.592 'Results\\n'>\n",
      "We were able to reproduce their results using their code, yielding mostly similar results.\n",
      "TGT generally outperforms DBM, especially when explanations use few features. TGT\n",
      "is consistent in terms of the features to which it attributes cluster differences, across\n",
      "different sparsity levels. TGT matches real patterns in data. When extending the types of\n",
      "functions used for explanations, performance did not improve significantly, suggesting\n",
      "translations make for adequate explanations. However, the scaling extension shows\n",
      "promising performance on the modified synthetic data to recover the original signal.\n",
      "\n",
      "94\n",
      "94  ---  RS_095_MLRC_2020_02\n",
      "<LTTextBoxHorizontal(13) 127.559,289.636,162.349,301.591 'Results\\n'>\n",
      "We reproduce the claims of both papers by conducting several experiments in the UAVA\n",
      "dataset [3]. The integration of a differentiable geometric module within an keypoint-\n",
      "based object pose estimation model improved its performance in metrics. We addition-\n",
      "ally verify that this is the case for other differentiable PnP implementations (i.e. EPnP).\n",
      "Further, our results indicate that indeed HigherHRNet improves keypoint localisation\n",
      "performance on small scale objects.\n",
      "\n",
      "95\n",
      "95  ---  RS_096_MLRC_2020_03\n",
      "<LTTextBoxHorizontal(14) 127.559,285.498,162.349,297.453 'Results\\n'>\n",
      "We were able to successfully replicate experiments supporting the central claim of the\n",
      "paper, that the proposed snake non-linearity can learn periodic functions. We also\n",
      "analyze the suitability of the snake activation for other tasks like generative modeling\n",
      "and sentiment analysis.\n",
      "\n",
      "96\n",
      "96  ---  RS_097_MLRC_2020_04\n",
      "<LTTextBoxHorizontal(13) 127.559,310.558,162.349,322.513 'Results\\n'>\n",
      "For the MNIST-USPS dataset, we report similar accuracy and NMI values that are within\n",
      "1.2% and 0.5% of the values reported in the original paper. However, the balance and\n",
      "entropy differed significantly, where our results were within 73.1% and 30.3% of the orig-\n",
      "inal values respectively. For the Color Reverse MNIST dataset, we report similar values\n",
      "on accuracy, balance and entropy, which are within 5.3%, 2.6% and 0.2% respectively.\n",
      "Only the value of the NMI differed significantly, name within 12.9% of the original value\n",
      "In general, our results still support the main claim of the original paper, even though\n",
      "on some metrics the results differ significantly.\n",
      "\n",
      "97\n",
      "97  ---  RS_098_MLRC_2020_05\n",
      "<LTTextBoxHorizontal(14) 127.559,236.927,162.349,248.882 'Results\\n'>\n",
      "We reproduced the accuracy of the BayesBiNN optimizer within less than 0.5% of the\n",
      "originally reported value, which upholds the conclusion that it performs nearly as well\n",
      "as its full-precision counterpart in classification tasks. When we tried this in a seman-\n",
      "tic segmentation context, we found that the results were very underwhelming and in\n",
      "\n",
      "98\n",
      "98  ---  RS_099_MLRC_2020_06\n",
      "<LTTextBoxHorizontal(13) 127.559,208.263,162.349,220.218 'Results\\n'>\n",
      "We reproduced the authorsʼ results across all models and all available datasets, confirm-\n",
      "ing their findings that attention-based explanations can be manipulated and that mod-\n",
      "\n",
      "99\n",
      "99  ---  RS_100_MLRC_2020_07\n",
      "<LTTextBoxHorizontal(13) 127.559,370.510,162.349,382.465 'Results\\n'>\n",
      "Due to numerous inconsistencies between code and paper, it is not possible to replicate\n",
      "the original results using the paper alone. With help of the original codebase, a number\n",
      "of the original results can be retrieved. The main comparison claim of the paper, to\n",
      "improve over the preceding GNNExplainer, does hold. However, after performing the\n",
      "replication experiments, some questions regarding the validity of the used evaluation\n",
      "setup in the original paper remain.\n",
      "\n",
      "100\n",
      "100  ---  RS_101_MLRC_2020_08\n",
      "<LTTextBoxHorizontal(13) 127.559,311.870,162.349,323.825 'Results\\n'>\n",
      "We found that both proposed methods in the original paper help mitigate contextual\n",
      "bias, although for some methods, we could not completely replicate the quantitative\n",
      "results in the paper even after completing an extensive hyperparameter search. For\n",
      "example, on COCO-Stuff, DeepFashion, and UnRel, our feature-split model achieved an\n",
      "increase in accuracy on out-of-context images over the standard baseline, whereas on\n",
      "AwA, we saw a drop in performance. For the proposed CAM-based method, we were able\n",
      "to reproduce the original paperʼs results to within 0.5% mAP.\n",
      "\n",
      "101\n",
      "101  ---  RS_102_MLRC_2020_09\n",
      "102\n",
      "102  ---  RS_103_MLRC_2020_10\n",
      "<LTTextBoxHorizontal(13) 127.559,382.466,162.349,394.421 'Results\\n'>\n",
      "Our findings support the authorsʼ central claims. In terms of uncertainty estimation our\n",
      "EnD2 achieved (99 ± 1) % of the AUC-ROC of our ensemble on the OOD-detection task.\n",
      "The corresponding value in the original paper was (100 ± 1) %. In terms of classification\n",
      "our EnD2 had (16 ± 1)% higher error than our ensemble. The corresponding values in\n",
      "the original paper was (11 ± 6)%. Other metrics showed similar agreement, but, signifi-\n",
      "cantly, in the OOD-detection task our EnD performed at least as well as our EnD2. This\n",
      "is in stark contrast with the original paper.\n",
      "We also took a novel approach to visualizing the uncertainty decomposition by plotting\n",
      "the resulting distributions on a simplex, offering a visual explanation to some surprising\n",
      "results in the original paper, while mostly supporting the authorsʼ intuitive justifications\n",
      "for the model.\n",
      "\n",
      "103\n",
      "103  ---  RS_104_MLRC_2020_11\n",
      "<LTTextBoxHorizontal(13) 127.559,344.151,162.349,356.106 'Results\\n'>\n",
      "We obtain results that are within a 3% range of the reported results for all datasets other\n",
      "than on the ShanghaiTech dataset[2]. The anomalous run stuck out since the ”Non Mem-\n",
      "ory” of the same run could score markedly better than the ”Memory” variant. This led us\n",
      "to investigate the behaviour of the memory module and found valuable insights which\n",
      "are presented in Section 5.\n",
      "\n",
      "104\n",
      "104  ---  RS_105_MLRC_2020_12\n",
      "<LTTextBoxHorizontal(14) 127.559,261.654,162.349,273.609 'Results\\n'>\n",
      "We have achieved to reproduce the results qualitatively and quantitatively on synthetic\n",
      "and noise removal tasks. SADNet has the capacity to learn to remove the synthetic and\n",
      "real noise in images, and it produces visually-plausible outputs even after a few epochs.\n",
      "Moreover, we have employed SSIM and PSNR metrics to measure the quantitative perfor-\n",
      "mance for all settings. The quantitative results on both tasks are on-par when compared\n",
      "to the reported results in the paper.\n",
      "\n",
      "105\n",
      "105  ---  RS_106_MLRC_2020_13\n",
      "<LTTextBoxHorizontal(13) 127.559,394.244,162.349,406.199 'Results\\n'>\n",
      "Overall, our results mostly support the claims of the original paper. For the synthetic\n",
      "experiments, our results differ when using the exact values described in the paper, al-\n",
      "though they still support the main claim. After slightly modifying some of the experi-\n",
      "ment settings, our reproduced figures are nearly identical to the figures from the original\n",
      "paper. For the deep learning experiments, our results differ, with some of the baselines\n",
      "reaching a much higher accuracy on MNIST, CIFAR-10 and CIFAR-100. Nonetheless,\n",
      "with the help of an additional experiment, our results support the authorsʼ claim that\n",
      "partially Huberised losses perform well on real-world datasets subject to label noise.\n",
      "\n",
      "106\n",
      "106  ---  RS_107_MLRC_2020_14\n",
      "<LTTextBoxHorizontal(13) 127.559,334.468,162.349,346.423 'Results\\n'>\n",
      "Most of our results closely match the reported results in the original paper. Therefore,\n",
      "we confirm that the warm-starting gap exists in certain settings and that the Shrink-\n",
      "Perturb method successfully reduces or eliminates this gap. However, in some cases,\n",
      "we were not able to completely reproduce their results. By investigating the root of such\n",
      "mismatches, we provide another solution to avoid this gap. In particular, we show that\n",
      "data augmentation also helps to reduce the warm-starting gap.\n",
      "\n",
      "107\n",
      "107  ---  RS_108_MLRC_2020_15\n",
      "<LTTextBoxHorizontal(20) 107.758,260.745,162.349,272.700 '1.3 Results\\n'>\n",
      "We were able to reproduce the Hits@1 to be within ±2.4% of the reported value (in most\n",
      "cases). Anomalies were observed in 2 cases.\n",
      "\n",
      "108\n",
      "108  ---  RS_109_MLRC_2020_16\n",
      "<LTTextBoxHorizontal(13) 127.559,336.637,162.349,348.592 'Results\\n'>\n",
      "The results presented in [1] were reproducible, both by using the provided code and our\n",
      "own implementation. Our additional experiments have highlighted several limitations\n",
      "of the explanatory algorithm in question: the algorithm severely relies on the shape and\n",
      "variance of the clusters present in the data (and, if applicable, the method used to la-\n",
      "bel these clusters), and highly non-linear dimensionality reduction algorithms perform\n",
      "worse in terms of explainability.\n",
      "\n",
      "109\n",
      "109  ---  RS_110_MLRC_2020_17\n",
      "<LTTextBoxHorizontal(13) 127.559,378.304,162.349,390.259 'Results\\n'>\n",
      "The obtained mean and standard deviation of the MCC over 100 seeds are within 1 per-\n",
      "cent of the results reported in the paper. The iFlow model obtained a mean MCC score of\n",
      "0.718 (0.067). Efforts to improve and correct the baseline increased the mean MCC score\n",
      "from 0.483 (0.059) to 0.556 (0.061). The performance, however, remains worse than the\n",
      "performance of iFlow, further supporting the authorsʼ claim that the iFlow implemen-\n",
      "tation is correct and more effective than iVAE.\n",
      "\n",
      "110\n",
      "110  ---  RS_111_MLRC_2020_18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "111  ---  RS_112_MLRC_2020_19\n",
      "<LTTextBoxHorizontal(14) 127.559,313.315,162.349,325.270 'Results\\n'>\n",
      "During the study, we were not able to reproduce the method due to a conceptual mis-\n",
      "interpretation of ours regarding the authorsʼ adaption of the Transformer [2]. However,\n",
      "the publicly available implementation helped us answering our questions and proved\n",
      "its validity during our experiments on different datasets. Additionally, we compared the\n",
      "papersʼ temporal attention encoder to our adaption of it, which we came across while\n",
      "we were trying to reimplement and grasp the authorsʼ ideas.\n",
      "\n",
      "112\n",
      "112  ---  RS_113_MLRC_2020_20\n",
      "<LTTextBoxHorizontal(13) 127.559,370.883,162.349,382.838 'Results\\n'>\n",
      "In contrast to the first claim, we have discovered that for most of the architectures, re-\n",
      "construction errors for the attacks are quite low, which means that in our models the\n",
      "first claim is not supported. We also found that for most of the models, the classifica-\n",
      "tion error is somewhat higher than those provided in the paper. However, these indeed\n",
      "relate to the original work and partially support the second claim of the authors.\n",
      "\n",
      "113\n",
      "113  ---  RS_114_MLRC_2020_21\n",
      "<LTTextBoxHorizontal(13) 127.559,358.379,162.349,370.334 'Results\\n'>\n",
      "We reproduce RigLʼs performance on CIFAR-10 within 0.1% of the reported value. On\n",
      "both CIFAR-10/100, the central claim holds—given a fixed training budget, RigL sur-\n",
      "passes existing dynamic-sparse training methods over a range of target sparsities. By\n",
      "training longer, the performance can match or exceed iterative pruning, while consum-\n",
      "ing constant FLOPs throughout training. We also show that there is little benefit in tun-\n",
      "ing RigLʼs hyper-parameters for every sparsity, initialization pair—the reference choice\n",
      "of hyperparameters is often close to optimal performance.\n",
      "\n",
      "114\n",
      "114  ---  RS_115_MLRC_2020_22\n",
      "115\n",
      "115  ---  RS_116_MLRC_2020_23\n",
      "<LTTextBoxHorizontal(13) 127.559,296.330,162.349,308.285 'Results\\n'>\n",
      "We reproduced the framework in the original paper and verified the main claims made\n",
      "by the authors in the original paper. However, the GCE model in extension study did\n",
      "not manage to separate causal factors and non-causal factors for a text classifier due to\n",
      "the complexity of fine-tuning the model.\n",
      "\n",
      "116\n",
      "116  ---  RS_117_MLRC_2020_24\n",
      "117\n",
      "117  ---  RS_118_NeurIPS_2019_01\n",
      "118\n",
      "118  ---  RS_119_NeurIPS_2019_02\n",
      "<LTTextBoxHorizontal(8) 301.332,595.020,336.464,603.987 'Abstract\\n'>\n",
      "Binarized Neural Networks are paving a way towards the deployment of deep\n",
      "neural networks with less memory and computation.\n",
      "In this report, we present\n",
      "a detailed study on the paper titled \"Latent Weights Do Not Exist: Rethinking\n",
      "Binarized Neural Network Optimization\" by [1] which proposes a new optimization\n",
      "method for training BNN called BOP. We ﬁrst investigate the eﬀect of using latent\n",
      "weights in BNN for analyzing prediction performance in terms of accuracy. Next, a\n",
      "comprehensive ablation study on hyperparameters is provided. Finally, we explore\n",
      "the usability of BNN in denoising autoencoders. Code for all our experiments are\n",
      "available at https://github.com/nancy-nayak/rethinking-bnn/\n",
      "\n",
      "119\n",
      "119  ---  RS_120_NeurIPS_2019_03\n",
      "<LTTextBoxHorizontal(8) 301.332,606.639,336.464,615.606 'Abstract\\n'>\n",
      "In this project we attempt to reproduce results from the paper Generative Mod-\n",
      "eling by Estimating Gradients of the Data Distribution by Song and Ermon1. The\n",
      "authors propose a novel generative framework based solely on gradients of data\n",
      "density estimated by a neural network. Once the model is trained, sampling can\n",
      "be performed with annealed Langevin dynamics. While we managed to reproduce\n",
      "the experiments qualitatively, we failed to achieve comparable results for Inception\n",
      "and FID scores for CIFAR-10. We further extended the original work in various di-\n",
      "rections (computing and analysing FID and IS also for CelebA, investigation of the\n",
      "sampling hyperparameters ϵ and T , linear instead of geometric annealing schedule\n",
      "for noise levels, and diﬀerent network architecture).\n",
      "\n",
      "120\n",
      "120  ---  RS_121_NeurIPS_2019_04\n",
      "121\n",
      "121  ---  RS_122_NeurIPS_2019_05\n",
      "122\n",
      "122  ---  RS_123_NeurIPS_2019_06\n",
      "<LTTextBoxHorizontal(7) 301.332,627.544,336.464,636.511 'Abstract\\n'>\n",
      "In today’s world, neural networks are being in almost every discipline resulting in\n",
      "signiﬁcant improvement in all the tools and applications. But in the ﬁeld of Physics,\n",
      "they struggle to attain the basic laws like conservation of momentum. The paper\n",
      "Hamiltonian Neural Networks addresses this issue by using Hamiltonian mechanics\n",
      "to train the neural network in an unsupervised method. The following report is an\n",
      "explanation of the paper and the code to reproduce the claimed results.\n",
      "\n",
      "123\n",
      "123  ---  RS_124_NeurIPS_2019_07\n",
      "<LTTextBoxHorizontal(8) 301.332,608.970,336.464,617.937 'Abstract\\n'>\n",
      "We reproduce the work in Zero-shot Knowledge Transfer via Adversarial Belief\n",
      "Matching, which describes a novel approach for knowledge transfer. A teacher net-\n",
      "work trained on real samples distills knowledge to a student network that is trained\n",
      "solely on pseudo data extracted from a generator network, with the student trying\n",
      "to mimic the teacher’s outputs on these datapoints. To this end, we additionally\n",
      "re-implement Wide Residual Networks which are used as the main framework for\n",
      "both teacher and student networks and train them from scratch on CIFAR10 and\n",
      "SVHN. We compare the results of the proposed method with a few-shot knowledge\n",
      "distillation attention transfer setting implemented and trained from scratch. We\n",
      "suggest an approach for further exploitation of the learnt mechanics of the gener-\n",
      "ator network in the zero-shot setting, which operates on top of the main method,\n",
      "and brieﬂy discuss the beneﬁts and drawbacks of this approach. Our code can be\n",
      "found publicly available in https://github.com/AlexandrosFerles/NIPS_2019_Reproducibilty_\n",
      "Challenge_Zero-shot_Knowledge_Transfer_via_Adversarial_Belief_Matching.\n",
      "\n",
      "124\n",
      "124  ---  RS_125_NeurIPS_2019_08\n",
      "125\n",
      "125  ---  RS_126_NeurIPS_2019_09\n",
      "<LTTextBoxHorizontal(10) 302.157,617.215,335.647,626.181 'Abstract\\n'>\n",
      "In this study, we performed some ablations on the main model developed in the pa-\n",
      "per Unsupervised Representation Learning in Atari [1] as part of the 2019 NeurIPS Re-\n",
      "producibility Challenge. In this paper, Anand et. al introduce a new learning method\n",
      "called SpatioTemporal DeepInfoMax (STDIM), which is an unsupervised method that\n",
      "aims at learning state representations by maximizing particular forms of mutual in-\n",
      "formation between a series of observations. Our work focuses on recreating a subset\n",
      "of their results, along with hyperparameter tuning, slightly altering the STDIM learn-\n",
      "ing objective, and altering the receptive field of the encoder model that Anand et. al\n",
      "introduce in their article. We also suggest directions for further expanding the STDIM\n",
      "method. Our results also suggest that creating an ensemble model would allow for\n",
      "further boosting of the effectiveness of this model.\n",
      "\n",
      "126\n",
      "126  ---  RS_127_NeurIPS_2019_10\n",
      "<LTTextBoxHorizontal(10) 301.332,601.002,336.464,609.969 'Abstract\\n'>\n",
      "The lottery ticket hypothesis states that smaller subnetworks within a larger\n",
      "deep network can be trained in isolation to achieve accuracy similar to that of\n",
      "original network, as long as they are initialized appropriately. However, whether\n",
      "these subnetworks or winning tickets are transferable across datasets and optimizers\n",
      "remains unclear. The paper \"One ticket to win them all:generalizing lottery ticket\n",
      "initializations across datasets and optimizers\" empirically shows that these winning\n",
      "tickets are transferable. We reproduce the results in the paper from scratch by\n",
      "implementing all the experiments. Our results support the original paper’s claim of\n",
      "the winning ticket initializations being transferable. While the paper is replicable,\n",
      "we ﬁnd that reproducing the paper requires access to large amount of computing\n",
      "resources for generating the winning tickets. Hence we also open-source the winning\n",
      "tickets we ﬁnd, so others can avoid the compute-intensive procedure of generating\n",
      "them.\n",
      "\n",
      "127\n",
      "127  ---  RS_128_NeurIPS_2019_11\n",
      "<LTTextBoxHorizontal(8) 301.332,598.613,336.464,607.580 'Abstract\\n'>\n",
      "Miscalibration of a model is deﬁned as the mismatch between predicting proba-\n",
      "bility estimates and the true correctness likelihood. In this work, we aim to replicate\n",
      "the results reported by [1] on their analysis of the eﬀect of Mixup [2] on a network’s\n",
      "calibration. Mixup is an eﬀective yet simple approach of data augmentation, which\n",
      "generates a convex combination of a pair of training images and their correspond-\n",
      "ing labels as the input and target for training a network. We replicate the results\n",
      "reported by the authors for CIFAR-100 [3], Fashion-MNIST [4], STL-10 [5], out-\n",
      "of-distribution and random noise data. Our implementation code can be found at\n",
      "https://github.com/MacroMayhem/OnMixup.\n",
      "\n",
      "128\n",
      "128  ---  RS_129_ICLR_2019_01\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "129\n",
      "129  ---  RS_130_ICLR_2019_02\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "130\n",
      "130  ---  RS_131_ICLR_2019_03\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "131\n",
      "131  ---  RS_132_ICLR_2019_04\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "132\n",
      "132  ---  RS_133_ICLR_2019_05\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "133\n",
      "133  ---  RS_134_ICDAR_2018_01\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "134\n",
      "134  ---  RS_135_ICDAR_2018_02\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "135\n",
      "135  ---  RS_136_ICDAR_2018_03\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "136\n",
      "136  ---  RS_137_ICDAR_2018_04\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "137\n",
      "137  ---  RS_138_ICDAR_2018_05\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "138\n",
      "138  ---  RS_139_ICDAR_2018_06\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "139\n",
      "139  ---  RS_140_ICDAR_2018_07\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "140\n",
      "140  ---  RS_141_ICDAR_2018_08\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "141\n",
      "141  ---  RS_142_ICDAR_2018_09\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "142\n",
      "142  ---  RS_143_ICDAR_2018_10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "143\n",
      "143  ---  RS_144_ICDAR_2018_11\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "144\n",
      "144  ---  RS_145_ICDAR_2018_12\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "145\n",
      "145  ---  RS_146_ICDAR_2018_13\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "146\n",
      "146  ---  RS_147_ICDAR_2018_14\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "147\n",
      "147  ---  RS_148_ICDAR_2018_15\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_names = glob.glob(\"RS_ALL_148/*.pdf\")\n",
    "\n",
    "df_excel = pd.read_excel('RS_ALL_IN_ONE_metadata_TEST.xlsx',sheet_name=\"Sheet1\")\n",
    "\n",
    "\n",
    "def extract_1st_page_results_2022(filename,max_pages = 1):\n",
    "    for page_layout in extract_pages(file,maxpages=max_pages):\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                el_text = element.get_text()\n",
    "\n",
    "                if el_text.split(' ')[0].lower() == 'results':\n",
    "                    print(el_text)\n",
    "                    return el_text\n",
    "                else:\n",
    "                    print(\"NOT FOUND\")\n",
    "\n",
    "                    \n",
    "def extract_1st_page_results_2020_2021(filename,max_pages = 1):\n",
    "    flag = 0\n",
    "    for page_layout in extract_pages(filename,maxpages=1):\n",
    "\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                if flag == 1 : \n",
    "                    print(element.get_text())\n",
    "                    return element.get_text()\n",
    "\n",
    "                el_text = element.get_text()\n",
    "\n",
    "                if 'results' in el_text.lower() and len(el_text.split())<6:\n",
    "                    print(element)\n",
    "                    flag = 1 # returning the next element values\n",
    "                elif el_text.split()[0].lower() == 'results':\n",
    "                    return element.get_text()\n",
    "                \n",
    "\n",
    "def extract_1st_page_results_NeurIPS_2019(filename,max_pages = 1):\n",
    "    flag = 0\n",
    "    for page_layout in extract_pages(filename,maxpages=1):\n",
    "\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                if flag == 1 : \n",
    "                    print(element.get_text())\n",
    "                    return element.get_text()\n",
    "\n",
    "                el_text = element.get_text()\n",
    "\n",
    "                if 'abstract' in el_text.lower():\n",
    "                    print(element)\n",
    "                    flag = 1 # returning the next element values\n",
    "\n",
    "\n",
    "for index, file in enumerate(file_names):\n",
    "\n",
    "    reults_text = \"NOT EXTRACTED\"\n",
    "    \n",
    "    rs_key = file.split('\\\\')[1].replace('.pdf','')\n",
    "    \n",
    "    required_index = df_excel.loc[df_excel[\"key_for_all_RS\"] == rs_key].index[0]\n",
    "    print(required_index)\n",
    "    print(index, \" --- \",  rs_key)\n",
    "    \n",
    "    if \"MLRC_2022\" in file:\n",
    "        reults_text =  extract_1st_page_results_2022(file)\n",
    "        \n",
    "        if reults_text == '' or reults_text is None:\n",
    "            df_excel.at[required_index, 'rs_comment'] = \"NOT EXTRACTED\"\n",
    "        else:\n",
    "            df_excel.at[required_index, 'rs_comment'] = reults_text\n",
    "        \n",
    "        continue\n",
    "    \n",
    "    elif \"MLRC_2021\" in file or \"MLRC_2020\" in file:\n",
    "        reults_text =  extract_1st_page_results_2020_2021(file)   \n",
    "\n",
    "        if reults_text == '' or reults_text is None:\n",
    "            df_excel.at[required_index, 'rs_comment'] = \"NOT EXTRACTED\"\n",
    "        else:\n",
    "            df_excel.at[required_index, 'rs_comment'] = reults_text        \n",
    "        \n",
    "        continue\n",
    "        \n",
    "    elif \"NeurIPS_2019\" in file:\n",
    "        reults_text =  extract_1st_page_results_NeurIPS_2019(file) \n",
    "\n",
    "        if reults_text == '' or reults_text is None:\n",
    "            df_excel.at[required_index, 'rs_comment'] = \"NOT EXTRACTED\"\n",
    "        else:\n",
    "            df_excel.at[required_index, 'rs_comment'] = reults_text        \n",
    "            \n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    print(\"\\n\\n\\n\")\n",
    "    \n",
    "df_excel.to_excel(\"RS_ALL_IN_ONE_metadata_with_rs_comment.xlsx\",index=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73a0d6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RS_001_MLRC_2022_01\n",
      "Results — Due to many missing implementation details, we were not able to reproduce all\n",
      "of the original results. Some claims can be supported by our results, but most results are\n",
      "very vague. Even though the new method outperforms the baselines in certain scenarios,\n",
      "we find that the superiority of the method is not as strong as presented in the original\n",
      "paper.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_002_MLRC_2022_02\n",
      "Results — We have reproduced the experiments done on two selected tasks, and com‐\n",
      "pared their results with the reported results. Although our experimental results are not\n",
      "identical to the reported ones, we can validate the claims made by the original study\n",
      "according to these results.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_003_MLRC_2022_03\n",
      "Results — We verify the authors’ claims by replicating the experiments presented in [1].\n",
      "All of our experiments show identical results to the ones presented in [1], apart from per‐\n",
      "turbation testing for which we provide an additional in depth analysis. We also provide\n",
      "an analysis of the new alpha hyperparameter and a hyperparameter search.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_004_MLRC_2022_04\n",
      "Results — We find that the three main claims of the paper hold true as we were able to\n",
      "successfully reproduce each corresponding experiment. Our results differ in some mi‐\n",
      "nor aspects, but they do support the validity of the three main claims made in the paper.\n",
      "Furthermore, we also demonstrated that the framework proposed is expandable in new\n",
      "contexts, thus providing further support for its utility and applicability.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_005_MLRC_2022_05\n",
      "Results — We only manage to partially reproduce the original scores. Nonetheless, the\n",
      "hypothesis formulated in the original paper are still corroborated.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_006_MLRC_2022_06\n",
      "Results — The obtained linear evaluation accuracies differ in a range between 0.19% and\n",
      "2.05%, while the ones in the original paper differ from 0.20% to 0.80%. Nonetheless, we\n",
      "believe that our results support that the differences in top‐1 accuracy among different\n",
      "batch sizes are minimal because of different choices of the dataset, base encoder, and\n",
      "batch sizes, and also because the range substantially increases when the projection head\n",
      "is not deep. All the other experiments support the original and the newly tested claims.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_007_MLRC_2022_07\n",
      "Results — We managed to reproduce all the experiments supporting the main claims of\n",
      "the original article. Additionally, we add uncertainty quantification to the results as\n",
      "we believe this is a crucial part to confirm any of the claims. Finally, we present the\n",
      "exploration‐exploitation trade‐off experiment in a more robust manner leveraging the\n",
      "nucleotide diversity metric to gain additional insight into how the proposed algorithm\n",
      "works.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_008_MLRC_2022_08\n",
      "Results — Our study confirms three (iii, iv and v) claims from [1]. However, it cannot fully\n",
      "support the other claims (i and ii). Using the configurations given in [1], we achieved\n",
      "comparable performances on the CARS196 dataset, but large deviances were observed\n",
      "on other datasets, which dropped by 1.5% and 1% R@1 using ResNet50. By implement‐\n",
      "ing Bayesian optimization, the performances are improved by 0.7% and 0.4% R@1 on\n",
      "CARS196 and CUB‐200‐2011. We also close the performance gap for SOP dataset from\n",
      "‐1% to ‐0.6% compared to the proposed results in [1] using ResNet50.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_009_MLRC_2022_09\n",
      "Results — Our results confirm most of the claims tested, although we could not achieve\n",
      "the exact same accuracy due to missing hyper‐parameters. We reproduced the trend in\n",
      "how scaling the prior impacts the performance and how a learned prior outperforms a\n",
      "non‐learned prior. On contrary, we could not reproduce the effect of rank in low‐rank\n",
      "covariance approximation on model performance, as well as the beneficial boost in per‐\n",
      "formance of Bayesian learning compared to the standard SGD.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_010_MLRC_2022_10\n",
      "Results — Our results support the claims made by the original authors. (i) We observe\n",
      "that CartoonX produces piece‐wise smooth explanations. Most of the explanations give\n",
      "valuable insights. (ii) Most experiments, that show how CartoonX achieves lower dis‐\n",
      "tortion outputs compared to other methods, have been reproduced. In the cases where\n",
      "exact reproducibility has not been achieved, claim (ii) of the author still holds. (iii) The\n",
      "model‐agnosticism claim still holds as the overall quality of the ViT‐based explanations\n",
      "almost matches that of the CNN‐based explanations. Finally, simple heuristical initial‐\n",
      "izations did not improve the runtime.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_011_MLRC_2022_11\n",
      "Results — We reproduced the original paper’s work through our experiments. We find\n",
      "that the main claims of the paper largely hold. We assess the robustness and general‐\n",
      "izability of some of the claims, through our additional experiments. In that case, we\n",
      "find that one claim is not generalizable and another is not reproducible for the graph\n",
      "dataset.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_012_MLRC_2022_12\n",
      "Results — This study was able to replicate the results of the original paper in terms of find‐\n",
      "ing counterfactual explanations for all instances in datasets. Additional experiments\n",
      "were conducted to validate the robustness and generality of the conclusion. While there\n",
      "were slight deviations in terms of generating smaller mean distances, half of the models\n",
      "still outperformed the results of the existing method.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_013_MLRC_2022_13\n",
      "Results — Our results approached the results reported in the original paper. They sup‐\n",
      "ported the claim that Shifty reliably guarantees fairness under demographic shift, but\n",
      "could not verify that Shifty performs at no loss of accuracy.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_014_MLRC_2022_14\n",
      "Results — The original paper’s main claims were reproduced. While not all original au‐\n",
      "thors’ arguments were replicated (e.g. ROUGE scoring higher for relevance), the correla‐\n",
      "tion between metrics and human judgments showed similar tendencies as in [1]. The\n",
      "annotations correlated with the original at a Pearson score of 0.6, sufficient for repro‐\n",
      "ducing main claims.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_015_MLRC_2022_15\n",
      "Results — In most cases, we were able to reproduce results from the original paper to\n",
      "within 1 standard deviation, and observe similar trends. However, due to missing infor‐\n",
      "mation about image pre‐processing, we were unable to reproduce the results exactly.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_016_MLRC_2022_16\n",
      "Results — The plots supporting the label‐free feature and example importance match the\n",
      "ones from the paper, except for the label‐free feature importance experiment for CIFAR‐\n",
      "10. Similarly, the Pearson correlation results were successfully reproduced. Due to the\n",
      "nature of the autoencoders used for evaluation, we could not obtain the exact numerical\n",
      "results. However, we visually and numerically compare the trends, and in most cases,\n",
      "we observe that our results are similar to the ones in the paper.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_017_MLRC_2022_17\n",
      "Results — The results that we got were very coherent with the ones exposed in the pa‐\n",
      "per. Although we were not able to run many experiments with large models, we got a\n",
      "big insight into the topic. Something that is somehow negligible in theory, combined\n",
      "with finite‐precision arithmetics and the bad election of the subgradient, might lead to\n",
      "chaotic behaviour. This gives big support to the default values and gives a solid answer\n",
      "to the question: What would it be the best subgradient election? The theory says: it\n",
      "doesn’t matter, yet it does.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_018_MLRC_2022_18\n",
      "Results — The claim that Shifty guarantees fairness with high confidence is strongly\n",
      "confirmed by the reproduction results of this study. It was also found in this repro‐\n",
      "ducibility study that Shifty achieves accuracy scores comparable to those of other\n",
      "fairness algorithms.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_019_MLRC_2022_19\n",
      "Results — We managed to reproduce most of the results the authors get. The method\n",
      "works well and most of the claims are supported. The method does increase the pre‐\n",
      "dictive performance of tree‐based models most of the time, but not always. When com‐\n",
      "pared to other regularization techniques the Hierarchical Shrinkage outperforms them\n",
      "when used on decision trees, but not when used on random forests. Since the method\n",
      "is applied after learning, it is extremely fast. And it does simplify decision boundaries\n",
      "for random forests, making them easier to interpret.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_020_MLRC_2022_20\n",
      "Results — We successfully reproduce the major trends of the core results, although some\n",
      "numerical deviations occur. We are able of providing support to two out of three claims.\n",
      "However, due to insufficient documentation and resources, we were unable to verify\n",
      "the paper’s third claim. We conclude that in order to determine the fairness of a rec‐\n",
      "ommender system, considering different fairness dimensions with a multi‐stakeholder\n",
      "perspective is essential.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_021_MLRC_2022_21\n",
      "Results — The results we obtained showed the same patterns as in the original authors’\n",
      "work. All our results were in the range of ±1 LIC score units compared to the original\n",
      "work, which supports the claims on the gender and racial bias amplification, robustness\n",
      "against encoders, and amplification by NIC+Equalizer beyond baseline. As for our con‐\n",
      "tributions, we show that the attribution scores obtained by using integrated gradients\n",
      "follow similar patterns in terms of gender amplification for all evaluated language mod‐\n",
      "els, providing additional support for the proposed LIC metric.\n",
      "During data set analysis we observed a leakage in the original data split being used, re‐\n",
      "sulting in identical captions occurring multiple times in both the training and test set.\n",
      "The removal of already seen captions during training from the test set reduced its size\n",
      "by 62.4% on average and caused a decline in LICM scores of approximately 5 units.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_022_MLRC_2022_22\n",
      "Results — The results we were able to produce support claim 1, albeit weakly. Further\n",
      "results are in line with the paper, but we found them to go against claim 3. In addition,\n",
      "we carried out a theoretical analysis which provides support for claim 4. Finally, we\n",
      "were unable to carry out our intended experiment to verify claim 2.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_023_MLRC_2022_23\n",
      "Results — The results from our experiments were overall in line with the paper. We show\n",
      "that CGD outperforms Group-DRO on synthetic datasets with induced spurious correla‐\n",
      "tions, but the benefits of CGD are not clear in a real‐world setting. Beyond the results\n",
      "of the original paper, our attempt to empirically verify the mathematical proof of the\n",
      "authors that CGD monotonically decreases the loss was not conclusive.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_024_MLRC_2022_24\n",
      "Results — We found that all of the main claims of the paper were reproducible. However,\n",
      "when we repeated the same experiments on two new datasets, we found that there was\n",
      "a much higher correlation in example scores across different tasks (point iv above).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_025_MLRC_2022_25\n",
      "Results — Our recall scores and qualitative experiments validate all claims of the authors:\n",
      "the framework creates an accurate mapping between the visual and semantic space, can\n",
      "analyze any trained CNN regardless of original training data availability, and is able to\n",
      "generate novel out‐of‐dataset descriptions for filters.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_026_MLRC_2022_26\n",
      "Results — The reproduced results are close to the original results and support all four\n",
      "main claims. Furthermore, our additional results show that only a subset of the models\n",
      "amplifies age bias, while they strengthen the claim that LIC is robust against encoders.\n",
      "However, we acknowledge that our extension to age bias has its limitations.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_027_MLRC_2022_27\n",
      "Results — Our results provide solid evidence in favor of the performance‐conserving prop‐\n",
      "erty of CrossWalk. However, we find inconclusive evidence of the fairness‐enhancing\n",
      "property of CrossWalk, mostly due to large variation in the reproduced disparity values.\n",
      "On the other hand, we find additional evidence in its favor by performing an experiment\n",
      "portraying the influence of the hyperparameters of CrossWalk.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_028_MLRC_2022_28\n",
      "Results — Compared to the data reported in the original paper, the reproduced results\n",
      "vary across embedding models and evaluation metrics, where some combinations per‐\n",
      "form very similarly to the original paper while other combinations deviate significantly.\n",
      "Despite this, the claims of the original paper have been confirmed, which include no\n",
      "loss of accuracy, fairly calibrated subgroups and predictive equality.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_029_MLRC_2022_29\n",
      "Results — We verified the main claims of the paper and offered extensions. Our quali‐\n",
      "tative CartoonX and PixelRDE visualization results were similar to the original paper’s.\n",
      "From visualizing them, we saw that CartoonX could reveal piece‐wise information in the\n",
      "image relevant to the classifier. Our quantitative distortion plots followed trends similar\n",
      "to the original plots, allowing us to verify their claims, but only after adjustments to the\n",
      "unclear ’λ’ and ’number of steps’ hyperparameters the paper provides.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_030_MLRC_2022_30\n",
      "Results — We are able to reproduce and verify the three main claims of the original paper,\n",
      "by reproducing the results within 5% of the reported values. The additional experiments\n",
      "were successful and strengthen the claims that LASSI increases certified individual fair‐\n",
      "ness compared to the baseline models. Outliers of the experiments are studied and\n",
      "found to be caused by biased and inaccurate input data.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_031_MLRC_2022_31\n",
      "Results — All but one of the figures and results from the original study were possible\n",
      "to reproduce. The obtained Evidence Lower Bound (ELBO) of the doubling VNCA was\n",
      "within 0.3% of the stated and for the non‐doubling VNCA the ELBO was within 1.8% and\n",
      "the observed damage recovery was similar. We were however not able to reproduce the\n",
      "t‐SNE reduction experiment for the baseline and were therefore not able to show the\n",
      "VNCA decoder having a cleaner t‐SNE separation than the baseline.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_032_MLRC_2022_32\n",
      "Results — We were able to reproduce the results on Least Core approximation. For the\n",
      "task of low‐value point identification we observed similar performance for least core\n",
      "and (Truncated Monte Carlo) Shapley values, whereas for high‐value identification, the\n",
      "least core outperformed other methods. In two experiments, we must depart from the\n",
      "original paper and arrive at different conclusions. Overall, we find that the Least Core\n",
      "offers similar results to other game‐theoretic approaches to data valuation, but it does\n",
      "not overcome the main drawbacks of computational complexity and sensitivity to ran‐\n",
      "domness that such techniques have.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_033_MLRC_2022_33\n",
      "Results — We validate the original paper’s claims by reproducing its metrics within the\n",
      "reported standard deviation, proving the method’s competitiveness. Our extended trials\n",
      "highlight the method’s potential applicability to real‐world data and reveal new consid‐\n",
      "erations about prompt template, language model, and seed for optimal performance.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_034_MLRC_2022_34\n",
      "Results — We verify their theoretical claims that G‐Mixup indeed leads to a mixture of\n",
      "“key graph topologies.” We reproduce their experimental results on classification accu‐\n",
      "racy for REDDIT‐B to within about 6% of the reported value, and for IMDB‐B to within\n",
      "about 2% of the reported value. However, our experimental results do not provide sta‐\n",
      "tistically significant evidence to support the paper’s experimental claims that G‐Mixup\n",
      "improves the performance of GNNs or performs better than other graph data augmen‐\n",
      "tation methods.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_035_MLRC_2022_35\n",
      "Results — Our results are consistent with the claim that the censored text (without swear\n",
      "words) is less often classified as hate speech, offensive, or abusive than the same text\n",
      "with swear words. However, we find the classification is very sensitive to the word re‐\n",
      "placement dictionary being used.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_036_MLRC_2022_36\n",
      "Results — All claims made by the paper [1] seem to hold, as the results we obtained fol‐\n",
      "low the same trends as those presented in the original paper even if they do not match\n",
      "exactly. However, the same cannot always be said of the additional experiments.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_037_MLRC_2022_37\n",
      "Results — We observe expected qualitative behavior predicted by the theory on all exper‐\n",
      "iments. We improve the best semantic hashing model’s test performance by 5 nats by\n",
      "using a modern method for gradient estimation of discrete random variables.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_038_MLRC_2022_38\n",
      "NOT EXTRACTED\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_039_MLRC_2022_39\n",
      "Results — Our work suggests that the first claim of the paper, which states that Crosswalk\n",
      "minimizes disparity and thus enhances fairness is partially reproducible, and only for\n",
      "the tasks of Node classification and Influence maximization as the parameters specified\n",
      "in the paper do not always yield similar results. Then, the second claim of the paper\n",
      "which states that Crosswalk attains the necessary structural properties of the graph is\n",
      "fully reproducible through our experiments.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_040_MLRC_2022_40\n",
      "Results — This paper successfully reproduces the claim that the MAE poses a nontrivial\n",
      "and meaningful self‐supervisory task. We show that models trained with this frame‐\n",
      "work generalize well to new datasets and conclude that the MAE is reproducible with\n",
      "exception for some hyperparameter choices. We also demonstrate that MAE performs\n",
      "well with smaller backbones and datasets. Finally, our results suggest that the SMAE\n",
      "extension improves the downstream classification accuracy of the MAE on CUB (+5 pp)\n",
      "when coupled with an appropriate masking strategy.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_041_MLRC_2022_41\n",
      "Results — We were able to reproduce the main claims on the MUTAG dataset, where Sub‐\n",
      "graphX has a better performance than GNNExplainer. Furthermore, SubgraphX has a\n",
      "reasonable runtime of about seven times longer than GNNExplainer. We successfully\n",
      "employed SubgraphX on the Karate Club dataset, where it outperforms GNNExplainer as\n",
      "well. The hyperparameter study revealed that the number of Monte‐Carlo Tree search it‐\n",
      "erations and Monte‐Carlo sampling steps are the most important hyperparameters and\n",
      "directly trade performance for runtime. Lastly, we show that our proposed improve‐\n",
      "ments to SubgraphX significantly enhance fidelity and runtime.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_042_MLRC_2022_42\n",
      "Results — We succeeded in reproducing phase diagrams for the toy example. For the\n",
      "MNIST dataset, we observed a behavior similar to the one from the paper. We used\n",
      "a wider range of hyperparameters leading us to an extra area with the memorization\n",
      "phase. We also argue that the original memorization phase is even more delayed\n",
      "grokking. Therefore, the authors’ findings about the MNIST phases are incomplete.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_043_MLRC_2022_43\n",
      "Results — Running the code released by the authors does not produce an evaluation of\n",
      "BeT according to the metrics reported in the paper. After extending the implementation\n",
      "with the proper evaluation metrics, we obtain results that support the main claims of\n",
      "the paper in a significant subset of the experiments but that also diverge in many of the\n",
      "actual values obtained. Therefore, we conclude that the paper is largely replicable but\n",
      "not readily reproducible.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_044_MLRC_2022_44\n",
      "Results — Claim 1 is verified: the model with the highest performance on SCAN does\n",
      "not maintain its high performance on other datasets (Section 4.1). Claims 2 and 3 are\n",
      "verified, with a comparison of performance between NQG‐T5 and the selected baseline\n",
      "models in [1] and [2]. Claim 4 is also verified by computing the coverage and precision\n",
      "of NQG in Section 4.4. Overall, accuracy for most experiments reaches within 2% of that\n",
      "reported in the original paper, with a deviation that our T5 achieves higher performance\n",
      "on some splits and slightly lower performance on one split than reported previously.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_045_MLRC_2022_45\n",
      "Results — We reproduced the main claims that (i) oversampling with pure noise improves\n",
      "generalization by improving the minority‐class accuracy, (ii) the proposed batch nor‐\n",
      "malization (BN) method outperforms baselines, (iii) and this improvement is robust\n",
      "across data augmentations. Our results also support that (iv) adding pure noise images\n",
      "can improve classification on balanced training data. However, additional experiments\n",
      "suggest that the performance improvement from OPeN may be more orthogonal to the\n",
      "improvement caused by a bigger network or more complex data augmentation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_046_MLRC_2021_01\n",
      "NOT EXTRACTED\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_047_MLRC_2021_02\n",
      "We validated each of the author’s claim through the experiments given in the original\n",
      "paper and few additional experiments of our own. Overall, we found many experiments\n",
      "yielding identical results while some deviations were observed with both the Counter‐\n",
      "factual Generative Network and the subsequent classification task. We were able to ex‐\n",
      "plain most of these deviations through our additional experiments while some couldn’t\n",
      "be validated due to computational limitations.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_048_MLRC_2021_03\n",
      "On the ResNet‐18 architecture and a high input resolution that the paper uses through‐\n",
      "out, our results on 6 datasets overall verify the claim that SSL regularizes few‐shot learn‐\n",
      "ers and provides higher gains with difficult tasks. Further, our results also verify that\n",
      "out‐of‐distribution images for SSL hurt the accuracy, and the domain selection algo‐\n",
      "rithm that we implement from scratch also verifies the paper’s claim that the algorithm\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_049_MLRC_2021_04\n",
      "Overall, we reproduced the experiments related to the vision task as conducted at [1].\n",
      "Our results are up to first decimal place identical to those reported in [1] thus support‐\n",
      "ing the authors’ claim of having implemented a relatively sufficient ViT interpretabil‐\n",
      "ity method. When it comes to the AffinityNet [2], the method has been adapted in the\n",
      "context of Hybrid‐ViT architectures with our experiments indicating that the weakly‐\n",
      "supervised semantic segmentation performance of Hybrid‐ViT architectures are inferior\n",
      "to the CNN‐based ones.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_050_MLRC_2021_05\n",
      "We find that the main claims of the paper of (i) generating high‐quality counterfactuals,\n",
      "(ii) utilizing appropriate inductive biases, and (iii) using them to instil invariance in clas‐\n",
      "sifiers, do largely hold. However, we found certain experiments that were not directly\n",
      "reproducible due to either inconsistency between the paper and code, or incomplete\n",
      "specification of the necessary hyperparameters. Further, we were unable to reproduce\n",
      "a subset of experiments on a large‐scale dataset due to resource constraints, for which\n",
      "we compensate by performing those on a smaller version of the same dataset with our\n",
      "results supporting the general performance trend.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_051_MLRC_2021_06\n",
      "The results in the original paper regarding different objective functions were repro‐\n",
      "duced within a margin of error. Also, income redistribution is able to reduce wage\n",
      "inequality, albeit to a lesser degree. The objective functions appear to be sensitive to\n",
      "the neighbourhood selection mechanism. While the results of the rider‐fairness objec‐\n",
      "tive functions are maintained, performance of the driver‐fairness objective functions\n",
      "declines. There appear to be only small differences in service rates between ethnicities,\n",
      "while rider‐side fairness seems to mitigate inequalities the most. However, this is only\n",
      "achieved by worsening the service for well‐served neighbourhoods instead of improving\n",
      "it for underserved ones.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_052_MLRC_2021_07\n",
      "Our findings diverge substantially from the results reported in the original paper. In\n",
      "particular, in our reproduction experiments, including sentiment features appears to\n",
      "hurt the performance of the model in the hate speech detection task (approximately 0.5\n",
      "to 2.0 F1‐score) in the setting we could reproduce based on the description in the original\n",
      "paper and published source code (and limited contact with the authors, see Section 1.6).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_053_MLRC_2021_08\n",
      "We were only able to reproduce 22.2% of the explanation evaluation metrics and could\n",
      "thus not find conclusive support for claim 1. We could only verify claim 2 for one of the\n",
      "datasets and in total could reproduce 55.5% of the original scores. We could reproduce\n",
      "all scores regarding claim 3, but the claim is still not justified, as the scores between the\n",
      "fully connected and SCOUTER models lie very close to one another.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_054_MLRC_2021_09\n",
      "The image classification experiments on CIFAR‐10, CIFAR‐100 and ImageNet are repro‐\n",
      "duced to within 0.29%, 0.18% and 0.25% of reported values respectively. The language\n",
      "modeling experiments produce an average deviation of 0.22%, while the generative mod‐\n",
      "eling experiments on WGAN, WGAN‐GP and SN‐GAN are replicated to within 2.2%, 1.8%\n",
      "and 0.33% of value reported in the original paper.\n",
      "We perform ablation studies for change of dataset in language modeling and for effect\n",
      "of weight decay on ImageNet. We also perform analysis of generalization ability of op‐\n",
      "timizers and of training stability of GANs. All of the results largely support the claims\n",
      "made in the paper [1].\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_055_MLRC_2021_10\n",
      "We were able to reproduce the results and verify the claims made by the authors for the\n",
      "StyleGAN and StyleGAN2 models by recreating the modified images, given the seed and\n",
      "other configuration parameters. Additionally, we also perform our own experiments to\n",
      "identify new edits and extend the truncation trick to images generated using StyleGAN.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_056_MLRC_2021_11\n",
      "Overall we find the original results to be reproducible; transformation policies found\n",
      "using Gao et al.1’s method can defend against gradient reconstruction attacks, and these\n",
      "transformations have negligible impact on training efficiency and model accuracy. How‐\n",
      "ever we do not observe the reported correlation between the proposed privacy‐score Spri\n",
      "and reconstruction PSNR. We also find that the degree of protection differs greatly from\n",
      "image to image, with poor protection in the worst case.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_057_MLRC_2021_12\n",
      "We found that the cluster‐based method does indeed consistently noticeably increase\n",
      "the isotropy of a set of CWRs over the global method. However, when it comes to se‐\n",
      "mantic tasks, we found that the cluster‐based method performs better than the global\n",
      "method in some and worse in other tasks, or that the improvements are within margin\n",
      "of error. Additionally, the results of one side experiment, which analyzes the structural\n",
      "information of CWRs, also contradict the authors’ findings for the GPT‐2 model.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_058_MLRC_2021_13\n",
      "Overall, our results support the main claims of the original paper1. Although the percent\n",
      "gendered words and male bias in our results are not exactly the same as those in the\n",
      "original paper1, the main trends are the same. The main difference is lower male bias\n",
      "for the baseline model in our results. However, our findings and the trend similarities\n",
      "between our results and those obtained by Dinan et al.1 demonstrate that bias controlled\n",
      "training or combining all three bias mitigation techniques can effectively control the\n",
      "amount of gender bias present in the model generated responses, supporting Dinan et\n",
      "al.’s claims1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_059_MLRC_2021_14\n",
      "We found that that three of the four claims made by Ziko et al. are supported, and that\n",
      "one claim is partially supported. VFC is mostly on par with SOTA clustering objectives,\n",
      "if the trade‐off parameter and Lipschitz constant are tuned. Additionally, we verified\n",
      "that VFC is scalable on large‐scale datasets and found that the trade‐off control works\n",
      "as stated by the authors. Moreover, we conclude that VFC is capable of handling both\n",
      "prototype‐based and graph‐based datasets. Regarding the replicability of VFC, the ex‐\n",
      "periment on the alternative dataset did not indicate that VFC is worse than SOTA base‐\n",
      "lines. The proposed kernel‐based VFC performs on par with the original framework.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_060_MLRC_2021_15\n",
      "We reproduce the StylEx model in a different framework and test the AttFind algorithm,\n",
      "verifying the original paper’s results for the perceived age classifier. However, we could\n",
      "not reproduce the results for the other classifiers used, due to time limitations in training\n",
      "and the absence of their pre‐trained models. In addition, we verify the paper’s claim\n",
      "of providing human‐interpretable explanations, by reproducing the two user studies\n",
      "outlined in the original paper.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_061_MLRC_2021_16\n",
      "This work finds that (1) adversarial training does in fact lead to classwise performance\n",
      "discrepancies not only in standard error (accuracy) but also in attack robustness, (2)\n",
      "these discrepancies exacerbate existing biases in the model, (3) upweighting the stan‐\n",
      "dard and robust errors of poorly performing classes during training decreased this dis‐\n",
      "crepancy for both both the standard error and robustness and (4) increasing the attack\n",
      "margin for poorly performing classes during training also decreased these discrepan‐\n",
      "cies, at the cost of some performance. (1) (2) and (3) match the conclusions of the origi‐\n",
      "nal paper, while (4) deviated in that it was unsuccessful in helping increasing the robust‐\n",
      "ness the most poorly performing classes. Because the model and datasets used were\n",
      "totally different from the original paper’s, it is hard to to quantify the exact similarity of\n",
      "our results. Conceptually however, I find very similar conclusions.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_062_MLRC_2021_17\n",
      "We show that the theoretical assumption regarding eigenspace alignment and symmetry\n",
      "hold also for a different dataset other than the one used in the original paper. In addition,\n",
      "we reproduce ablations regarding learning rate, weight decay and Exponential Moving\n",
      "Average.\n",
      "Since we used CIFAR‐10 in all experiments we can not directly compare accuracies.\n",
      "However, we show the same relative behaviour of different networks given hyperparam‐\n",
      "eter changes. We can directly compare performance for one of the experiments (Table 8.\n",
      "in [1] bottom left part). Our models, namely SGD Baseline, DirectPred (with and without\n",
      "frequency=5), achieve comparable accuracy which differ by at most 1%. We also con‐\n",
      "firm the claim that DirectPred outperforms its one‐layer SGD alternative. Our code can be\n",
      "accessed under the following link: https://anonymous.4open.science/r/SelfSupervisedLearning-FD0F.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_063_MLRC_2021_18\n",
      "Most of our results closely match the reported results in the original paper for the Rotated‐\n",
      "MNIST [1], Fashion‐MNIST [2], PACS [3, 4], and Chest‐Xray [5] datasets. However, in\n",
      "some cases, as described later, we obtained better results quantitatively than the ones\n",
      "reported in the paper. By investigating the root cause of such mismatches, we provide\n",
      "a possible reason to avoid such a gap. We performed additional experiments by making\n",
      "necessary modifications for the Rotated‐MNIST and Rotated Fashion‐MNIST dataset. In\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_064_MLRC_2021_19\n",
      "Our results followed similar patterns as those of the authors, which backs up their\n",
      "claims regarding the attacks. However, our results did slightly deviate from their re‐\n",
      "sults, meaning the original paper has some reproducibility issues in the context of our\n",
      "experimental setup.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_065_MLRC_2021_20\n",
      "The reproduction of the original paper as well as the extended implementation were suc‐\n",
      "cessful. We were able to reproduce the original results and examine the performance\n",
      "of the proposed model in an environment where strategic and non‐strategic users both\n",
      "present. Linear models seem to struggle with different proportions of strategic users,\n",
      "while the non‐linear model (RNN) achieves good performance regardless of the propor‐\n",
      "tion of strategic users.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_066_MLRC_2021_21\n",
      "A model is created for both dog‐cat and age classification. The models performed worse\n",
      "than stated than the pretrained models, most likely due to some issues with the StylEx\n",
      "style space, as AttFind performed well on the StyleGAN2 model. Due to the limitations\n",
      "in the adaptation it is not possible to definitively state whether the claims are true or\n",
      "false.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_067_MLRC_2021_22\n",
      "Due to many missing implementation details, it is not possible to reproduce the original\n",
      "results using the paper alone. However, in a specific setting motivated by the authors’\n",
      "code (more details in section 3), we managed to obtain results that support 3 out of 5\n",
      "claims. Even though the IAF and anchoring attacks outperform the baselines in certain\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_068_MLRC_2021_23\n",
      "There are no exact numbers in the original paper to reproduce, rather the main claims\n",
      "are supported with some visualisations. With this in mind, our reproduction confirms\n",
      "the advantage provided by clustering over the assumption of independent arms, as well\n",
      "as the newly proposed algorithms outperforming the referenced benchmarks. We re‐\n",
      "peat all the experiments with multiple seeds to obtain robust estimates of the algorithms’\n",
      "performance and reduce the risk of drawing any conclusions out of results obtained by\n",
      "chance.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_069_MLRC_2021_24\n",
      "The reproduced results support all claims made in [1]. However, in the case of the unfair\n",
      "secretary algorithm (SA), some irregular results arise in the experiments due to random‐\n",
      "ness. This irregularity is also existent in the original code.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_070_MLRC_2021_25\n",
      "We successfuly reproduced the task‐agnostic experiments of the original paper, finding\n",
      "our results to strongly match with the original results. We also carried out a comparison\n",
      "with FrequentDirections but found the evaluation metrics of the original paper to\n",
      "be ill‐suited to compare ‐ setting up for further work on developing fair comparisons.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_071_MLRC_2021_26\n",
      "We reproduced and verified all the central claims made by the authors in the paper,\n",
      "confirming the intuition behind the novel methodologies introduced in the paper. Our\n",
      "results differ using the parameters given in the paper for the segmentation experiments\n",
      "but still support the claim of NAL being superior to its counterpart losses.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_072_MLRC_2021_27\n",
      "We were able to reproduce the exact results reported by the authors in all originally re‐\n",
      "ported scenarios. However, extended results on larger Wide Residual Networks have\n",
      "demonstrated the limitations of the newly proposed learning rate rewinding – we ob‐\n",
      "served a previously unreported accuracy degradation in low sparsity ranges. Neverthe‐\n",
      "less, the general conclusion of the paper still holds and was indeed reproduced.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_073_MLRC_2021_28\n",
      "Our results slightly deviate from the ones reported by the authors. This could be at‐\n",
      "tributed to the design choices we had to make, due to ambiguities present in the orig‐\n",
      "inal paper. After inspecting the provided codebase along with relevant literature, we\n",
      "were able to replicate the experimental setup. In our experiments, we observe similar\n",
      "trends and hence we can verify most of the paper’s claims, albeit not getting identical\n",
      "experimental results.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_074_MLRC_2021_29\n",
      "We reproduced the first claim since we observed the same monotonic increase of the\n",
      "success rate in the worst‐served neighborhood with respect to the overall success rate.\n",
      "The second claim we did not reproduce, since we found that the driver‐side fairness ob‐\n",
      "jective function obtains a higher income for the lowest‐earning drivers than the request‐\n",
      "maximizing objective function. We reproduced the third claim, since the driver‐side\n",
      "objective function performs best in terms of overall success rate and success rate in\n",
      "the worst‐served neighborhood, and also reduces the spread of income. Changes of the\n",
      "value estimator, preprocessing method and even dataset all led to consistent results re‐\n",
      "garding these claims.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_075_MLRC_2021_30\n",
      "We replicate the results of Pan et al. [1] on a subset of the LSUN Cat, LSUN Car [2] and\n",
      "CelebA [3] datasets and observe varying degrees of success. We perform several exper‐\n",
      "iments and illustrate the successes and shortcomings of the method. Our novel shape\n",
      "priors improve the 3D shape recovery in certain cases where the original shape prior\n",
      "was unsuitable. Our generalized training approach shows initial promise, but has to be\n",
      "confirmed with increased computational resources.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_076_MLRC_2021_31\n",
      "The techniques proposed by authors in [1] can successfully address the value alignment\n",
      "verification problem in different settings. We empirically demonstrate the effectiveness\n",
      "of their proposals by performing exhaustive experiments with several variations to their\n",
      "original claims. We show high accuracy and low false positive and false negative rates in\n",
      "the value alignment verification task with a minimum number of questions for different\n",
      "algorithms and heuristics.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_077_MLRC_2021_32\n",
      "For the Multi‐Color Secretary problem, we were able to recreate the outcomes, as well\n",
      "as the performance of the proposed algorithm (with a margin of 3‐4%). However, one\n",
      "baseline within the second experiment returned different results, due to inconsisten‐\n",
      "cies in the original implementation. In the context of the Multi‐Color Prophet problem,\n",
      "we were not able to exactly reproduce the original results, as the authors ran their exper‐\n",
      "iments with twice as many runs as reported. After correcting this, the original outcomes\n",
      "are reproduced.\n",
      "A drawback of the proposed prophet algorithms is that they only select a candidate in 50‐\n",
      "70% of cases. None results are often undesirable, so we extend the paper by proposing\n",
      "adjusted algorithms that pick a candidate (almost) every time. Furthermore, we show\n",
      "empirically that these algorithms maintain similar levels of fairness.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_078_MLRC_2021_33\n",
      "We were not able to fully reproduce the results of the original paper in this setting. The\n",
      "numbers (accuracies, precisions and margin distributions) obtained in our experiments\n",
      "differ significantly from those reported in the original paper. Though differences be‐\n",
      "tween the baseline model and the sufficiency model are not as significant as in the\n",
      "original paper, our results do support the main claims about sufficiency being able to\n",
      "increase the worst‐group precision and thus causing disparities between groups to de‐\n",
      "crease.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_079_MLRC_2021_34\n",
      "We reproduced the accuracy for the SPT planner module to within 14.7% of reported\n",
      "value, which, while outperforming the baselines [2] [3] in select cases, fails to support\n",
      "the paper’s conclusion that it outperforms the baselines. However, we achieve a similar\n",
      "drop‐off in accuracy in percentage points over different model settings. We suspect that\n",
      "the vagueness in the accuracy metric leads to the absolute difference of 14.7% despite\n",
      "the paper being reproducible. We further improve the reproduced figures by increasing\n",
      "model complexity. The Mapper module’s accuracy could not be tested.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_080_MLRC_2021_35\n",
      "In general, we are able to reproduce the results of Hyder et al. [1]. Because of the hy‐\n",
      "perparameter search, we are certain that the results are not cherry‐picked and mostly\n",
      "reproducible using the authors’ implementation of the algorithm. With our additional\n",
      "experiments, we further strengthen the validity of the proposed method and help future\n",
      "researchers and practitioners by providing additional information on the learning rates\n",
      "in the training and retrieval process.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_081_MLRC_2021_36\n",
      "We were able to reproduce majority of the results claimed in the paper except the Social‐\n",
      "LSTM and Directional‐LSTM models due to lack of time, and got a maximum of 2% de‐\n",
      "viation from that of the original paper.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_082_MLRC_2021_37\n",
      "Through our testing, we were able to come within 2% of the proposed metrics on certain\n",
      "datasets like Stanford Drone Dataset (SDD) and ETH/UCY, implying the author’s claims\n",
      "are sanguine and reproducible on varied hardware. However, certain results such as\n",
      "long‐term predictions on the Intersection Drone (InD) dataset were quite different; the\n",
      "probable reasons for which have been discussed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_083_MLRC_2021_38\n",
      "We were able to reproduce GEN’s out‐performance of a chosen baseline and its perfect\n",
      "scores on synthetic data sets. We also confirm the author’s claims of the sub‐quadratic\n",
      "scaling of GEN’s forward passes and deduce that they reported the scaling of back‐passes\n",
      "too favourably. We conclude our work with scepticism of the chosen experiments’ suit‐\n",
      "ability to evaluate the model’s performance and discuss our findings.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_084_MLRC_2021_39\n",
      "We could not reproduce the density maps, but we produced similar density maps by\n",
      "modifying some of the parameters. We exactly reproduced the results on the paper’s\n",
      "data set. We did not get the same results on the CARPK data set and in experiments\n",
      "where implementation details were not provided. However, the differences are within\n",
      "standard error and our results support the claim that the model outperforms the base‐\n",
      "lines.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_085_MLRC_2021_40\n",
      "We reproduce most of the results supporting the original authors’ general claim: seed\n",
      "sets often suffer from biases that affect their performance as a baseline for bias metrics.\n",
      "Generally, our results mirror the original paper’s. They are slightly different on select\n",
      "occasions, but not in ways that undermine the paper’s general intent to show the fragility\n",
      "of seed sets.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_086_MLRC_2021_41\n",
      "The tracking performance was reproduced in terms of success, precision, and normal‐\n",
      "ized precision, and the reported value is in the 95 percent confidence interval, which\n",
      "supports the paper’s conclusion that TransATOM significantly outperforms other state‐\n",
      "of‐the‐art algorithms on TOTB database. Also, it supports a claim that including a trans‐\n",
      "parency feature in the tracker improves performance when tracking transparent objects.\n",
      "However, we refuted the claim that TransATOM well handles all challenges for robust\n",
      "target localization.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_087_MLRC_2021_42\n",
      "We verified that the publicly available pre‐trained model has a ’sufficiency’ measure\n",
      "within 1% of the value reported in the paper. Additionally, we evaluate the Fréchet in-\n",
      "ception distance (FID) scores of images generated by the released model. We show that\n",
      "the FID score increases with the number of attributes used to generate a counterfac‐\n",
      "tual explanation. Custom models were trained on three datasets, with a reduced image\n",
      "dimensionality (642px). Additionally, a user study was conducted to evaluate the distinc‐\n",
      "tiveness and coherence of the images. We report a significantly lower accuracy in the\n",
      "identification of the extracted attributes and ’sufficiency’ scores on our model.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_088_MLRC_2021_43\n",
      "We reproduced most of the data utility results reported in the first experiment for the\n",
      "Adult dataset. However, while the fairness metrics generally match the original paper\n",
      "they are numerically not comparable in absolute or relative terms. For the second exper‐\n",
      "iment, we were unsuccessful in reproducing results found by the authors. We note how‐\n",
      "ever that we made considerable changes to the experimental setup, which may make it\n",
      "difficult to perform a direct comparison of the results.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_089_MLRC_2021_44\n",
      "It was possible to verify the results from the original paper within a reasonable margin\n",
      "of error. However, the reproduced results show that the claimed protection does not\n",
      "generalize to an attacker that has knowledge over the augmentations used. Addition‐\n",
      "ally, the results show that the optimal augmentations are often predictable since the\n",
      "policies found by the proposed search algorithm mostly consist of the augmentations\n",
      "that perform best individually.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_090_MLRC_2021_45\n",
      "For the validation of the original paper, we compare the pre‐trained model and the re‐\n",
      "trained model to the results reported in the original paper. The retrained RCExplainer\n",
      "outperformed the other methods on fidelity and robustness, which corresponds with\n",
      "the results of the original authors. The measured efficiency of the method also corre‐\n",
      "sponds to the original result. To extend the paper, this comparison is also performed\n",
      "using a train‐test split, which showed no significant difference. The implementation of\n",
      "the metric is investigated and concerns are raised. Finally, the method generalises well\n",
      "to MNISTSuperpixels in terms of fidelity, but lacks in robustness.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_091_MLRC_2021_46\n",
      "We have achieved to reproduce the results qualitatively and quantitatively on a large\n",
      "scale. We also validated the generalization ability of the model by training and testing it\n",
      "on CelebA dataset. Although our experimental results are not identical with the original\n",
      "paper, they are consistent and validates the claims made by the original work.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_092_MLRC_2021_47\n",
      "We reproduced the papers results within standard deviations of our repeated experi‐\n",
      "ments. But in some cases, this still means there is a big difference between the perfor‐\n",
      "mances, which is coming from different train‐test splits of the newly proposed split‐\n",
      "ting schemes. Even with these discrepancies we still managed to (at least partially)\n",
      "confirm all authors claims. The proposed model GNN‐PPI performed better than PIPR\n",
      "overall and for inter‐novel‐protein interactions, evaluation on their proposed schemes\n",
      "predicted the generalization performance better, and their model is also robust for pre‐\n",
      "dictions for newly discovered proteins – here our results were surprising, they were even\n",
      "better when the network was built knowing fewer proteins.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_093_MLRC_2021_48\n",
      "With our tests and the obtained results, we confirm that all individual and combined\n",
      "sources of nondeterminism have similar effects on model variability and that instabil‐\n",
      "ity in neural network optimization is the main reason for this phenomenon. However,\n",
      "our results show some discrepancies in the reduction of variability by test‐time data aug‐\n",
      "mentation (TTA) and accelerated ensembling (claim 3 above). Like the original study, we\n",
      "show that these approaches successfully reduce variability, but the degree of reduction\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_094_MLRC_2020_01\n",
      "We were able to reproduce their results using their code, yielding mostly similar results.\n",
      "TGT generally outperforms DBM, especially when explanations use few features. TGT\n",
      "is consistent in terms of the features to which it attributes cluster differences, across\n",
      "different sparsity levels. TGT matches real patterns in data. When extending the types of\n",
      "functions used for explanations, performance did not improve significantly, suggesting\n",
      "translations make for adequate explanations. However, the scaling extension shows\n",
      "promising performance on the modified synthetic data to recover the original signal.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_095_MLRC_2020_02\n",
      "We reproduce the claims of both papers by conducting several experiments in the UAVA\n",
      "dataset [3]. The integration of a differentiable geometric module within an keypoint-\n",
      "based object pose estimation model improved its performance in metrics. We addition-\n",
      "ally verify that this is the case for other differentiable PnP implementations (i.e. EPnP).\n",
      "Further, our results indicate that indeed HigherHRNet improves keypoint localisation\n",
      "performance on small scale objects.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_096_MLRC_2020_03\n",
      "We were able to successfully replicate experiments supporting the central claim of the\n",
      "paper, that the proposed snake non-linearity can learn periodic functions. We also\n",
      "analyze the suitability of the snake activation for other tasks like generative modeling\n",
      "and sentiment analysis.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_097_MLRC_2020_04\n",
      "For the MNIST-USPS dataset, we report similar accuracy and NMI values that are within\n",
      "1.2% and 0.5% of the values reported in the original paper. However, the balance and\n",
      "entropy differed significantly, where our results were within 73.1% and 30.3% of the orig-\n",
      "inal values respectively. For the Color Reverse MNIST dataset, we report similar values\n",
      "on accuracy, balance and entropy, which are within 5.3%, 2.6% and 0.2% respectively.\n",
      "Only the value of the NMI differed significantly, name within 12.9% of the original value\n",
      "In general, our results still support the main claim of the original paper, even though\n",
      "on some metrics the results differ significantly.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_098_MLRC_2020_05\n",
      "We reproduced the accuracy of the BayesBiNN optimizer within less than 0.5% of the\n",
      "originally reported value, which upholds the conclusion that it performs nearly as well\n",
      "as its full-precision counterpart in classification tasks. When we tried this in a seman-\n",
      "tic segmentation context, we found that the results were very underwhelming and in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_099_MLRC_2020_06\n",
      "We reproduced the authorsʼ results across all models and all available datasets, confirm-\n",
      "ing their findings that attention-based explanations can be manipulated and that mod-\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_100_MLRC_2020_07\n",
      "Due to numerous inconsistencies between code and paper, it is not possible to replicate\n",
      "the original results using the paper alone. With help of the original codebase, a number\n",
      "of the original results can be retrieved. The main comparison claim of the paper, to\n",
      "improve over the preceding GNNExplainer, does hold. However, after performing the\n",
      "replication experiments, some questions regarding the validity of the used evaluation\n",
      "setup in the original paper remain.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_101_MLRC_2020_08\n",
      "We found that both proposed methods in the original paper help mitigate contextual\n",
      "bias, although for some methods, we could not completely replicate the quantitative\n",
      "results in the paper even after completing an extensive hyperparameter search. For\n",
      "example, on COCO-Stuff, DeepFashion, and UnRel, our feature-split model achieved an\n",
      "increase in accuracy on out-of-context images over the standard baseline, whereas on\n",
      "AwA, we saw a drop in performance. For the proposed CAM-based method, we were able\n",
      "to reproduce the original paperʼs results to within 0.5% mAP.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_102_MLRC_2020_09\n",
      "NOT EXTRACTED\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_103_MLRC_2020_10\n",
      "Our findings support the authorsʼ central claims. In terms of uncertainty estimation our\n",
      "EnD2 achieved (99 ± 1) % of the AUC-ROC of our ensemble on the OOD-detection task.\n",
      "The corresponding value in the original paper was (100 ± 1) %. In terms of classification\n",
      "our EnD2 had (16 ± 1)% higher error than our ensemble. The corresponding values in\n",
      "the original paper was (11 ± 6)%. Other metrics showed similar agreement, but, signifi-\n",
      "cantly, in the OOD-detection task our EnD performed at least as well as our EnD2. This\n",
      "is in stark contrast with the original paper.\n",
      "We also took a novel approach to visualizing the uncertainty decomposition by plotting\n",
      "the resulting distributions on a simplex, offering a visual explanation to some surprising\n",
      "results in the original paper, while mostly supporting the authorsʼ intuitive justifications\n",
      "for the model.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_104_MLRC_2020_11\n",
      "We obtain results that are within a 3% range of the reported results for all datasets other\n",
      "than on the ShanghaiTech dataset[2]. The anomalous run stuck out since the ”Non Mem-\n",
      "ory” of the same run could score markedly better than the ”Memory” variant. This led us\n",
      "to investigate the behaviour of the memory module and found valuable insights which\n",
      "are presented in Section 5.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_105_MLRC_2020_12\n",
      "We have achieved to reproduce the results qualitatively and quantitatively on synthetic\n",
      "and noise removal tasks. SADNet has the capacity to learn to remove the synthetic and\n",
      "real noise in images, and it produces visually-plausible outputs even after a few epochs.\n",
      "Moreover, we have employed SSIM and PSNR metrics to measure the quantitative perfor-\n",
      "mance for all settings. The quantitative results on both tasks are on-par when compared\n",
      "to the reported results in the paper.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_106_MLRC_2020_13\n",
      "Overall, our results mostly support the claims of the original paper. For the synthetic\n",
      "experiments, our results differ when using the exact values described in the paper, al-\n",
      "though they still support the main claim. After slightly modifying some of the experi-\n",
      "ment settings, our reproduced figures are nearly identical to the figures from the original\n",
      "paper. For the deep learning experiments, our results differ, with some of the baselines\n",
      "reaching a much higher accuracy on MNIST, CIFAR-10 and CIFAR-100. Nonetheless,\n",
      "with the help of an additional experiment, our results support the authorsʼ claim that\n",
      "partially Huberised losses perform well on real-world datasets subject to label noise.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_107_MLRC_2020_14\n",
      "Most of our results closely match the reported results in the original paper. Therefore,\n",
      "we confirm that the warm-starting gap exists in certain settings and that the Shrink-\n",
      "Perturb method successfully reduces or eliminates this gap. However, in some cases,\n",
      "we were not able to completely reproduce their results. By investigating the root of such\n",
      "mismatches, we provide another solution to avoid this gap. In particular, we show that\n",
      "data augmentation also helps to reduce the warm-starting gap.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_108_MLRC_2020_15\n",
      "We were able to reproduce the Hits@1 to be within ±2.4% of the reported value (in most\n",
      "cases). Anomalies were observed in 2 cases.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_109_MLRC_2020_16\n",
      "The results presented in [1] were reproducible, both by using the provided code and our\n",
      "own implementation. Our additional experiments have highlighted several limitations\n",
      "of the explanatory algorithm in question: the algorithm severely relies on the shape and\n",
      "variance of the clusters present in the data (and, if applicable, the method used to la-\n",
      "bel these clusters), and highly non-linear dimensionality reduction algorithms perform\n",
      "worse in terms of explainability.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_110_MLRC_2020_17\n",
      "The obtained mean and standard deviation of the MCC over 100 seeds are within 1 per-\n",
      "cent of the results reported in the paper. The iFlow model obtained a mean MCC score of\n",
      "0.718 (0.067). Efforts to improve and correct the baseline increased the mean MCC score\n",
      "from 0.483 (0.059) to 0.556 (0.061). The performance, however, remains worse than the\n",
      "performance of iFlow, further supporting the authorsʼ claim that the iFlow implemen-\n",
      "tation is correct and more effective than iVAE.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_111_MLRC_2020_18\n",
      "NOT EXTRACTED\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_112_MLRC_2020_19\n",
      "During the study, we were not able to reproduce the method due to a conceptual mis-\n",
      "interpretation of ours regarding the authorsʼ adaption of the Transformer [2]. However,\n",
      "the publicly available implementation helped us answering our questions and proved\n",
      "its validity during our experiments on different datasets. Additionally, we compared the\n",
      "papersʼ temporal attention encoder to our adaption of it, which we came across while\n",
      "we were trying to reimplement and grasp the authorsʼ ideas.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_113_MLRC_2020_20\n",
      "In contrast to the first claim, we have discovered that for most of the architectures, re-\n",
      "construction errors for the attacks are quite low, which means that in our models the\n",
      "first claim is not supported. We also found that for most of the models, the classifica-\n",
      "tion error is somewhat higher than those provided in the paper. However, these indeed\n",
      "relate to the original work and partially support the second claim of the authors.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_114_MLRC_2020_21\n",
      "We reproduce RigLʼs performance on CIFAR-10 within 0.1% of the reported value. On\n",
      "both CIFAR-10/100, the central claim holds—given a fixed training budget, RigL sur-\n",
      "passes existing dynamic-sparse training methods over a range of target sparsities. By\n",
      "training longer, the performance can match or exceed iterative pruning, while consum-\n",
      "ing constant FLOPs throughout training. We also show that there is little benefit in tun-\n",
      "ing RigLʼs hyper-parameters for every sparsity, initialization pair—the reference choice\n",
      "of hyperparameters is often close to optimal performance.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_115_MLRC_2020_22\n",
      "Results — We confirm that the Orthogonal and Diversity LSTM achieve similar accura-\n",
      "cies as the Vanilla LSTM, while lowering conicity. However, we cannot reproduce the\n",
      "results of several of the experiments in the paper that underlie their claim of better trans-\n",
      "parency. In addition, a close inspection of the code base reveals some potentially prob-\n",
      "lematic inconsistencies. Despite this, under certain conditions, we do confirm that the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_116_MLRC_2020_23\n",
      "We reproduced the framework in the original paper and verified the main claims made\n",
      "by the authors in the original paper. However, the GCE model in extension study did\n",
      "not manage to separate causal factors and non-causal factors for a text classifier due to\n",
      "the complexity of fine-tuning the model.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_117_MLRC_2020_24\n",
      "NOT EXTRACTED\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_118_NeurIPS_2019_01\n",
      "NOT EXTRACTED\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_119_NeurIPS_2019_02\n",
      "Binarized Neural Networks are paving a way towards the deployment of deep\n",
      "neural networks with less memory and computation.\n",
      "In this report, we present\n",
      "a detailed study on the paper titled \"Latent Weights Do Not Exist: Rethinking\n",
      "Binarized Neural Network Optimization\" by [1] which proposes a new optimization\n",
      "method for training BNN called BOP. We ﬁrst investigate the eﬀect of using latent\n",
      "weights in BNN for analyzing prediction performance in terms of accuracy. Next, a\n",
      "comprehensive ablation study on hyperparameters is provided. Finally, we explore\n",
      "the usability of BNN in denoising autoencoders. Code for all our experiments are\n",
      "available at https://github.com/nancy-nayak/rethinking-bnn/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_120_NeurIPS_2019_03\n",
      "In this project we attempt to reproduce results from the paper Generative Mod-\n",
      "eling by Estimating Gradients of the Data Distribution by Song and Ermon1. The\n",
      "authors propose a novel generative framework based solely on gradients of data\n",
      "density estimated by a neural network. Once the model is trained, sampling can\n",
      "be performed with annealed Langevin dynamics. While we managed to reproduce\n",
      "the experiments qualitatively, we failed to achieve comparable results for Inception\n",
      "and FID scores for CIFAR-10. We further extended the original work in various di-\n",
      "rections (computing and analysing FID and IS also for CelebA, investigation of the\n",
      "sampling hyperparameters ϵ and T , linear instead of geometric annealing schedule\n",
      "for noise levels, and diﬀerent network architecture).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_121_NeurIPS_2019_04\n",
      "NOT EXTRACTED\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_122_NeurIPS_2019_05\n",
      "NOT EXTRACTED\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_123_NeurIPS_2019_06\n",
      "In today’s world, neural networks are being in almost every discipline resulting in\n",
      "signiﬁcant improvement in all the tools and applications. But in the ﬁeld of Physics,\n",
      "they struggle to attain the basic laws like conservation of momentum. The paper\n",
      "Hamiltonian Neural Networks addresses this issue by using Hamiltonian mechanics\n",
      "to train the neural network in an unsupervised method. The following report is an\n",
      "explanation of the paper and the code to reproduce the claimed results.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_124_NeurIPS_2019_07\n",
      "We reproduce the work in Zero-shot Knowledge Transfer via Adversarial Belief\n",
      "Matching, which describes a novel approach for knowledge transfer. A teacher net-\n",
      "work trained on real samples distills knowledge to a student network that is trained\n",
      "solely on pseudo data extracted from a generator network, with the student trying\n",
      "to mimic the teacher’s outputs on these datapoints. To this end, we additionally\n",
      "re-implement Wide Residual Networks which are used as the main framework for\n",
      "both teacher and student networks and train them from scratch on CIFAR10 and\n",
      "SVHN. We compare the results of the proposed method with a few-shot knowledge\n",
      "distillation attention transfer setting implemented and trained from scratch. We\n",
      "suggest an approach for further exploitation of the learnt mechanics of the gener-\n",
      "ator network in the zero-shot setting, which operates on top of the main method,\n",
      "and brieﬂy discuss the beneﬁts and drawbacks of this approach. Our code can be\n",
      "found publicly available in https://github.com/AlexandrosFerles/NIPS_2019_Reproducibilty_\n",
      "Challenge_Zero-shot_Knowledge_Transfer_via_Adversarial_Belief_Matching.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_125_NeurIPS_2019_08\n",
      "NOT EXTRACTED\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_126_NeurIPS_2019_09\n",
      "In this study, we performed some ablations on the main model developed in the pa-\n",
      "per Unsupervised Representation Learning in Atari [1] as part of the 2019 NeurIPS Re-\n",
      "producibility Challenge. In this paper, Anand et. al introduce a new learning method\n",
      "called SpatioTemporal DeepInfoMax (STDIM), which is an unsupervised method that\n",
      "aims at learning state representations by maximizing particular forms of mutual in-\n",
      "formation between a series of observations. Our work focuses on recreating a subset\n",
      "of their results, along with hyperparameter tuning, slightly altering the STDIM learn-\n",
      "ing objective, and altering the receptive field of the encoder model that Anand et. al\n",
      "introduce in their article. We also suggest directions for further expanding the STDIM\n",
      "method. Our results also suggest that creating an ensemble model would allow for\n",
      "further boosting of the effectiveness of this model.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_127_NeurIPS_2019_10\n",
      "The lottery ticket hypothesis states that smaller subnetworks within a larger\n",
      "deep network can be trained in isolation to achieve accuracy similar to that of\n",
      "original network, as long as they are initialized appropriately. However, whether\n",
      "these subnetworks or winning tickets are transferable across datasets and optimizers\n",
      "remains unclear. The paper \"One ticket to win them all:generalizing lottery ticket\n",
      "initializations across datasets and optimizers\" empirically shows that these winning\n",
      "tickets are transferable. We reproduce the results in the paper from scratch by\n",
      "implementing all the experiments. Our results support the original paper’s claim of\n",
      "the winning ticket initializations being transferable. While the paper is replicable,\n",
      "we ﬁnd that reproducing the paper requires access to large amount of computing\n",
      "resources for generating the winning tickets. Hence we also open-source the winning\n",
      "tickets we ﬁnd, so others can avoid the compute-intensive procedure of generating\n",
      "them.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_128_NeurIPS_2019_11\n",
      "Miscalibration of a model is deﬁned as the mismatch between predicting proba-\n",
      "bility estimates and the true correctness likelihood. In this work, we aim to replicate\n",
      "the results reported by [1] on their analysis of the eﬀect of Mixup [2] on a network’s\n",
      "calibration. Mixup is an eﬀective yet simple approach of data augmentation, which\n",
      "generates a convex combination of a pair of training images and their correspond-\n",
      "ing labels as the input and target for training a network. We replicate the results\n",
      "reported by the authors for CIFAR-100 [3], Fashion-MNIST [4], STL-10 [5], out-\n",
      "of-distribution and random noise data. Our implementation code can be found at\n",
      "https://github.com/MacroMayhem/OnMixup.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_129_ICLR_2019_01\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_130_ICLR_2019_02\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_131_ICLR_2019_03\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_132_ICLR_2019_04\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_133_ICLR_2019_05\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_134_ICDAR_2018_01\n",
      "Non-Reproducible - Kenny's Remarks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_135_ICDAR_2018_02\n",
      "Non-Reproducible - Kenny's Remarks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_136_ICDAR_2018_03\n",
      "Reproducible - Kenny's Remarks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_137_ICDAR_2018_04\n",
      "Non-Reproducible - Kenny's Remarks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_138_ICDAR_2018_05\n",
      "Non-Reproducible - Kenny's Remarks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_139_ICDAR_2018_06\n",
      "Partially Reproducible\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_140_ICDAR_2018_07\n",
      "Non-Reproducible - Kenny's Remarks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_141_ICDAR_2018_08\n",
      "Non-Reproducible - Kenny's Remarks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_142_ICDAR_2018_09\n",
      "Non-Reproducible - Kenny's Remarks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_143_ICDAR_2018_10\n",
      "Reproducible - Kenny's Remarks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_144_ICDAR_2018_11\n",
      "Reproducible - Kenny's Remarks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_145_ICDAR_2018_12\n",
      "Non-Reproducible - Kenny's Remarks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_146_ICDAR_2018_13\n",
      "Reproducible - Kenny's Remarks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_147_ICDAR_2018_14\n",
      "Non-Reproducible - Kenny's Remarks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_148_ICDAR_2018_15\n",
      "Non-Reproducible - Kenny's Remarks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RS_149_ICDAR_2018_16\n",
      "Non-Reproducible - Kenny's Remarks\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_excel = pd.read_excel('RS_ALL_IN_ONE_metadata_TEST.xlsx',sheet_name=\"Sheet1\")\n",
    "\n",
    "for index, row in df_excel.iterrows():\n",
    "    print(row['key_for_all_RS'])\n",
    "    print(row['rs_comment'])\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd35f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6471461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
